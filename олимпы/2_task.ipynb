{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13367894,
          "sourceType": "datasetVersion",
          "datasetId": 8480207
        },
        {
          "sourceId": 13368115,
          "sourceType": "datasetVersion",
          "datasetId": 8480372
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "2 task",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRanker\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "df = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "\n",
        "df['user_clicks'] = df.groupby('user_id')['item_id'].transform('count')\n",
        "df['item_popularity'] = df.groupby('item_id')['user_id'].transform('count')\n",
        "\n",
        "# Normalize popularity\n",
        "df['item_popularity'] = np.log1p(df['item_popularity'])\n",
        "df['user_clicks'] = np.log1p(df['user_clicks'])\n",
        "\n",
        "# Temporal recency feature\n",
        "df['recency'] = df['date'].max() - df['date']\n",
        "\n",
        "# Aggregate to (user_id, item_id)\n",
        "features = df.groupby(['user_id', 'item_id'], as_index=False).agg({\n",
        "    'recency': 'min',\n",
        "    'item_popularity': 'mean',\n",
        "    'user_clicks': 'mean',\n",
        "    'date': 'max'\n",
        "})\n",
        "\n",
        "# Label = 1 (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∫–ª–∏–∫–∞–ª –Ω–∞ —Ç–æ–≤–∞—Ä)\n",
        "features['label'] = 1\n",
        "\n",
        "# ========== 3. NEGATIVE SAMPLING ==========\n",
        "# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–æ–±–∞–≤–ª—è–µ–º 20 —Å–ª—É—á–∞–π–Ω—ã—Ö –Ω–µ –∫–ª–∏–∫–Ω—É—Ç—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤\n",
        "unique_items = df['item_id'].unique()\n",
        "neg_samples = []\n",
        "\n",
        "print(\"Generating negatives...\")\n",
        "for uid, grp in tqdm(df.groupby('user_id')):\n",
        "    pos_items = set(grp['item_id'])\n",
        "    neg_items = np.random.choice(list(set(unique_items) - pos_items), size=20, replace=False)\n",
        "    tmp = pd.DataFrame({'user_id': uid, 'item_id': neg_items})\n",
        "    tmp['label'] = 0\n",
        "    neg_samples.append(tmp)\n",
        "\n",
        "neg_df = pd.concat(neg_samples, ignore_index=True)\n",
        "\n",
        "# Merge with features\n",
        "train_df = pd.concat([features[['user_id', 'item_id', 'recency', 'item_popularity', 'user_clicks', 'label']], neg_df])\n",
        "train_df = train_df.fillna(0)\n",
        "\n",
        "# ========== 4. PREPARE DATA FOR LIGHTGBM ==========\n",
        "X = train_df[['recency', 'item_popularity', 'user_clicks']]\n",
        "y = train_df['label']\n",
        "group = train_df.groupby('user_id').size().to_numpy()\n",
        "\n",
        "# ========== 5. TRAIN MODEL ==========\n",
        "print(\"Training LGBMRanker...\")\n",
        "model = LGBMRanker(\n",
        "    objective='lambdarank',\n",
        "    metric='map',\n",
        "    boosting_type='gbdt',\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=200,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X, y, group=group)\n",
        "\n",
        "# ========== 6. GENERATE RECOMMENDATIONS ==========\n",
        "print(\"Generating recommendations...\")\n",
        "user_features = train_df[['user_id']].drop_duplicates()\n",
        "item_features = df[['item_id']].drop_duplicates()\n",
        "\n",
        "preds = []\n",
        "for uid in tqdm(sample['user_id']):\n",
        "    # –ë–µ—Ä—ë–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å top-500 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö)\n",
        "    candidates = item_features.copy()\n",
        "    candidates['user_id'] = uid\n",
        "    candidates['recency'] = 0\n",
        "    candidates['user_clicks'] = np.log1p(df.loc[df['user_id'] == uid, 'item_id'].count())\n",
        "    candidates['item_popularity'] = np.log1p(candidates['item_id'].map(df['item_id'].value_counts()).fillna(0))\n",
        "\n",
        "    X_pred = candidates[['recency', 'item_popularity', 'user_clicks']]\n",
        "    candidates['score'] = model.predict(X_pred)\n",
        "    top20 = candidates.nlargest(20, 'score')['item_id'].tolist()\n",
        "    preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "sub = pd.DataFrame(preds)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(\"‚úÖ Saved submission.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:42:08.314384Z",
          "iopub.execute_input": "2025-10-15T11:42:08.314731Z",
          "iopub.status.idle": "2025-10-15T11:55:39.337127Z",
          "shell.execute_reply.started": "2025-10-15T11:42:08.314705Z",
          "shell.execute_reply": "2025-10-15T11:55:39.335549Z"
        },
        "id": "0Xi9tciHbgdf",
        "outputId": "e00b33b6-f989-4ff2-a418-321f357b7560"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading data...\nTrain shape: (8777975, 3), Sample: (5864600, 2)\nGenerating negatives...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "  0%|          | 3446/2682603 [12:48<165:53:25,  4.49it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/3555291496.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpos_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mneg_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_items\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpos_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mneg_items\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRanker\n",
        "\n",
        "# ========== 1. LOAD DATA ==========\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "# ========== 2. FILTER ACTIVE USERS ==========\n",
        "# –í–æ–∑—å–º–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (—É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ 10-20 —Ä–∞–∑)\n",
        "user_activity = df['user_id'].value_counts()\n",
        "active_users = user_activity.head(250_000).index  # –º–æ–∂–Ω–æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å\n",
        "df = df[df['user_id'].isin(active_users)]\n",
        "\n",
        "print(f\"Using {len(active_users)} active users, {len(df)} interactions\")\n",
        "\n",
        "# ========== 3. FEATURE ENGINEERING ==========\n",
        "item_pop = df['item_id'].value_counts()\n",
        "df['item_popularity'] = np.log1p(df['item_id'].map(item_pop))\n",
        "user_clicks = df['user_id'].value_counts()\n",
        "df['user_clicks'] = np.log1p(df['user_id'].map(user_clicks))\n",
        "df['recency'] = df['date'].max() - df['date']\n",
        "\n",
        "features = df.groupby(['user_id', 'item_id'], as_index=False).agg({\n",
        "    'recency': 'min',\n",
        "    'item_popularity': 'mean',\n",
        "    'user_clicks': 'mean',\n",
        "    'date': 'max'\n",
        "})\n",
        "features['label'] = 1\n",
        "\n",
        "# ========== 4. FAST NEGATIVE SAMPLING ==========\n",
        "# –ë–µ—Ä—ë–º top-20000 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –∏ —Å–ª—É—á–∞–π–Ω–æ –Ω–∞–∑–Ω–∞—á–∞–µ–º –∫–∞–∫ –Ω–µ–≥–∞—Ç–∏–≤—ã\n",
        "top_items = item_pop.head(20_000).index\n",
        "n_neg = 2  # –Ω–∞ –∫–∞–∂–¥—ã–π –ø–æ–∑–∏—Ç–∏–≤ –¥–æ–±–∞–≤–∏–º 2 –Ω–µ–≥–∞—Ç–∏–≤–∞\n",
        "\n",
        "user_ids = features['user_id'].unique()\n",
        "neg_samples = pd.DataFrame({\n",
        "    'user_id': np.repeat(user_ids, n_neg),\n",
        "    'item_id': np.random.choice(top_items, size=len(user_ids) * n_neg)\n",
        "})\n",
        "neg_samples['label'] = 0\n",
        "\n",
        "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–ª–∏–∫–∞–º–∏\n",
        "train_df = pd.concat([features[['user_id', 'item_id', 'recency', 'item_popularity', 'user_clicks', 'label']], neg_samples])\n",
        "train_df.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)\n",
        "\n",
        "# ========== 5. PREPARE DATA ==========\n",
        "X = train_df[['recency', 'item_popularity', 'user_clicks']]\n",
        "y = train_df['label']\n",
        "group = train_df.groupby('user_id').size().to_numpy()\n",
        "\n",
        "# ========== 6. TRAIN LIGHTGBM RANKER ==========\n",
        "print(\"Training model...\")\n",
        "model = LGBMRanker(\n",
        "    objective='lambdarank',\n",
        "    metric='map',\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=100,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X, y, group=group)\n",
        "\n",
        "# ========== 7. FAST RECOMMENDATION GENERATION ==========\n",
        "print(\"Generating predictions...\")\n",
        "popular_items = item_pop.head(500).index.tolist()  # –æ–≥—Ä–∞–Ω–∏—á–∏–º top-500 —Ç–æ–≤–∞—Ä–æ–≤\n",
        "preds = []\n",
        "\n",
        "for uid in sample['user_id']:\n",
        "    if uid not in active_users:\n",
        "        # fallback ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã\n",
        "        preds.append({'user_id': uid, 'item_id': ' '.join(map(str, popular_items[:20]))})\n",
        "        continue\n",
        "\n",
        "    user_clicks_val = np.log1p(user_clicks.get(uid, 1))\n",
        "    cand = pd.DataFrame({\n",
        "        'item_id': popular_items,\n",
        "        'user_id': uid,\n",
        "        'recency': 0,\n",
        "        'user_clicks': user_clicks_val,\n",
        "        'item_popularity': np.log1p(item_pop.loc[popular_items].values)\n",
        "    })\n",
        "\n",
        "    cand['score'] = model.predict(cand[['recency', 'item_popularity', 'user_clicks']])\n",
        "    top20 = cand.nlargest(20, 'score')['item_id'].tolist()\n",
        "    preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "submission = pd.DataFrame(preds)\n",
        "submission.to_csv('submission_fast.csv', index=False)\n",
        "print(\"‚úÖ Done! Saved as submission_fast.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.337784Z",
          "iopub.status.idle": "2025-10-15T11:55:39.338025Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.337903Z",
          "shell.execute_reply": "2025-10-15T11:55:39.337913Z"
        },
        "id": "jK5neLHMbgdg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import gc\n",
        "from lightgbm import LGBMRanker\n",
        "from xgboost import XGBRanker\n",
        "from catboost import CatBoostRanker\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. LOAD DATA ==========\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "# ========== 2. REDUCE USERS FOR SPEED ==========\n",
        "user_activity = df['user_id'].value_counts()\n",
        "active_users = user_activity.head(250_000).index\n",
        "df = df[df['user_id'].isin(active_users)]\n",
        "print(f\"Using {len(active_users)} active users, {len(df)} interactions\")\n",
        "\n",
        "# ========== 3. FEATURE ENGINEERING ==========\n",
        "item_pop = df['item_id'].value_counts()\n",
        "user_clicks = df['user_id'].value_counts()\n",
        "\n",
        "df['item_popularity'] = np.log1p(df['item_id'].map(item_pop))\n",
        "df['user_clicks'] = np.log1p(df['user_id'].map(user_clicks))\n",
        "df['recency'] = df['date'].max() - df['date']\n",
        "\n",
        "features = df.groupby(['user_id', 'item_id'], as_index=False).agg({\n",
        "    'recency': 'min',\n",
        "    'item_popularity': 'mean',\n",
        "    'user_clicks': 'mean',\n",
        "    'date': 'max'\n",
        "})\n",
        "features['label'] = 1\n",
        "\n",
        "# ========== 4. FAST NEGATIVE SAMPLING ==========\n",
        "top_items = item_pop.head(20_000).index\n",
        "n_neg = 2  # 2 –Ω–µ–≥–∞—Ç–∏–≤–∞ –Ω–∞ 1 –ø–æ–∑–∏—Ç–∏–≤\n",
        "\n",
        "user_ids = features['user_id'].unique()\n",
        "neg_samples = pd.DataFrame({\n",
        "    'user_id': np.repeat(user_ids, n_neg),\n",
        "    'item_id': np.random.choice(top_items, size=len(user_ids) * n_neg)\n",
        "})\n",
        "neg_samples['label'] = 0\n",
        "\n",
        "train_df = pd.concat([\n",
        "    features[['user_id', 'item_id', 'recency', 'item_popularity', 'user_clicks', 'label']],\n",
        "    neg_samples\n",
        "], ignore_index=True)\n",
        "\n",
        "train_df.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)\n",
        "train_df.fillna(0, inplace=True)\n",
        "gc.collect()\n",
        "\n",
        "# ========== 5. PREPARE DATA ==========\n",
        "X = train_df[['recency', 'item_popularity', 'user_clicks']].values\n",
        "y = train_df['label'].values\n",
        "group = train_df.groupby('user_id').size().to_numpy()\n",
        "\n",
        "# ========== 6. TRAIN MODELS ==========\n",
        "models = {}\n",
        "\n",
        "# --- LightGBM ---\n",
        "\n",
        "\n",
        "# --- XGBoost ---\n",
        "print(\"\\nüîµ Training XGBoost Ranker...\")\n",
        "start = time.time()\n",
        "xgb = XGBRanker(\n",
        "    objective='rank:pairwise',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist',\n",
        "    n_jobs=-1,\n",
        ")\n",
        "xgb.fit(X, y, group=group)\n",
        "models['XGBoost'] = xgb\n",
        "print(f\"‚úÖ XGBoost trained in {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# --- CatBoost ---\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# ========== 7. GENERATE PREDICTIONS ==========\n",
        "popular_items = item_pop.head(500).index.tolist()\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nüöÄ Generating predictions for {model_name}...\")\n",
        "    preds = []\n",
        "    t0 = time.time()\n",
        "    total_users = len(sample)\n",
        "    step = max(1, total_users // 100)  # –∫–∞–∂–¥—ã–µ 1% ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞\n",
        "\n",
        "    for i, uid in enumerate(sample['user_id'], start=1):\n",
        "        if uid not in active_users:\n",
        "            preds.append({'user_id': uid, 'item_id': ' '.join(map(str, popular_items[:20]))})\n",
        "            continue\n",
        "\n",
        "        user_clicks_val = np.log1p(user_clicks.get(uid, 1))\n",
        "        cand = pd.DataFrame({\n",
        "            'item_id': popular_items,\n",
        "            'user_id': uid,\n",
        "            'recency': 0,\n",
        "            'user_clicks': user_clicks_val,\n",
        "            'item_popularity': np.log1p(item_pop.loc[popular_items].values)\n",
        "        })\n",
        "\n",
        "        X_pred = cand[['recency', 'item_popularity', 'user_clicks']].values\n",
        "        cand['score'] = model.predict(X_pred)\n",
        "        top20 = cand.nlargest(20, 'score')['item_id'].tolist()\n",
        "        preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "        if i % step == 0:\n",
        "            done = (i / total_users) * 100\n",
        "            elapsed = (time.time() - t0) / 60\n",
        "            print(f\"{done:.1f}% done ({elapsed:.1f} min elapsed)\")\n",
        "\n",
        "    submission = pd.DataFrame(preds)\n",
        "    filename = f'submission_{model_name.lower()}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "    print(f\"‚úÖ Saved {filename} | Total time: {(time.time()-t0)/60:.1f} min\")\n",
        "\n",
        "print(\"\\nüéØ All models finished and submissions saved.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.339159Z",
          "iopub.status.idle": "2025-10-15T11:55:39.339384Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.33928Z",
          "shell.execute_reply": "2025-10-15T11:55:39.33929Z"
        },
        "id": "ebYfwIlAbgdg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "popular_items = item_pop.head(20).index.tolist()  # top-200 –≤–º–µ—Å—Ç–æ 500\n",
        "preds = []\n",
        "\n",
        "for uid in tqdm(sample['user_id'], desc=\"Generating predictions\"):\n",
        "    if uid not in active_users:\n",
        "        # fallback ‚Äî —Ç–æ–ª—å–∫–æ —Ç–æ–ø-20 —Ç–æ–≤–∞—Ä–æ–≤\n",
        "        preds.append({'user_id': uid, 'item_id': ' '.join(map(str, popular_items[:20]))})\n",
        "        continue\n",
        "\n",
        "    user_clicks_val = np.log1p(user_clicks.get(uid, 1))\n",
        "    cand = pd.DataFrame({\n",
        "        'item_id': popular_items,\n",
        "        'user_id': uid,\n",
        "        'recency': 0,\n",
        "        'user_clicks': user_clicks_val,\n",
        "        'item_popularity': np.log1p(item_pop.loc[popular_items].values)\n",
        "    })\n",
        "\n",
        "    X_pred = cand[['recency', 'item_popularity', 'user_clicks']].values\n",
        "    cand['score'] = model.predict(X_pred)\n",
        "    top20 = cand.nlargest(20, 'score')['item_id'].tolist()\n",
        "    preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "submission = pd.DataFrame(preds)\n",
        "submission.to_csv('submission_small.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.342964Z",
          "iopub.status.idle": "2025-10-15T11:55:39.343237Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.343121Z",
          "shell.execute_reply": "2025-10-15T11:55:39.343132Z"
        },
        "id": "JrxvUe0Ibgdh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.344373Z",
          "iopub.status.idle": "2025-10-15T11:55:39.344695Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.344533Z",
          "shell.execute_reply": "2025-10-15T11:55:39.344547Z"
        },
        "id": "y6hW205xbgdi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–æ–ª—å—à–æ–π —Å–∞–±–º–∏—Ç\n",
        "submission = pd.read_csv('/kaggle/working/submission_small.csv')\n",
        "\n",
        "# –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ç–æ–ø-20 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "def top20_items(item_str):\n",
        "    items = item_str.split()\n",
        "    return ' '.join(items[:20])\n",
        "\n",
        "submission['item_id'] = submission['item_id'].astype(str).apply(top20_items)\n",
        "\n",
        "# –ü—Ä–∏–≤–æ–¥–∏–º –∫ int, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ª–∏—à–Ω–∏—Ö –∑–Ω–∞–∫–æ–≤\n",
        "submission['item_id'] = submission['item_id'].apply(lambda x: ' '.join(map(str, map(int, x.split()))))\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ –∏–Ω–¥–µ–∫—Å–∞\n",
        "submission.to_csv('submission_small.csv.gz', index=False, compression='gzip')\n",
        "\n",
        "print(\"‚úÖ –§–∞–π–ª —É–º–µ–Ω—å—à–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–∫ submission_small.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.345488Z",
          "iopub.status.idle": "2025-10-15T11:55:39.345692Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.345594Z",
          "shell.execute_reply": "2025-10-15T11:55:39.345603Z"
        },
        "id": "e0X-EgUkbgdi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "\n",
        "print(train.head())\n",
        "print(sample.head())\n",
        "# –¢–æ–ø-1000 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤\n",
        "top_items = train['item_id'].value_counts().head(500).index.tolist()\n",
        "\n",
        "# –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∫–ª–∏–∫–∏ –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "last_clicks = train.groupby('user_id')['item_id'].apply(lambda x: x.tolist()[-20:])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.346542Z",
          "iopub.status.idle": "2025-10-15T11:55:39.346831Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.346655Z",
          "shell.execute_reply": "2025-10-15T11:55:39.34667Z"
        },
        "id": "ICniTvaRbgdj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "user_pos_items = train.groupby('user_id')['item_id'].apply(set).to_dict()\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for user, pos_items in user_pos_items.items():\n",
        "    for item in pos_items:\n",
        "        #print(user,pos_items)\n",
        "        neg_item = random.choice([i for i in top_items if i not in pos_items])\n",
        "        pairs.append([user, item, neg_item])\n",
        "        print(pairs)\n",
        "\n",
        "pairs = pd.DataFrame(pairs, columns=['user_id', 'pos_item', 'neg_item'])\n",
        "print(pairs.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:16:08.407965Z",
          "iopub.execute_input": "2025-10-15T11:16:08.408345Z"
        },
        "id": "gf1LAR0Ubgdk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = df['user_id'].values\n",
        "        self.pos_items = df['pos_item'].values\n",
        "        self.neg_items = df['neg_item'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.pos_items[idx], self.neg_items[idx]\n",
        "\n",
        "class RankNetModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=32):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(n_items, emb_size)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        u_e = self.user_emb(u)\n",
        "        i_e = self.item_emb(i)\n",
        "        j_e = self.item_emb(j)\n",
        "        # score difference\n",
        "        x = (i_e - j_e) * u_e\n",
        "        return torch.sum(x, dim=1)\n",
        "\n",
        "# –ü—Å–µ–≤–¥–æ-–ø—Ä–æ–Ω—É–º–µ—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ç–æ–≤–∞—Ä—ã\n",
        "user2id = {u:i for i,u in enumerate(train['user_id'].unique())}\n",
        "item2id = {i:i for i,i in enumerate(train['item_id'].unique())}\n",
        "\n",
        "pairs['user_id'] = pairs['user_id'].map(user2id)\n",
        "pairs['pos_item'] = pairs['pos_item'].map(item2id)\n",
        "pairs['neg_item'] = pairs['neg_item'].map(item2id)\n",
        "\n",
        "dataset = PairDataset(pairs)\n",
        "loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = RankNetModel(len(user2id), len(item2id), emb_size=32).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "V6lIS3Rkbgdl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):  # –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –¥–ª—è baseline\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for u, i, j in loader:\n",
        "        u, i, j = u.to(device), i.to(device), j.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # label = 1 –¥–ª—è pos>neg\n",
        "        scores = model(u, i, j)\n",
        "        loss = criterion(scores, torch.ones_like(scores))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YHX3bswTbgdl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "topk = 20\n",
        "preds = []\n",
        "\n",
        "for user in train['user_id'].unique():\n",
        "    u_id = torch.tensor([user2id[user]]*len(item2id)).to(device)\n",
        "    items = torch.tensor(list(range(len(item2id)))).to(device)\n",
        "    with torch.no_grad():\n",
        "        scores = model.user_emb(u_id) * model.item_emb(items)\n",
        "        scores = scores.sum(dim=1)\n",
        "    top_items_idx = torch.topk(scores, topk).indices.cpu().numpy()\n",
        "    top_items_ids = [list(item2id.keys())[i] for i in top_items_idx]\n",
        "    preds.append([user] + top_items_ids)\n",
        "\n",
        "submission = pd.DataFrame(preds, columns=sample.columns)\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8Ldj4Yp9bgdl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü–æ–ª–Ω—ã–π/–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä (–∑–∞–º–µ–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —á–∞—Å—Ç–µ–π –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ========== –ó–ê–ì–†–£–ó–ö–ê ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')  # –≤–∞—à train\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')  # –æ–±—Ä–∞–∑–µ—Ü —Å–∞–±–º–∏—Ç–∞ (–¥–ª–∏–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)\n",
        "# –ü–æ–ª—É—á–∏–º —Å–ø–∏—Å–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å (–≤ sample)\n",
        "test_users = sample['user_id'].unique()\n",
        "print(\"users in sample:\", len(test_users))\n",
        "\n",
        "# ========== –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï –ö–ê–ù–î–ò–î–ê–¢–û–í ==========\n",
        "# –í–æ–∑—å–º—ë–º —Ç–æ–ø-N –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å N)\n",
        "TOP_N = 1000\n",
        "top_items = train['item_id'].value_counts().head(TOP_N).index.tolist()\n",
        "\n",
        "# –°–ª–æ–≤–∞—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–º–µ—Ç–æ–≤\n",
        "item_list = top_items\n",
        "item2id = {item: idx for idx, item in enumerate(item_list)}\n",
        "id2item = {idx: item for item, idx in item2id.items()}\n",
        "\n",
        "# –°–ª–æ–≤–∞—Ä—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ç–æ–ª—å–∫–æ –∏–∑ top_items)\n",
        "user_pos_items_full = train.groupby('user_id')['item_id'].apply(list).to_dict()\n",
        "user_pos_items = {u: set([i for i in lst if i in item2id]) for u, lst in user_pos_items_full.items()}\n",
        "\n",
        "# ========== –°–û–ó–î–ê–ù–ò–ï –ü–ê–† –î–õ–Ø PAIRWISE –û–ë–£–ß–ï–ù–ò–Ø (RankNet) ==========\n",
        "pairs = []\n",
        "for user, pos_set in user_pos_items.items():\n",
        "    if not pos_set:\n",
        "        continue\n",
        "    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –ø–æ–¥–±–∏—Ä–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–∑ top_items\n",
        "    for pos in pos_set:\n",
        "        # negative ‚Äî —Å–ª—É—á–∞–π–Ω—ã–π item –∏–∑ top_items, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "        neg_candidates = [it for it in item_list if it not in pos_set]\n",
        "        if not neg_candidates:\n",
        "            continue\n",
        "        neg = random.choice(neg_candidates)\n",
        "        pairs.append((user, pos, neg))\n",
        "\n",
        "pairs_df = pd.DataFrame(pairs, columns=['user_id', 'pos_item', 'neg_item'])\n",
        "# –ü–µ—Ä–µ–Ω—É–º–µ—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ id\n",
        "user_list = list({u for u in pairs_df['user_id'].unique()}.union(set(test_users)))\n",
        "user2id = {u: idx for idx, u in enumerate(user_list)}\n",
        "id2user = {idx: u for u, idx in user2id.items()}\n",
        "\n",
        "pairs_df['u_id'] = pairs_df['user_id'].map(user2id)\n",
        "pairs_df['pos_id'] = pairs_df['pos_item'].map(item2id)\n",
        "pairs_df['neg_id'] = pairs_df['neg_item'].map(item2id)\n",
        "\n",
        "# –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–∞—Ä—ã, –≥–¥–µ pos –∏–ª–∏ neg –Ω–µ –≤ item2id (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)\n",
        "pairs_df = pairs_df.dropna(subset=['pos_id', 'neg_id']).astype({'u_id':int,'pos_id':int,'neg_id':int})\n",
        "\n",
        "# ========== DATASET / DATALOADER ==========\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.u = df['u_id'].values\n",
        "        self.i = df['pos_id'].values\n",
        "        self.j = df['neg_id'].values\n",
        "    def __len__(self):\n",
        "        return len(self.u)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.u[idx], self.i[idx], self.j[idx]\n",
        "\n",
        "dataset = PairDataset(pairs_df)\n",
        "loader = DataLoader(dataset, batch_size=2048, shuffle=True, num_workers=2)\n",
        "\n",
        "# ========== –ú–û–î–ï–õ–¨ (RankNet-–ø–æ–¥–æ–±–Ω–∞—è) ==========\n",
        "class RankNetModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=64):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(n_items, emb_size)\n",
        "        # –Ω–µ–±–æ–ª—å—à–æ–π MLP –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ —Ä–∞–∑–ª–∏—á–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "        self.out = nn.Linear(emb_size, 1, bias=False)  # –º–æ–∂–Ω–æ —É–ø—Ä–æ—Å—Ç–∏—Ç—å/—É—Å–ª–æ–∂–Ω–∏—Ç—å\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        u_e = self.user_emb(u)             # (B, E)\n",
        "        i_e = self.item_emb(i)             # (B, E)\n",
        "        j_e = self.item_emb(j)             # (B, E)\n",
        "        # score = dot(user, item)\n",
        "        s_i = (u_e * i_e).sum(dim=1)       # (B,)\n",
        "        s_j = (u_e * j_e).sum(dim=1)       # (B,)\n",
        "        x = s_i - s_j                      # (B,)\n",
        "        return x\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = RankNetModel(n_users=len(user2id), n_items=len(item2id), emb_size=64).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:56:09.152982Z",
          "iopub.execute_input": "2025-10-15T11:56:09.153777Z",
          "iopub.status.idle": "2025-10-15T12:04:10.853273Z",
          "shell.execute_reply.started": "2025-10-15T11:56:09.153753Z",
          "shell.execute_reply": "2025-10-15T12:04:10.852418Z"
        },
        "id": "nv5zygFJbgdl",
        "outputId": "1705bced-28b3-453f-8f8e-0c9b61562e3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "users in sample: 293230\nEpoch 1/3  avg loss: 3.942721\nEpoch 2/3  avg loss: 2.414918\nEpoch 3/3  avg loss: 1.482728\nSaved submission.csv, rows: 5864600\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== –û–ë–£–ß–ï–ù–ò–ï (–Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –¥–ª—è baseline) ==========\n",
        "EPOCHS = 16\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for u, i, j in loader:\n",
        "        u = u.to(device).long()\n",
        "        i = i.to(device).long()\n",
        "        j = j.to(device).long()\n",
        "        opt.zero_grad()\n",
        "        logits = model(u, i, j)\n",
        "        loss = criterion(logits, torch.ones_like(logits, device=device))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}  avg loss: {total_loss/len(loader):.6f}\")\n",
        "\n",
        "# ========== –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –î–õ–Ø users –∏–∑ sample (top-20) ==========\n",
        "model.eval()\n",
        "TOPK = 20\n",
        "# Prepare item tensors once\n",
        "all_item_ids = torch.arange(len(item2id), device=device).long()\n",
        "all_item_emb = model.item_emb(all_item_ids)  # (N_items, E)\n",
        "\n",
        "submission_rows = []\n",
        "with torch.no_grad():\n",
        "    for u in test_users:\n",
        "        # map user to internal id; –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –±—ã–ª –≤ train pairs, –¥–æ–±–∞–≤–∏–º –Ω–æ–≤—ã–π id (–µ—Å–ª–∏ –Ω–µ—Ç ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏)\n",
        "        if u in user2id:\n",
        "            u_id = torch.tensor([user2id[u]], device=device).long()\n",
        "            u_emb = model.user_emb(u_id)  # (1, E)\n",
        "            # –≤—ã—á–∏—Å–ª–∏–º —Å–∫–æ—Ä –¥–ª—è –≤—Å–µ—Ö candidate items: dot(u_emb, all_item_emb)\n",
        "            scores = (u_emb @ all_item_emb.t()).squeeze(0)  # (N_items,)\n",
        "            topk_idx = torch.topk(scores, min(TOPK, scores.size(0))).indices.cpu().numpy().tolist()\n",
        "            top_items_pred = [id2item[idx] for idx in topk_idx]\n",
        "        else:\n",
        "            # fallback: –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω—É—Ç—å —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–æ–ø-N (–≤ –ø–æ—Ä—è–¥–∫–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏)\n",
        "            top_items_pred = item_list[:TOPK]\n",
        "\n",
        "        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —É–∂–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–≤—à–∏—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "        # –ù–æ –¥–ª—è baseline ‚Äî –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å; –º–æ–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ user_pos_items_full.\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –¥–ª–∏–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π item\n",
        "        for it in top_items_pred:\n",
        "            submission_rows.append((u, it))\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–º sample\n",
        "sub_df = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv, rows:\", len(sub_df))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T12:47:38.314565Z",
          "iopub.execute_input": "2025-10-15T12:47:38.31536Z",
          "iopub.status.idle": "2025-10-15T12:59:04.226688Z",
          "shell.execute_reply.started": "2025-10-15T12:47:38.315329Z",
          "shell.execute_reply": "2025-10-15T12:59:04.225704Z"
        },
        "id": "JrVIPbE2bgdm",
        "outputId": "9bb3f483-a9b3-4b02-dd8f-cd1aaca80265"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/16  avg loss: 0.003568\nEpoch 2/16  avg loss: 0.001962\nEpoch 3/16  avg loss: 0.001055\nEpoch 4/16  avg loss: 0.000557\nEpoch 5/16  avg loss: 0.000293\nEpoch 6/16  avg loss: 0.000153\nEpoch 7/16  avg loss: 0.000080\nEpoch 8/16  avg loss: 0.000042\nEpoch 9/16  avg loss: 0.000022\nEpoch 10/16  avg loss: 0.000012\nEpoch 11/16  avg loss: 0.000006\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/1623722598.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS}  avg loss: {total_loss/len(loader):.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv, rows:\", len(sub_df))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T12:59:09.669192Z",
          "iopub.execute_input": "2025-10-15T12:59:09.669473Z",
          "iopub.status.idle": "2025-10-15T12:59:25.464021Z",
          "shell.execute_reply.started": "2025-10-15T12:59:09.669448Z",
          "shell.execute_reply": "2025-10-15T12:59:25.46325Z"
        },
        "id": "WloSSNfubgdm",
        "outputId": "6270f56a-8653-464e-9fc0-6380d7d18ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Saved submission.csv, rows: 5864600\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load data ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "test_users = sample['user_id'].unique()\n",
        "\n",
        "# ========== 2. Feature generation ==========\n",
        "print(\"Building simple features...\")\n",
        "\n",
        "# Item popularity\n",
        "item_pop = train['item_id'].value_counts().rename('item_pop')\n",
        "\n",
        "# User activity\n",
        "user_freq = train['user_id'].value_counts().rename('user_freq')\n",
        "\n",
        "# Recency feature: max date per user/item\n",
        "user_last = train.groupby('user_id')['date'].max().rename('user_last')\n",
        "item_last = train.groupby('item_id')['date'].max().rename('item_last')\n",
        "\n",
        "train = train.join(item_pop, on='item_id')\n",
        "train = train.join(user_freq, on='user_id')\n",
        "train = train.join(user_last, on='user_id')\n",
        "train = train.join(item_last, on='item_id')\n",
        "train['recency'] = train['user_last'] - train['date']\n",
        "\n",
        "# ========== 3. Build candidates (top popular + recent clicks per user) ==========\n",
        "top_items = train['item_id'].value_counts().head(1000).index.tolist()\n",
        "user_recent = train.groupby('user_id')['item_id'].apply(lambda x: x.tail(10).tolist())\n",
        "\n",
        "pairs = []\n",
        "for user, items in tqdm(user_recent.items()):\n",
        "    for pos in items:\n",
        "        neg = np.random.choice(top_items)\n",
        "        pairs.append((user, pos, 1))  # positive\n",
        "        pairs.append((user, neg, 0))  # negative\n",
        "\n",
        "pairs = pd.DataFrame(pairs, columns=['user_id', 'item_id', 'label'])\n",
        "pairs = pairs.join(item_pop, on='item_id')\n",
        "pairs = pairs.join(item_last, on='item_id')\n",
        "pairs = pairs.join(user_freq, on='user_id')\n",
        "pairs = pairs.join(user_last, on='user_id')\n",
        "pairs['recency'] = pairs['user_last'] - pairs['item_last']\n",
        "pairs = pairs.fillna(0)\n",
        "\n",
        "# ========== 4. Prepare for XGBoost ==========\n",
        "features = ['item_pop', 'item_last', 'user_freq', 'user_last', 'recency']\n",
        "X = pairs[features].values\n",
        "y = pairs['label'].values\n",
        "\n",
        "# group sizes (number of items per user)\n",
        "group = pairs.groupby('user_id').size().values\n",
        "\n",
        "ranker = xgb.XGBRanker(\n",
        "    objective='rank:pairwise',\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    n_estimators=100,\n",
        "    tree_method='hist',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training XGBoost Ranker...\")\n",
        "ranker.fit(X, y, group=group)\n",
        "\n",
        "# ========== 5. Prediction ==========\n",
        "print(\"Generating recommendations...\")\n",
        "sub_rows = []\n",
        "for user in tqdm(test_users):\n",
        "    # —Å–æ–∑–¥–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –∫–ª–∏–∫–∏ + —Ç–æ–ø)\n",
        "    cands = list(set(user_recent.get(user, [])) | set(top_items))\n",
        "    df = pd.DataFrame({'user_id': user, 'item_id': cands})\n",
        "    df = df.join(item_pop, on='item_id')\n",
        "    df = df.join(item_last, on='item_id')\n",
        "    df = df.join(user_freq, on='user_id')\n",
        "    df = df.join(user_last, on='user_id')\n",
        "    df['recency'] = df['user_last'] - df['item_last']\n",
        "    df = df.fillna(0)\n",
        "    X_test = df[features].values\n",
        "    preds = ranker.predict(X_test)\n",
        "    df['pred'] = preds\n",
        "    top20 = df.sort_values('pred', ascending=False).head(20)\n",
        "    for item in top20['item_id'].tolist():\n",
        "        sub_rows.append((user, item))\n",
        "\n",
        "sub = pd.DataFrame(sub_rows, columns=['user_id', 'item_id'])\n",
        "sub.to_csv('submission_xgboost.csv', index=False)\n",
        "print(\"‚úÖ submission_xgboost.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T13:15:29.885833Z",
          "iopub.execute_input": "2025-10-15T13:15:29.886448Z",
          "iopub.status.idle": "2025-10-15T14:04:22.031196Z",
          "shell.execute_reply.started": "2025-10-15T13:15:29.886422Z",
          "shell.execute_reply": "2025-10-15T14:04:22.029821Z"
        },
        "id": "A9166nyHbgdm",
        "outputId": "dc02b7f1-ac68-4503-a3aa-823f4ec0f9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Building simple features...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2682603it [09:39, 4628.76it/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training XGBoost Ranker...\nGenerating recommendations...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "  1%|          | 2722/293230 [29:18<52:08:34,  1.55it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/1973811664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recency'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_last'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_last'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[1;32m  10755\u001b[0m                     \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10756\u001b[0m                 )\n\u001b[0;32m> 10757\u001b[0;31m             return merge(\n\u001b[0m\u001b[1;32m  10758\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10759\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         )\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         result = self._reindex_and_concat(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             join_index, left_indexer, right_indexer = _left_join_on_index(\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0mleft_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_left_join_on_index\u001b[0;34m(left_ax, right_ax, join_keys, sort)\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0mleft_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factorize_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m     left_indexer, right_indexer = libjoin.left_outer_join(\n\u001b[0m\u001b[1;32m   2386\u001b[0m         \u001b[0mleft_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load and sort data ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "test_users = sample['user_id'].unique()\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ –¥–∞—Ç–µ\n",
        "train = train.sort_values(['user_id', 'date'])\n",
        "\n",
        "# ========== 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ==========\n",
        "max_len = 20  # –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏\n",
        "user_sequences = (\n",
        "    train.groupby('user_id')['item_id']\n",
        "    .apply(lambda x: x.tolist()[-max_len:])\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# –°–ª–æ–≤–∞—Ä—å item_id ‚Üí –∏–Ω–¥–µ–∫—Å\n",
        "item_vocab = {it: idx+1 for idx, it in enumerate(train['item_id'].unique())}  # +1 –¥–ª—è PAD=0\n",
        "id2item = {v:k for k,v in item_vocab.items()}\n",
        "n_items = len(item_vocab) + 1\n",
        "\n",
        "# ========== 3. Dataset ==========\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, user_seq):\n",
        "        self.users = list(user_seq.keys())\n",
        "        self.seqs = list(user_seq.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq  # left padding\n",
        "        target = seq[-1]  # last item\n",
        "        return torch.tensor(seq[:-1]), torch.tensor(target)\n",
        "\n",
        "train_ds = SeqDataset(user_sequences)\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "# ========== 4. SASRec-–ø–æ–¥–æ–±–Ω–∞—è –º–æ–¥–µ–ª—å ==========\n",
        "class TransformerRec(nn.Module):\n",
        "    def __init__(self, num_items, d_model=512, nhead=8, num_layers=6, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.num_items = num_items\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.item_emb = nn.Embedding(num_items + 1, d_model, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            batch_first=False,  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (L, B, D)\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.proj = nn.Linear(d_model, num_items + 1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.item_emb.weight, mean=0, std=0.01)\n",
        "        nn.init.normal_(self.pos_emb.weight, mean=0, std=0.01)\n",
        "        nn.init.xavier_normal_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        batch_size, seq_len = seq.size()\n",
        "\n",
        "        pos = torch.arange(seq_len, device=seq.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        x = self.item_emb(seq) + self.pos_emb(pos)  # (B, L, D)\n",
        "        x = x.permute(1, 0, 2)  # (L, B, D)\n",
        "\n",
        "        out = self.encoder(x)  # (L, B, D)\n",
        "        out = out.permute(1, 0, 2)  # (B, L, D)\n",
        "\n",
        "        logits = self.proj(out)  # (B, L, num_items + 1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = TransformerRec(n_items=n_items).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# ========== 5. –û–±—É—á–µ–Ω–∏–µ ==========\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for seq, tgt in train_dl:\n",
        "        seq, tgt = seq.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ========== 6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ==========\n",
        "model.eval()\n",
        "TOPK = 20\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user in tqdm(test_users):\n",
        "        seq = user_sequences.get(user, [])\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq\n",
        "        seq_t = torch.tensor(seq[:-1]).unsqueeze(0).to(device)\n",
        "        logits = model(seq_t)\n",
        "        topk_idx = torch.topk(logits, TOPK, dim=1).indices[0].cpu().numpy()\n",
        "        top_items = [id2item[i] for i in topk_idx if i in id2item]\n",
        "        for it in top_items:\n",
        "            submission_rows.append((user, it))\n",
        "\n",
        "sub = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub.to_csv('submission_transformer.csv', index=False)\n",
        "print(\"‚úÖ submission_transformer.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T14:30:15.215263Z",
          "iopub.execute_input": "2025-10-15T14:30:15.215994Z",
          "iopub.status.idle": "2025-10-15T14:30:52.77293Z",
          "shell.execute_reply.started": "2025-10-15T14:30:15.215967Z",
          "shell.execute_reply": "2025-10-15T14:30:52.771951Z"
        },
        "id": "qWKOECzdbgdm",
        "outputId": "447f8a8c-a32e-488d-8c1c-11be4862f48e"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/930912039.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerRec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TransformerRec.__init__() got an unexpected keyword argument 'n_items'"
          ],
          "ename": "TypeError",
          "evalue": "TransformerRec.__init__() got an unexpected keyword argument 'n_items'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load and sort data ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "test_users = sample['user_id'].unique()\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ –¥–∞—Ç–µ\n",
        "train = train.sort_values(['user_id', 'date'])\n",
        "\n",
        "# ========== 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ==========\n",
        "max_len = 20  # –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏\n",
        "user_sequences = (\n",
        "    train.groupby('user_id')['item_id']\n",
        "    .apply(lambda x: x.tolist()[-max_len:])\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# –°–ª–æ–≤–∞—Ä—å item_id ‚Üí –∏–Ω–¥–µ–∫—Å\n",
        "item_vocab = {it: idx+1 for idx, it in enumerate(train['item_id'].unique())}  # +1 –¥–ª—è PAD=0\n",
        "id2item = {v:k for k,v in item_vocab.items()}\n",
        "n_items = len(item_vocab) + 1\n",
        "\n",
        "# ========== 3. Dataset ==========\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, user_seq):\n",
        "        self.users = list(user_seq.keys())\n",
        "        self.seqs = list(user_seq.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq  # left padding\n",
        "        target = seq[-1]  # last item\n",
        "        return torch.tensor(seq[:-1]), torch.tensor(target)\n",
        "\n",
        "train_ds = SeqDataset(user_sequences)\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "# ========== 4. SASRec-–ø–æ–¥–æ–±–Ω–∞—è –º–æ–¥–µ–ª—å ==========\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, n_items, d_model=64, n_heads=4, n_layers=2, max_len=20, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.item_emb = nn.Embedding(n_items, d_model, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n",
        "            batch_first=True, dropout=dropout, activation='gelu'\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.fc = nn.Linear(d_model, n_items)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, seq):\n",
        "        B, L = seq.shape\n",
        "        pos = torch.arange(L, device=seq.device).unsqueeze(0).expand(B, -1)\n",
        "        x = self.item_emb(seq) + self.pos_emb(pos)\n",
        "        x = self.layernorm(self.dropout(x))\n",
        "\n",
        "        # causal mask: –º–æ–¥–µ–ª—å –Ω–µ –≤–∏–¥–∏—Ç –±—É–¥—É—â–µ–µ\n",
        "        mask = torch.triu(torch.ones(L, L, device=seq.device), diagonal=1).bool()\n",
        "        out = self.encoder(x, mask)\n",
        "        logits = self.fc(out)  # (B, L, n_items)\n",
        "        return logits\n",
        "\n",
        "def compute_loss(logits, seq):\n",
        "    # —Ç–∞—Ä–≥–µ—Ç—ã: —Å–ª–µ–¥—É—é—â–∏–π item –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏\n",
        "    targets = seq[:, 1:]\n",
        "    inputs = logits[:, :-1, :]\n",
        "    loss = F.cross_entropy(\n",
        "        inputs.reshape(-1, inputs.size(-1)),\n",
        "        targets.reshape(-1),\n",
        "        ignore_index=0\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SASRec(n_items=n_items).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# ========== 5. –û–±—É—á–µ–Ω–∏–µ ==========\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for seq, tgt in train_dl:\n",
        "        seq, tgt = seq.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ========== 6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ==========\n",
        "model.eval()\n",
        "TOPK = 20\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user in tqdm(test_users):\n",
        "        seq = user_sequences.get(user, [])\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq\n",
        "        seq_t = torch.tensor(seq[:-1]).unsqueeze(0).to(device)\n",
        "        logits = model(seq_t)\n",
        "        topk_idx = torch.topk(logits, TOPK, dim=1).indices[0].cpu().numpy()\n",
        "        top_items = [id2item[i] for i in topk_idx if i in id2item]\n",
        "        for it in top_items:\n",
        "            submission_rows.append((user, it))\n",
        "\n",
        "sub = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub.to_csv('submission_transformer.csv', index=False)\n",
        "print(\"‚úÖ submission_transformer.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T11:23:51.754163Z",
          "iopub.execute_input": "2025-10-18T11:23:51.754774Z",
          "iopub.status.idle": "2025-10-18T11:24:40.494183Z",
          "shell.execute_reply.started": "2025-10-18T11:23:51.754747Z",
          "shell.execute_reply": "2025-10-18T11:24:40.493054Z"
        },
        "id": "Phay42GTbgdm",
        "outputId": "7541b1d4-e819-4c58-9a15-0d32cf1f0584"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/307201193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 13.42 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.74 GiB is free. Process 2500 has 14.15 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 26.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ],
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 13.42 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.74 GiB is free. Process 2500 has 14.15 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 26.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'my_model.pth')\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T15:38:53.294494Z",
          "iopub.execute_input": "2025-10-17T15:38:53.295294Z",
          "iopub.status.idle": "2025-10-17T15:38:54.178026Z",
          "shell.execute_reply.started": "2025-10-17T15:38:53.295269Z",
          "shell.execute_reply": "2025-10-17T15:38:54.177183Z"
        },
        "id": "Q0yVqL2Ubgdn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# two_tower_recommender.py\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "TRAIN_PATH = '/kaggle/input/reccomend/train_data.pq'\n",
        "SAMPLE_PATH = '/kaggle/input/asddbfd/sample_submission (1).csv'  # –∑–∞–º–µ–Ω–∏—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "OUTPUT_PATH = 'submission_two_tower.csv'\n",
        "\n",
        "EMB_DIM = 64        # —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
        "HIDDEN = 128        # —Ä–∞–∑–º–µ—Ä MLP –≤–Ω—É—Ç—Ä–∏ –±–∞—à–µ–Ω\n",
        "BATCH_SIZE = 4096\n",
        "EPOCHS = 3\n",
        "LR = 1e-3\n",
        "TOP_ITEMS = 2000    # –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (–ø–æ–ø—É–ª—è—Ä–Ω—ã–µ) ‚Äî –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å\n",
        "TOPK = 20\n",
        "NUM_NEG = 1         # –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π\n",
        "SEED = 42\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ----------------- DATA LOAD -----------------\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_parquet(TRAIN_PATH)\n",
        "sample = pd.read_csv(SAMPLE_PATH)\n",
        "\n",
        "# users to predict (preserve order if needed)\n",
        "test_users_ordered = sample['user_id'].values  # –¥–ª–∏–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç likely repeats users; we'll take unique in order\n",
        "_, idx = np.unique(test_users_ordered, return_index=True)\n",
        "test_users = test_users_ordered[np.sort(idx)]\n",
        "\n",
        "# ----------------- CANDIDATE REDUCTION -----------------\n",
        "# —Ç–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ)\n",
        "popular_items = train['item_id'].value_counts().index.tolist()\n",
        "top_items = popular_items[:TOP_ITEMS]\n",
        "\n",
        "# ----------------- ID MAPPINGS -----------------\n",
        "all_users = train['user_id'].unique().tolist()\n",
        "# include test users even if they didn't appear in training pairs mapping ‚Äî but prompt said they do\n",
        "for u in test_users:\n",
        "    if u not in all_users:\n",
        "        all_users.append(u)\n",
        "all_items = list({i for i in train['item_id'].unique() if i in top_items})  # limit items to top_items\n",
        "# ensure top_items are present in item list\n",
        "all_items_set = set(all_items)\n",
        "for it in top_items:\n",
        "    if it not in all_items_set:\n",
        "        all_items.append(it)\n",
        "\n",
        "user2id = {u: idx for idx, u in enumerate(all_users)}\n",
        "id2user = {v:k for k,v in user2id.items()}\n",
        "item2id = {i: idx for idx, i in enumerate(all_items)}\n",
        "id2item = {v:k for k,v in item2id.items()}\n",
        "\n",
        "n_users = len(user2id)\n",
        "n_items = len(item2id)\n",
        "print(f\"n_users={n_users}, n_items={n_items}\")\n",
        "\n",
        "# ----------------- BUILD USER HISTORIES -----------------\n",
        "user_pos = train.groupby('user_id')['item_id'].apply(list).to_dict()\n",
        "# filter histories to items inside item2id (candidates)\n",
        "user_pos_filtered = {u: [i for i in lst if i in item2id] for u, lst in user_pos.items()}\n",
        "\n",
        "# ----------------- TRAINING PAIR DATASET -----------------\n",
        "pairs = []\n",
        "for u, items in user_pos_filtered.items():\n",
        "    if not items:\n",
        "        continue\n",
        "    # each positive item can produce NUM_NEG negative samples\n",
        "    for pos in set(items):\n",
        "        for _ in range(NUM_NEG):\n",
        "            # sample negative from top_items but exclude user's positives\n",
        "            neg = random.choice(top_items)\n",
        "            # if neg not in item2id, skip (could happen if top_items larger than our filtered)\n",
        "            if neg not in item2id:\n",
        "                continue\n",
        "            if neg in items:\n",
        "                continue\n",
        "            pairs.append((u, pos, neg))\n",
        "\n",
        "print(f\"Pairs for training: {len(pairs)}\")\n",
        "\n",
        "pairs_df = pd.DataFrame(pairs, columns=['user', 'pos', 'neg'])\n",
        "pairs_df['u_id'] = pairs_df['user'].map(user2id)\n",
        "pairs_df['pos_id'] = pairs_df['pos'].map(item2id)\n",
        "pairs_df['neg_id'] = pairs_df['neg'].map(item2id)\n",
        "pairs_df = pairs_df.dropna().astype(int)\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.u = df['u_id'].values\n",
        "        self.pos = df['pos_id'].values\n",
        "        self.neg = df['neg_id'].values\n",
        "    def __len__(self):\n",
        "        return len(self.u)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.u[idx], self.pos[idx], self.neg[idx]\n",
        "\n",
        "train_loader = DataLoader(PairDataset(pairs_df), batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ----------------- MODEL -----------------\n",
        "class Tower(nn.Module):\n",
        "    def __init__(self, n_entities, emb_dim, hidden):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(n_entities, emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, emb_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, idx):\n",
        "        x = self.emb(idx)\n",
        "        x = self.mlp(x)\n",
        "        # L2-normalize for stable dot product / cosine-like scoring\n",
        "        x = x / (x.norm(p=2, dim=1, keepdim=True) + 1e-8)\n",
        "        return x\n",
        "\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_dim=64, hidden=128):\n",
        "        super().__init__()\n",
        "        self.user_tower = Tower(n_users, emb_dim, hidden)\n",
        "        self.item_tower = Tower(n_items, emb_dim, hidden)\n",
        "\n",
        "    def forward(self, u_idx, i_idx=None, j_idx=None):\n",
        "        u_emb = self.user_tower(u_idx)  # (B, E)\n",
        "        if i_idx is not None:\n",
        "            i_emb = self.item_tower(i_idx)\n",
        "        else:\n",
        "            i_emb = None\n",
        "        if j_idx is not None:\n",
        "            j_emb = self.item_tower(j_idx)\n",
        "        else:\n",
        "            j_emb = None\n",
        "        return u_emb, i_emb, j_emb\n",
        "\n",
        "model = TwoTowerModel(n_users=n_users, n_items=n_items, emb_dim=EMB_DIM, hidden=HIDDEN).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# BPR-style loss using logits difference + softplus\n",
        "def bpr_loss(u_emb, pos_emb, neg_emb):\n",
        "    # u_emb: (B, E), pos_emb/neg_emb: (B, E)\n",
        "    pos_scores = (u_emb * pos_emb).sum(dim=1)   # (B,)\n",
        "    neg_scores = (u_emb * neg_emb).sum(dim=1)\n",
        "    x = pos_scores - neg_scores\n",
        "    # softplus(-x) encourages pos>neg; equivalently use -log(sigmoid(x))\n",
        "    return torch.nn.functional.softplus(-x).mean()\n",
        "\n",
        "# ----------------- TRAINING LOOP -----------------\n",
        "print(\"Start training...\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        u_ids, pos_ids, neg_ids = batch\n",
        "        u_ids = u_ids.to(device).long()\n",
        "        pos_ids = pos_ids.to(device).long()\n",
        "        neg_ids = neg_ids.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        u_emb, pos_emb, neg_emb = model(u_ids, pos_ids, neg_ids)\n",
        "        loss = bpr_loss(u_emb, pos_emb, neg_emb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} avg loss: {avg_loss:.6f}\")\n",
        "\n",
        "# ----------------- GENERATE RECOMMENDATIONS -----------------\n",
        "# Precompute all item embeddings (for candidate set)\n",
        "print(\"Computing item embeddings...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # item indices 0..n_items-1\n",
        "    all_item_idx = torch.arange(n_items, device=device).long()\n",
        "    item_embs = model.item_tower.emb(all_item_idx)\n",
        "    item_embs = model.item_tower.mlp(item_embs)\n",
        "    item_embs = item_embs / (item_embs.norm(p=2, dim=1, keepdim=True) + 1e-8)  # (n_items, E)\n",
        "    # to cpu for faster topk if GPU memory limited (we'll keep on device to use torch.topk)\n",
        "    item_embs_t = item_embs  # keep on device\n",
        "\n",
        "# We'll produce recommendations for unique test users in order\n",
        "print(\"Generating submission...\")\n",
        "submission_rows = []\n",
        "unique_test_users = test_users  # ordered unique users from earlier\n",
        "\n",
        "batch_size_predict = 1024\n",
        "for i in tqdm(range(0, len(unique_test_users), batch_size_predict)):\n",
        "    batch_users = unique_test_users[i:i+batch_size_predict]\n",
        "    u_idx = []\n",
        "    for u in batch_users:\n",
        "        u_idx.append(user2id.get(u, None))\n",
        "    # if some users missing mapping (unlikely), fallback to popular items\n",
        "    valid_mask = [idx is not None and idx < n_users for idx in u_idx]\n",
        "    # create tensor of user indices (fill invalid with 0)\n",
        "    u_idx_tensor = torch.tensor([x if x is not None else 0 for x in u_idx], device=device).long()\n",
        "    with torch.no_grad():\n",
        "        u_emb_batch = model.user_tower.emb(u_idx_tensor)\n",
        "        u_emb_batch = model.user_tower.mlp(u_emb_batch)\n",
        "        u_emb_batch = u_emb_batch / (u_emb_batch.norm(p=2, dim=1, keepdim=True) + 1e-8)  # (B, E)\n",
        "        # scores = u_emb_batch @ item_embs_t.T  -> (B, n_items)\n",
        "        scores = torch.matmul(u_emb_batch, item_embs_t.t())  # (B, n_items)\n",
        "        topk_vals, topk_idxs = torch.topk(scores, k=min(TOPK, n_items), dim=1)\n",
        "\n",
        "    for bi, uid in enumerate(batch_users):\n",
        "        if not valid_mask[bi]:\n",
        "            # fallback: top popular items\n",
        "            recs = top_items[:TOPK]\n",
        "        else:\n",
        "            idxs = topk_idxs[bi].cpu().numpy().tolist()\n",
        "            recs = [id2item[idx] for idx in idxs]\n",
        "        # append to long-format (one row per recommendation)\n",
        "        for it in recs:\n",
        "            submission_rows.append((uid, it))\n",
        "\n",
        "sub_df = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub_df.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"Saved {OUTPUT_PATH} with {len(sub_df)} rows\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T10:44:02.854317Z",
          "iopub.execute_input": "2025-10-18T10:44:02.854496Z",
          "iopub.status.idle": "2025-10-18T11:16:53.146277Z",
          "execution_failed": "2025-10-18T11:22:22.759Z"
        },
        "id": "sRIJqACKbgdn",
        "outputId": "769430be-0a20-46d2-928d-0d580fd59d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading data...\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_37/4168239699.py\", line None, in <cell line: 0>\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n    start = lineno - 1 - context//2\n            ~~~~~~~^~~\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_37/4168239699.py\", line None, in <cell line: 0>\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n    self.showtraceback(running_compiled_code=True)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(etype,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n    return FormattedTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n    return VerboseTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n    return len(records), 0\n           ^^^^^^^^^^^^\nTypeError: object of type 'NoneType' has no len()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n    start = lineno - 1 - context//2\n            ~~~~~~~^~~\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_37/4168239699.py\", line None, in <cell line: 0>\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n    self.showtraceback(running_compiled_code=True)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(etype,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n    return FormattedTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n    return VerboseTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n    return len(records), 0\n           ^^^^^^^^^^^^\nTypeError: object of type 'NoneType' has no len()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n    self.showtraceback()\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(etype,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n    return FormattedTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n    return VerboseTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n    return len(records), 0\n           ^^^^^^^^^^^^\nTypeError: object of type 'NoneType' has no len()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n    start = lineno - 1 - context//2\n            ~~~~~~~^~~\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}