{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13367894,
          "sourceType": "datasetVersion",
          "datasetId": 8480207
        },
        {
          "sourceId": 13368115,
          "sourceType": "datasetVersion",
          "datasetId": 8480372
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "2 task",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRanker\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "df = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "\n",
        "df['user_clicks'] = df.groupby('user_id')['item_id'].transform('count')\n",
        "df['item_popularity'] = df.groupby('item_id')['user_id'].transform('count')\n",
        "\n",
        "# Normalize popularity\n",
        "df['item_popularity'] = np.log1p(df['item_popularity'])\n",
        "df['user_clicks'] = np.log1p(df['user_clicks'])\n",
        "\n",
        "# Temporal recency feature\n",
        "df['recency'] = df['date'].max() - df['date']\n",
        "\n",
        "# Aggregate to (user_id, item_id)\n",
        "features = df.groupby(['user_id', 'item_id'], as_index=False).agg({\n",
        "    'recency': 'min',\n",
        "    'item_popularity': 'mean',\n",
        "    'user_clicks': 'mean',\n",
        "    'date': 'max'\n",
        "})\n",
        "\n",
        "# Label = 1 (пользователь кликал на товар)\n",
        "features['label'] = 1\n",
        "\n",
        "# ========== 3. NEGATIVE SAMPLING ==========\n",
        "# Для каждого пользователя добавляем 20 случайных не кликнутых товаров\n",
        "unique_items = df['item_id'].unique()\n",
        "neg_samples = []\n",
        "\n",
        "print(\"Generating negatives...\")\n",
        "for uid, grp in tqdm(df.groupby('user_id')):\n",
        "    pos_items = set(grp['item_id'])\n",
        "    neg_items = np.random.choice(list(set(unique_items) - pos_items), size=20, replace=False)\n",
        "    tmp = pd.DataFrame({'user_id': uid, 'item_id': neg_items})\n",
        "    tmp['label'] = 0\n",
        "    neg_samples.append(tmp)\n",
        "\n",
        "neg_df = pd.concat(neg_samples, ignore_index=True)\n",
        "\n",
        "# Merge with features\n",
        "train_df = pd.concat([features[['user_id', 'item_id', 'recency', 'item_popularity', 'user_clicks', 'label']], neg_df])\n",
        "train_df = train_df.fillna(0)\n",
        "\n",
        "# ========== 4. PREPARE DATA FOR LIGHTGBM ==========\n",
        "X = train_df[['recency', 'item_popularity', 'user_clicks']]\n",
        "y = train_df['label']\n",
        "group = train_df.groupby('user_id').size().to_numpy()\n",
        "\n",
        "# ========== 5. TRAIN MODEL ==========\n",
        "print(\"Training LGBMRanker...\")\n",
        "model = LGBMRanker(\n",
        "    objective='lambdarank',\n",
        "    metric='map',\n",
        "    boosting_type='gbdt',\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=200,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X, y, group=group)\n",
        "\n",
        "# ========== 6. GENERATE RECOMMENDATIONS ==========\n",
        "print(\"Generating recommendations...\")\n",
        "user_features = train_df[['user_id']].drop_duplicates()\n",
        "item_features = df[['item_id']].drop_duplicates()\n",
        "\n",
        "preds = []\n",
        "for uid in tqdm(sample['user_id']):\n",
        "    # Берём все известные товары (для ускорения можно ограничить top-500 популярных)\n",
        "    candidates = item_features.copy()\n",
        "    candidates['user_id'] = uid\n",
        "    candidates['recency'] = 0\n",
        "    candidates['user_clicks'] = np.log1p(df.loc[df['user_id'] == uid, 'item_id'].count())\n",
        "    candidates['item_popularity'] = np.log1p(candidates['item_id'].map(df['item_id'].value_counts()).fillna(0))\n",
        "\n",
        "    X_pred = candidates[['recency', 'item_popularity', 'user_clicks']]\n",
        "    candidates['score'] = model.predict(X_pred)\n",
        "    top20 = candidates.nlargest(20, 'score')['item_id'].tolist()\n",
        "    preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "sub = pd.DataFrame(preds)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(\"✅ Saved submission.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:42:08.314384Z",
          "iopub.execute_input": "2025-10-15T11:42:08.314731Z",
          "iopub.status.idle": "2025-10-15T11:55:39.337127Z",
          "shell.execute_reply.started": "2025-10-15T11:42:08.314705Z",
          "shell.execute_reply": "2025-10-15T11:55:39.335549Z"
        },
        "id": "0Xi9tciHbgdf",
        "outputId": "e00b33b6-f989-4ff2-a418-321f357b7560"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading data...\nTrain shape: (8777975, 3), Sample: (5864600, 2)\nGenerating negatives...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "  0%|          | 3446/2682603 [12:48<165:53:25,  4.49it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/3555291496.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpos_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mneg_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_items\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpos_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mneg_items\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRanker\n",
        "\n",
        "# ========== 1. LOAD DATA ==========\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "# ========== 2. FILTER ACTIVE USERS ==========\n",
        "# Возьмем только самых активных пользователей (ускоряет обучение в 10-20 раз)\n",
        "user_activity = df['user_id'].value_counts()\n",
        "active_users = user_activity.head(250_000).index  # можно регулировать\n",
        "df = df[df['user_id'].isin(active_users)]\n",
        "\n",
        "print(f\"Using {len(active_users)} active users, {len(df)} interactions\")\n",
        "\n",
        "# ========== 3. FEATURE ENGINEERING ==========\n",
        "item_pop = df['item_id'].value_counts()\n",
        "df['item_popularity'] = np.log1p(df['item_id'].map(item_pop))\n",
        "user_clicks = df['user_id'].value_counts()\n",
        "df['user_clicks'] = np.log1p(df['user_id'].map(user_clicks))\n",
        "df['recency'] = df['date'].max() - df['date']\n",
        "\n",
        "features = df.groupby(['user_id', 'item_id'], as_index=False).agg({\n",
        "    'recency': 'min',\n",
        "    'item_popularity': 'mean',\n",
        "    'user_clicks': 'mean',\n",
        "    'date': 'max'\n",
        "})\n",
        "features['label'] = 1\n",
        "\n",
        "# ========== 4. FAST NEGATIVE SAMPLING ==========\n",
        "# Берём top-20000 популярных товаров и случайно назначаем как негативы\n",
        "top_items = item_pop.head(20_000).index\n",
        "n_neg = 2  # на каждый позитив добавим 2 негатива\n",
        "\n",
        "user_ids = features['user_id'].unique()\n",
        "neg_samples = pd.DataFrame({\n",
        "    'user_id': np.repeat(user_ids, n_neg),\n",
        "    'item_id': np.random.choice(top_items, size=len(user_ids) * n_neg)\n",
        "})\n",
        "neg_samples['label'] = 0\n",
        "\n",
        "# Объединяем и убираем пересечения с реальными кликами\n",
        "train_df = pd.concat([features[['user_id', 'item_id', 'recency', 'item_popularity', 'user_clicks', 'label']], neg_samples])\n",
        "train_df.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)\n",
        "\n",
        "# ========== 5. PREPARE DATA ==========\n",
        "X = train_df[['recency', 'item_popularity', 'user_clicks']]\n",
        "y = train_df['label']\n",
        "group = train_df.groupby('user_id').size().to_numpy()\n",
        "\n",
        "# ========== 6. TRAIN LIGHTGBM RANKER ==========\n",
        "print(\"Training model...\")\n",
        "model = LGBMRanker(\n",
        "    objective='lambdarank',\n",
        "    metric='map',\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=100,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X, y, group=group)\n",
        "\n",
        "# ========== 7. FAST RECOMMENDATION GENERATION ==========\n",
        "print(\"Generating predictions...\")\n",
        "popular_items = item_pop.head(500).index.tolist()  # ограничим top-500 товаров\n",
        "preds = []\n",
        "\n",
        "for uid in sample['user_id']:\n",
        "    if uid not in active_users:\n",
        "        # fallback — популярные товары\n",
        "        preds.append({'user_id': uid, 'item_id': ' '.join(map(str, popular_items[:20]))})\n",
        "        continue\n",
        "\n",
        "    user_clicks_val = np.log1p(user_clicks.get(uid, 1))\n",
        "    cand = pd.DataFrame({\n",
        "        'item_id': popular_items,\n",
        "        'user_id': uid,\n",
        "        'recency': 0,\n",
        "        'user_clicks': user_clicks_val,\n",
        "        'item_popularity': np.log1p(item_pop.loc[popular_items].values)\n",
        "    })\n",
        "\n",
        "    cand['score'] = model.predict(cand[['recency', 'item_popularity', 'user_clicks']])\n",
        "    top20 = cand.nlargest(20, 'score')['item_id'].tolist()\n",
        "    preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "submission = pd.DataFrame(preds)\n",
        "submission.to_csv('submission_fast.csv', index=False)\n",
        "print(\"✅ Done! Saved as submission_fast.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.337784Z",
          "iopub.status.idle": "2025-10-15T11:55:39.338025Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.337903Z",
          "shell.execute_reply": "2025-10-15T11:55:39.337913Z"
        },
        "id": "jK5neLHMbgdg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import gc\n",
        "from lightgbm import LGBMRanker\n",
        "from xgboost import XGBRanker\n",
        "from catboost import CatBoostRanker\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. LOAD DATA ==========\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "# ========== 2. REDUCE USERS FOR SPEED ==========\n",
        "user_activity = df['user_id'].value_counts()\n",
        "active_users = user_activity.head(250_000).index\n",
        "df = df[df['user_id'].isin(active_users)]\n",
        "print(f\"Using {len(active_users)} active users, {len(df)} interactions\")\n",
        "\n",
        "# ========== 3. FEATURE ENGINEERING ==========\n",
        "item_pop = df['item_id'].value_counts()\n",
        "user_clicks = df['user_id'].value_counts()\n",
        "\n",
        "df['item_popularity'] = np.log1p(df['item_id'].map(item_pop))\n",
        "df['user_clicks'] = np.log1p(df['user_id'].map(user_clicks))\n",
        "df['recency'] = df['date'].max() - df['date']\n",
        "\n",
        "features = df.groupby(['user_id', 'item_id'], as_index=False).agg({\n",
        "    'recency': 'min',\n",
        "    'item_popularity': 'mean',\n",
        "    'user_clicks': 'mean',\n",
        "    'date': 'max'\n",
        "})\n",
        "features['label'] = 1\n",
        "\n",
        "# ========== 4. FAST NEGATIVE SAMPLING ==========\n",
        "top_items = item_pop.head(20_000).index\n",
        "n_neg = 2  # 2 негатива на 1 позитив\n",
        "\n",
        "user_ids = features['user_id'].unique()\n",
        "neg_samples = pd.DataFrame({\n",
        "    'user_id': np.repeat(user_ids, n_neg),\n",
        "    'item_id': np.random.choice(top_items, size=len(user_ids) * n_neg)\n",
        "})\n",
        "neg_samples['label'] = 0\n",
        "\n",
        "train_df = pd.concat([\n",
        "    features[['user_id', 'item_id', 'recency', 'item_popularity', 'user_clicks', 'label']],\n",
        "    neg_samples\n",
        "], ignore_index=True)\n",
        "\n",
        "train_df.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)\n",
        "train_df.fillna(0, inplace=True)\n",
        "gc.collect()\n",
        "\n",
        "# ========== 5. PREPARE DATA ==========\n",
        "X = train_df[['recency', 'item_popularity', 'user_clicks']].values\n",
        "y = train_df['label'].values\n",
        "group = train_df.groupby('user_id').size().to_numpy()\n",
        "\n",
        "# ========== 6. TRAIN MODELS ==========\n",
        "models = {}\n",
        "\n",
        "# --- LightGBM ---\n",
        "\n",
        "\n",
        "# --- XGBoost ---\n",
        "print(\"\\n🔵 Training XGBoost Ranker...\")\n",
        "start = time.time()\n",
        "xgb = XGBRanker(\n",
        "    objective='rank:pairwise',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist',\n",
        "    n_jobs=-1,\n",
        ")\n",
        "xgb.fit(X, y, group=group)\n",
        "models['XGBoost'] = xgb\n",
        "print(f\"✅ XGBoost trained in {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# --- CatBoost ---\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# ========== 7. GENERATE PREDICTIONS ==========\n",
        "popular_items = item_pop.head(500).index.tolist()\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n🚀 Generating predictions for {model_name}...\")\n",
        "    preds = []\n",
        "    t0 = time.time()\n",
        "    total_users = len(sample)\n",
        "    step = max(1, total_users // 100)  # каждые 1% — обновление статуса\n",
        "\n",
        "    for i, uid in enumerate(sample['user_id'], start=1):\n",
        "        if uid not in active_users:\n",
        "            preds.append({'user_id': uid, 'item_id': ' '.join(map(str, popular_items[:20]))})\n",
        "            continue\n",
        "\n",
        "        user_clicks_val = np.log1p(user_clicks.get(uid, 1))\n",
        "        cand = pd.DataFrame({\n",
        "            'item_id': popular_items,\n",
        "            'user_id': uid,\n",
        "            'recency': 0,\n",
        "            'user_clicks': user_clicks_val,\n",
        "            'item_popularity': np.log1p(item_pop.loc[popular_items].values)\n",
        "        })\n",
        "\n",
        "        X_pred = cand[['recency', 'item_popularity', 'user_clicks']].values\n",
        "        cand['score'] = model.predict(X_pred)\n",
        "        top20 = cand.nlargest(20, 'score')['item_id'].tolist()\n",
        "        preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "        if i % step == 0:\n",
        "            done = (i / total_users) * 100\n",
        "            elapsed = (time.time() - t0) / 60\n",
        "            print(f\"{done:.1f}% done ({elapsed:.1f} min elapsed)\")\n",
        "\n",
        "    submission = pd.DataFrame(preds)\n",
        "    filename = f'submission_{model_name.lower()}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "    print(f\"✅ Saved {filename} | Total time: {(time.time()-t0)/60:.1f} min\")\n",
        "\n",
        "print(\"\\n🎯 All models finished and submissions saved.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.339159Z",
          "iopub.status.idle": "2025-10-15T11:55:39.339384Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.33928Z",
          "shell.execute_reply": "2025-10-15T11:55:39.33929Z"
        },
        "id": "ebYfwIlAbgdg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "popular_items = item_pop.head(20).index.tolist()  # top-200 вместо 500\n",
        "preds = []\n",
        "\n",
        "for uid in tqdm(sample['user_id'], desc=\"Generating predictions\"):\n",
        "    if uid not in active_users:\n",
        "        # fallback — только топ-20 товаров\n",
        "        preds.append({'user_id': uid, 'item_id': ' '.join(map(str, popular_items[:20]))})\n",
        "        continue\n",
        "\n",
        "    user_clicks_val = np.log1p(user_clicks.get(uid, 1))\n",
        "    cand = pd.DataFrame({\n",
        "        'item_id': popular_items,\n",
        "        'user_id': uid,\n",
        "        'recency': 0,\n",
        "        'user_clicks': user_clicks_val,\n",
        "        'item_popularity': np.log1p(item_pop.loc[popular_items].values)\n",
        "    })\n",
        "\n",
        "    X_pred = cand[['recency', 'item_popularity', 'user_clicks']].values\n",
        "    cand['score'] = model.predict(X_pred)\n",
        "    top20 = cand.nlargest(20, 'score')['item_id'].tolist()\n",
        "    preds.append({'user_id': uid, 'item_id': ' '.join(map(str, top20))})\n",
        "\n",
        "submission = pd.DataFrame(preds)\n",
        "submission.to_csv('submission_small.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.342964Z",
          "iopub.status.idle": "2025-10-15T11:55:39.343237Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.343121Z",
          "shell.execute_reply": "2025-10-15T11:55:39.343132Z"
        },
        "id": "JrxvUe0Ibgdh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.344373Z",
          "iopub.status.idle": "2025-10-15T11:55:39.344695Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.344533Z",
          "shell.execute_reply": "2025-10-15T11:55:39.344547Z"
        },
        "id": "y6hW205xbgdi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Загружаем большой сабмит\n",
        "submission = pd.read_csv('/kaggle/working/submission_small.csv')\n",
        "\n",
        "# Обрезаем до топ-20 для каждого пользователя\n",
        "def top20_items(item_str):\n",
        "    items = item_str.split()\n",
        "    return ' '.join(items[:20])\n",
        "\n",
        "submission['item_id'] = submission['item_id'].astype(str).apply(top20_items)\n",
        "\n",
        "# Приводим к int, чтобы не было лишних знаков\n",
        "submission['item_id'] = submission['item_id'].apply(lambda x: ' '.join(map(str, map(int, x.split()))))\n",
        "\n",
        "# Сохраняем без индекса\n",
        "submission.to_csv('submission_small.csv.gz', index=False, compression='gzip')\n",
        "\n",
        "print(\"✅ Файл уменьшен и сохранён как submission_small.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.345488Z",
          "iopub.status.idle": "2025-10-15T11:55:39.345692Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.345594Z",
          "shell.execute_reply": "2025-10-15T11:55:39.345603Z"
        },
        "id": "e0X-EgUkbgdi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Загружаем данные\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "\n",
        "print(train.head())\n",
        "print(sample.head())\n",
        "# Топ-1000 популярных товаров\n",
        "top_items = train['item_id'].value_counts().head(500).index.tolist()\n",
        "\n",
        "# Последние клики каждого пользователя\n",
        "last_clicks = train.groupby('user_id')['item_id'].apply(lambda x: x.tolist()[-20:])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:55:39.346542Z",
          "iopub.status.idle": "2025-10-15T11:55:39.346831Z",
          "shell.execute_reply.started": "2025-10-15T11:55:39.346655Z",
          "shell.execute_reply": "2025-10-15T11:55:39.34667Z"
        },
        "id": "ICniTvaRbgdj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "user_pos_items = train.groupby('user_id')['item_id'].apply(set).to_dict()\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for user, pos_items in user_pos_items.items():\n",
        "    for item in pos_items:\n",
        "        #print(user,pos_items)\n",
        "        neg_item = random.choice([i for i in top_items if i not in pos_items])\n",
        "        pairs.append([user, item, neg_item])\n",
        "        print(pairs)\n",
        "\n",
        "pairs = pd.DataFrame(pairs, columns=['user_id', 'pos_item', 'neg_item'])\n",
        "print(pairs.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:16:08.407965Z",
          "iopub.execute_input": "2025-10-15T11:16:08.408345Z"
        },
        "id": "gf1LAR0Ubgdk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = df['user_id'].values\n",
        "        self.pos_items = df['pos_item'].values\n",
        "        self.neg_items = df['neg_item'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.pos_items[idx], self.neg_items[idx]\n",
        "\n",
        "class RankNetModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=32):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(n_items, emb_size)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        u_e = self.user_emb(u)\n",
        "        i_e = self.item_emb(i)\n",
        "        j_e = self.item_emb(j)\n",
        "        # score difference\n",
        "        x = (i_e - j_e) * u_e\n",
        "        return torch.sum(x, dim=1)\n",
        "\n",
        "# Псевдо-пронумеруем пользователей и товары\n",
        "user2id = {u:i for i,u in enumerate(train['user_id'].unique())}\n",
        "item2id = {i:i for i,i in enumerate(train['item_id'].unique())}\n",
        "\n",
        "pairs['user_id'] = pairs['user_id'].map(user2id)\n",
        "pairs['pos_item'] = pairs['pos_item'].map(item2id)\n",
        "pairs['neg_item'] = pairs['neg_item'].map(item2id)\n",
        "\n",
        "dataset = PairDataset(pairs)\n",
        "loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = RankNetModel(len(user2id), len(item2id), emb_size=32).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "V6lIS3Rkbgdl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):  # несколько эпох для baseline\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for u, i, j in loader:\n",
        "        u, i, j = u.to(device), i.to(device), j.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # label = 1 для pos>neg\n",
        "        scores = model(u, i, j)\n",
        "        loss = criterion(scores, torch.ones_like(scores))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YHX3bswTbgdl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "topk = 20\n",
        "preds = []\n",
        "\n",
        "for user in train['user_id'].unique():\n",
        "    u_id = torch.tensor([user2id[user]]*len(item2id)).to(device)\n",
        "    items = torch.tensor(list(range(len(item2id)))).to(device)\n",
        "    with torch.no_grad():\n",
        "        scores = model.user_emb(u_id) * model.item_emb(items)\n",
        "        scores = scores.sum(dim=1)\n",
        "    top_items_idx = torch.topk(scores, topk).indices.cpu().numpy()\n",
        "    top_items_ids = [list(item2id.keys())[i] for i in top_items_idx]\n",
        "    preds.append([user] + top_items_ids)\n",
        "\n",
        "submission = pd.DataFrame(preds, columns=sample.columns)\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8Ldj4Yp9bgdl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Полный/обновлённый пример (замена соответствующих частей предыдущего скрипта)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ========== ЗАГРУЗКА ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')  # ваш train\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')  # образец сабмита (длинный формат)\n",
        "# Получим список пользователей, для которых требуется предсказать (в sample)\n",
        "test_users = sample['user_id'].unique()\n",
        "print(\"users in sample:\", len(test_users))\n",
        "\n",
        "# ========== ОГРАНИЧЕНИЕ КАНДИДАТОВ ==========\n",
        "# Возьмём топ-N популярных товаров для ускорения (можно увеличить N)\n",
        "TOP_N = 1000\n",
        "top_items = train['item_id'].value_counts().head(TOP_N).index.tolist()\n",
        "\n",
        "# Словари сопоставления для ограниченного множества предметов\n",
        "item_list = top_items\n",
        "item2id = {item: idx for idx, item in enumerate(item_list)}\n",
        "id2item = {idx: item for item, idx in item2id.items()}\n",
        "\n",
        "# Словарь положительных товаров для каждого пользователя (только из top_items)\n",
        "user_pos_items_full = train.groupby('user_id')['item_id'].apply(list).to_dict()\n",
        "user_pos_items = {u: set([i for i in lst if i in item2id]) for u, lst in user_pos_items_full.items()}\n",
        "\n",
        "# ========== СОЗДАНИЕ ПАР ДЛЯ PAIRWISE ОБУЧЕНИЯ (RankNet) ==========\n",
        "pairs = []\n",
        "for user, pos_set in user_pos_items.items():\n",
        "    if not pos_set:\n",
        "        continue\n",
        "    # Для каждого положительного примера подбираем отрицательный из top_items\n",
        "    for pos in pos_set:\n",
        "        # negative — случайный item из top_items, которого нет у пользователя\n",
        "        neg_candidates = [it for it in item_list if it not in pos_set]\n",
        "        if not neg_candidates:\n",
        "            continue\n",
        "        neg = random.choice(neg_candidates)\n",
        "        pairs.append((user, pos, neg))\n",
        "\n",
        "pairs_df = pd.DataFrame(pairs, columns=['user_id', 'pos_item', 'neg_item'])\n",
        "# Перенумеруем пользователей в компактные id\n",
        "user_list = list({u for u in pairs_df['user_id'].unique()}.union(set(test_users)))\n",
        "user2id = {u: idx for idx, u in enumerate(user_list)}\n",
        "id2user = {idx: u for u, idx in user2id.items()}\n",
        "\n",
        "pairs_df['u_id'] = pairs_df['user_id'].map(user2id)\n",
        "pairs_df['pos_id'] = pairs_df['pos_item'].map(item2id)\n",
        "pairs_df['neg_id'] = pairs_df['neg_item'].map(item2id)\n",
        "\n",
        "# Отбрасываем пары, где pos или neg не в item2id (на всякий случай)\n",
        "pairs_df = pairs_df.dropna(subset=['pos_id', 'neg_id']).astype({'u_id':int,'pos_id':int,'neg_id':int})\n",
        "\n",
        "# ========== DATASET / DATALOADER ==========\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.u = df['u_id'].values\n",
        "        self.i = df['pos_id'].values\n",
        "        self.j = df['neg_id'].values\n",
        "    def __len__(self):\n",
        "        return len(self.u)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.u[idx], self.i[idx], self.j[idx]\n",
        "\n",
        "dataset = PairDataset(pairs_df)\n",
        "loader = DataLoader(dataset, batch_size=2048, shuffle=True, num_workers=2)\n",
        "\n",
        "# ========== МОДЕЛЬ (RankNet-подобная) ==========\n",
        "class RankNetModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=64):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(n_items, emb_size)\n",
        "        # небольшой MLP для преобразования скалярного различия (опционально)\n",
        "        self.out = nn.Linear(emb_size, 1, bias=False)  # можно упростить/усложнить\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        u_e = self.user_emb(u)             # (B, E)\n",
        "        i_e = self.item_emb(i)             # (B, E)\n",
        "        j_e = self.item_emb(j)             # (B, E)\n",
        "        # score = dot(user, item)\n",
        "        s_i = (u_e * i_e).sum(dim=1)       # (B,)\n",
        "        s_j = (u_e * j_e).sum(dim=1)       # (B,)\n",
        "        x = s_i - s_j                      # (B,)\n",
        "        return x\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = RankNetModel(n_users=len(user2id), n_items=len(item2id), emb_size=64).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T11:56:09.152982Z",
          "iopub.execute_input": "2025-10-15T11:56:09.153777Z",
          "iopub.status.idle": "2025-10-15T12:04:10.853273Z",
          "shell.execute_reply.started": "2025-10-15T11:56:09.153753Z",
          "shell.execute_reply": "2025-10-15T12:04:10.852418Z"
        },
        "id": "nv5zygFJbgdl",
        "outputId": "1705bced-28b3-453f-8f8e-0c9b61562e3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "users in sample: 293230\nEpoch 1/3  avg loss: 3.942721\nEpoch 2/3  avg loss: 2.414918\nEpoch 3/3  avg loss: 1.482728\nSaved submission.csv, rows: 5864600\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== ОБУЧЕНИЕ (несколько эпох для baseline) ==========\n",
        "EPOCHS = 16\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for u, i, j in loader:\n",
        "        u = u.to(device).long()\n",
        "        i = i.to(device).long()\n",
        "        j = j.to(device).long()\n",
        "        opt.zero_grad()\n",
        "        logits = model(u, i, j)\n",
        "        loss = criterion(logits, torch.ones_like(logits, device=device))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}  avg loss: {total_loss/len(loader):.6f}\")\n",
        "\n",
        "# ========== ПРЕДСКАЗАНИЯ ДЛЯ users из sample (top-20) ==========\n",
        "model.eval()\n",
        "TOPK = 20\n",
        "# Prepare item tensors once\n",
        "all_item_ids = torch.arange(len(item2id), device=device).long()\n",
        "all_item_emb = model.item_emb(all_item_ids)  # (N_items, E)\n",
        "\n",
        "submission_rows = []\n",
        "with torch.no_grad():\n",
        "    for u in test_users:\n",
        "        # map user to internal id; если пользователь не был в train pairs, добавим новый id (если нет — рекомендовать по популярности)\n",
        "        if u in user2id:\n",
        "            u_id = torch.tensor([user2id[u]], device=device).long()\n",
        "            u_emb = model.user_emb(u_id)  # (1, E)\n",
        "            # вычислим скор для всех candidate items: dot(u_emb, all_item_emb)\n",
        "            scores = (u_emb @ all_item_emb.t()).squeeze(0)  # (N_items,)\n",
        "            topk_idx = torch.topk(scores, min(TOPK, scores.size(0))).indices.cpu().numpy().tolist()\n",
        "            top_items_pred = [id2item[idx] for idx in topk_idx]\n",
        "        else:\n",
        "            # fallback: просто вернуть самые популярные топ-N (в порядке популярности)\n",
        "            top_items_pred = item_list[:TOPK]\n",
        "\n",
        "        # Убедимся, что предсказания уникальны и не содержат уже взаимодействовавших (опционально)\n",
        "        # Но для baseline — оставим как есть; можно фильтровать по user_pos_items_full.\n",
        "\n",
        "        # Добавляем в длинный формат: одну строку на предсказанный item\n",
        "        for it in top_items_pred:\n",
        "            submission_rows.append((u, it))\n",
        "\n",
        "# Создаём DataFrame и сохраняем в формате, аналогичном sample\n",
        "sub_df = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv, rows:\", len(sub_df))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T12:47:38.314565Z",
          "iopub.execute_input": "2025-10-15T12:47:38.31536Z",
          "iopub.status.idle": "2025-10-15T12:59:04.226688Z",
          "shell.execute_reply.started": "2025-10-15T12:47:38.315329Z",
          "shell.execute_reply": "2025-10-15T12:59:04.225704Z"
        },
        "id": "JrVIPbE2bgdm",
        "outputId": "9bb3f483-a9b3-4b02-dd8f-cd1aaca80265"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/16  avg loss: 0.003568\nEpoch 2/16  avg loss: 0.001962\nEpoch 3/16  avg loss: 0.001055\nEpoch 4/16  avg loss: 0.000557\nEpoch 5/16  avg loss: 0.000293\nEpoch 6/16  avg loss: 0.000153\nEpoch 7/16  avg loss: 0.000080\nEpoch 8/16  avg loss: 0.000042\nEpoch 9/16  avg loss: 0.000022\nEpoch 10/16  avg loss: 0.000012\nEpoch 11/16  avg loss: 0.000006\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/1623722598.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{EPOCHS}  avg loss: {total_loss/len(loader):.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv, rows:\", len(sub_df))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T12:59:09.669192Z",
          "iopub.execute_input": "2025-10-15T12:59:09.669473Z",
          "iopub.status.idle": "2025-10-15T12:59:25.464021Z",
          "shell.execute_reply.started": "2025-10-15T12:59:09.669448Z",
          "shell.execute_reply": "2025-10-15T12:59:25.46325Z"
        },
        "id": "WloSSNfubgdm",
        "outputId": "6270f56a-8653-464e-9fc0-6380d7d18ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Saved submission.csv, rows: 5864600\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load data ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "test_users = sample['user_id'].unique()\n",
        "\n",
        "# ========== 2. Feature generation ==========\n",
        "print(\"Building simple features...\")\n",
        "\n",
        "# Item popularity\n",
        "item_pop = train['item_id'].value_counts().rename('item_pop')\n",
        "\n",
        "# User activity\n",
        "user_freq = train['user_id'].value_counts().rename('user_freq')\n",
        "\n",
        "# Recency feature: max date per user/item\n",
        "user_last = train.groupby('user_id')['date'].max().rename('user_last')\n",
        "item_last = train.groupby('item_id')['date'].max().rename('item_last')\n",
        "\n",
        "train = train.join(item_pop, on='item_id')\n",
        "train = train.join(user_freq, on='user_id')\n",
        "train = train.join(user_last, on='user_id')\n",
        "train = train.join(item_last, on='item_id')\n",
        "train['recency'] = train['user_last'] - train['date']\n",
        "\n",
        "# ========== 3. Build candidates (top popular + recent clicks per user) ==========\n",
        "top_items = train['item_id'].value_counts().head(1000).index.tolist()\n",
        "user_recent = train.groupby('user_id')['item_id'].apply(lambda x: x.tail(10).tolist())\n",
        "\n",
        "pairs = []\n",
        "for user, items in tqdm(user_recent.items()):\n",
        "    for pos in items:\n",
        "        neg = np.random.choice(top_items)\n",
        "        pairs.append((user, pos, 1))  # positive\n",
        "        pairs.append((user, neg, 0))  # negative\n",
        "\n",
        "pairs = pd.DataFrame(pairs, columns=['user_id', 'item_id', 'label'])\n",
        "pairs = pairs.join(item_pop, on='item_id')\n",
        "pairs = pairs.join(item_last, on='item_id')\n",
        "pairs = pairs.join(user_freq, on='user_id')\n",
        "pairs = pairs.join(user_last, on='user_id')\n",
        "pairs['recency'] = pairs['user_last'] - pairs['item_last']\n",
        "pairs = pairs.fillna(0)\n",
        "\n",
        "# ========== 4. Prepare for XGBoost ==========\n",
        "features = ['item_pop', 'item_last', 'user_freq', 'user_last', 'recency']\n",
        "X = pairs[features].values\n",
        "y = pairs['label'].values\n",
        "\n",
        "# group sizes (number of items per user)\n",
        "group = pairs.groupby('user_id').size().values\n",
        "\n",
        "ranker = xgb.XGBRanker(\n",
        "    objective='rank:pairwise',\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    n_estimators=100,\n",
        "    tree_method='hist',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training XGBoost Ranker...\")\n",
        "ranker.fit(X, y, group=group)\n",
        "\n",
        "# ========== 5. Prediction ==========\n",
        "print(\"Generating recommendations...\")\n",
        "sub_rows = []\n",
        "for user in tqdm(test_users):\n",
        "    # создаем кандидатов (последние клики + топ)\n",
        "    cands = list(set(user_recent.get(user, [])) | set(top_items))\n",
        "    df = pd.DataFrame({'user_id': user, 'item_id': cands})\n",
        "    df = df.join(item_pop, on='item_id')\n",
        "    df = df.join(item_last, on='item_id')\n",
        "    df = df.join(user_freq, on='user_id')\n",
        "    df = df.join(user_last, on='user_id')\n",
        "    df['recency'] = df['user_last'] - df['item_last']\n",
        "    df = df.fillna(0)\n",
        "    X_test = df[features].values\n",
        "    preds = ranker.predict(X_test)\n",
        "    df['pred'] = preds\n",
        "    top20 = df.sort_values('pred', ascending=False).head(20)\n",
        "    for item in top20['item_id'].tolist():\n",
        "        sub_rows.append((user, item))\n",
        "\n",
        "sub = pd.DataFrame(sub_rows, columns=['user_id', 'item_id'])\n",
        "sub.to_csv('submission_xgboost.csv', index=False)\n",
        "print(\"✅ submission_xgboost.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T13:15:29.885833Z",
          "iopub.execute_input": "2025-10-15T13:15:29.886448Z",
          "iopub.status.idle": "2025-10-15T14:04:22.031196Z",
          "shell.execute_reply.started": "2025-10-15T13:15:29.886422Z",
          "shell.execute_reply": "2025-10-15T14:04:22.029821Z"
        },
        "id": "A9166nyHbgdm",
        "outputId": "dc02b7f1-ac68-4503-a3aa-823f4ec0f9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Building simple features...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2682603it [09:39, 4628.76it/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training XGBoost Ranker...\nGenerating recommendations...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "  1%|          | 2722/293230 [29:18<52:08:34,  1.55it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/1973811664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recency'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_last'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_last'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[1;32m  10755\u001b[0m                     \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10756\u001b[0m                 )\n\u001b[0;32m> 10757\u001b[0;31m             return merge(\n\u001b[0m\u001b[1;32m  10758\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10759\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         )\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         result = self._reindex_and_concat(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             join_index, left_indexer, right_indexer = _left_join_on_index(\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0mleft_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_left_join_on_index\u001b[0;34m(left_ax, right_ax, join_keys, sort)\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0mleft_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factorize_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m     left_indexer, right_indexer = libjoin.left_outer_join(\n\u001b[0m\u001b[1;32m   2386\u001b[0m         \u001b[0mleft_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load and sort data ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "test_users = sample['user_id'].unique()\n",
        "\n",
        "# Сортируем взаимодействия по дате\n",
        "train = train.sort_values(['user_id', 'date'])\n",
        "\n",
        "# ========== 2. Подготовим последовательности ==========\n",
        "max_len = 20  # длина истории\n",
        "user_sequences = (\n",
        "    train.groupby('user_id')['item_id']\n",
        "    .apply(lambda x: x.tolist()[-max_len:])\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# Словарь item_id → индекс\n",
        "item_vocab = {it: idx+1 for idx, it in enumerate(train['item_id'].unique())}  # +1 для PAD=0\n",
        "id2item = {v:k for k,v in item_vocab.items()}\n",
        "n_items = len(item_vocab) + 1\n",
        "\n",
        "# ========== 3. Dataset ==========\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, user_seq):\n",
        "        self.users = list(user_seq.keys())\n",
        "        self.seqs = list(user_seq.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq  # left padding\n",
        "        target = seq[-1]  # last item\n",
        "        return torch.tensor(seq[:-1]), torch.tensor(target)\n",
        "\n",
        "train_ds = SeqDataset(user_sequences)\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "# ========== 4. SASRec-подобная модель ==========\n",
        "class TransformerRec(nn.Module):\n",
        "    def __init__(self, num_items, d_model=512, nhead=8, num_layers=6, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.num_items = num_items\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.item_emb = nn.Embedding(num_items + 1, d_model, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            batch_first=False,  # стандартный формат (L, B, D)\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.proj = nn.Linear(d_model, num_items + 1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.item_emb.weight, mean=0, std=0.01)\n",
        "        nn.init.normal_(self.pos_emb.weight, mean=0, std=0.01)\n",
        "        nn.init.xavier_normal_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        batch_size, seq_len = seq.size()\n",
        "\n",
        "        pos = torch.arange(seq_len, device=seq.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        x = self.item_emb(seq) + self.pos_emb(pos)  # (B, L, D)\n",
        "        x = x.permute(1, 0, 2)  # (L, B, D)\n",
        "\n",
        "        out = self.encoder(x)  # (L, B, D)\n",
        "        out = out.permute(1, 0, 2)  # (B, L, D)\n",
        "\n",
        "        logits = self.proj(out)  # (B, L, num_items + 1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = TransformerRec(n_items=n_items).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# ========== 5. Обучение ==========\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for seq, tgt in train_dl:\n",
        "        seq, tgt = seq.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ========== 6. Предсказания ==========\n",
        "model.eval()\n",
        "TOPK = 20\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user in tqdm(test_users):\n",
        "        seq = user_sequences.get(user, [])\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq\n",
        "        seq_t = torch.tensor(seq[:-1]).unsqueeze(0).to(device)\n",
        "        logits = model(seq_t)\n",
        "        topk_idx = torch.topk(logits, TOPK, dim=1).indices[0].cpu().numpy()\n",
        "        top_items = [id2item[i] for i in topk_idx if i in id2item]\n",
        "        for it in top_items:\n",
        "            submission_rows.append((user, it))\n",
        "\n",
        "sub = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub.to_csv('submission_transformer.csv', index=False)\n",
        "print(\"✅ submission_transformer.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-15T14:30:15.215263Z",
          "iopub.execute_input": "2025-10-15T14:30:15.215994Z",
          "iopub.status.idle": "2025-10-15T14:30:52.77293Z",
          "shell.execute_reply.started": "2025-10-15T14:30:15.215967Z",
          "shell.execute_reply": "2025-10-15T14:30:52.771951Z"
        },
        "id": "qWKOECzdbgdm",
        "outputId": "447f8a8c-a32e-488d-8c1c-11be4862f48e"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_141/930912039.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerRec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TransformerRec.__init__() got an unexpected keyword argument 'n_items'"
          ],
          "ename": "TypeError",
          "evalue": "TransformerRec.__init__() got an unexpected keyword argument 'n_items'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load and sort data ==========\n",
        "train = pd.read_parquet('/kaggle/input/reccomend/train_data.pq')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "test_users = sample['user_id'].unique()\n",
        "\n",
        "# Сортируем взаимодействия по дате\n",
        "train = train.sort_values(['user_id', 'date'])\n",
        "\n",
        "# ========== 2. Подготовим последовательности ==========\n",
        "max_len = 20  # длина истории\n",
        "user_sequences = (\n",
        "    train.groupby('user_id')['item_id']\n",
        "    .apply(lambda x: x.tolist()[-max_len:])\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# Словарь item_id → индекс\n",
        "item_vocab = {it: idx+1 for idx, it in enumerate(train['item_id'].unique())}  # +1 для PAD=0\n",
        "id2item = {v:k for k,v in item_vocab.items()}\n",
        "n_items = len(item_vocab) + 1\n",
        "\n",
        "# ========== 3. Dataset ==========\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, user_seq):\n",
        "        self.users = list(user_seq.keys())\n",
        "        self.seqs = list(user_seq.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq  # left padding\n",
        "        target = seq[-1]  # last item\n",
        "        return torch.tensor(seq[:-1]), torch.tensor(target)\n",
        "\n",
        "train_ds = SeqDataset(user_sequences)\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "# ========== 4. SASRec-подобная модель ==========\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, n_items, d_model=64, n_heads=4, n_layers=2, max_len=20, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.item_emb = nn.Embedding(n_items, d_model, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n",
        "            batch_first=True, dropout=dropout, activation='gelu'\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.fc = nn.Linear(d_model, n_items)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, seq):\n",
        "        B, L = seq.shape\n",
        "        pos = torch.arange(L, device=seq.device).unsqueeze(0).expand(B, -1)\n",
        "        x = self.item_emb(seq) + self.pos_emb(pos)\n",
        "        x = self.layernorm(self.dropout(x))\n",
        "\n",
        "        # causal mask: модель не видит будущее\n",
        "        mask = torch.triu(torch.ones(L, L, device=seq.device), diagonal=1).bool()\n",
        "        out = self.encoder(x, mask)\n",
        "        logits = self.fc(out)  # (B, L, n_items)\n",
        "        return logits\n",
        "\n",
        "def compute_loss(logits, seq):\n",
        "    # таргеты: следующий item для каждой позиции\n",
        "    targets = seq[:, 1:]\n",
        "    inputs = logits[:, :-1, :]\n",
        "    loss = F.cross_entropy(\n",
        "        inputs.reshape(-1, inputs.size(-1)),\n",
        "        targets.reshape(-1),\n",
        "        ignore_index=0\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SASRec(n_items=n_items).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# ========== 5. Обучение ==========\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for seq, tgt in train_dl:\n",
        "        seq, tgt = seq.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ========== 6. Предсказания ==========\n",
        "model.eval()\n",
        "TOPK = 20\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user in tqdm(test_users):\n",
        "        seq = user_sequences.get(user, [])\n",
        "        seq = [item_vocab[i] for i in seq if i in item_vocab]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq\n",
        "        seq_t = torch.tensor(seq[:-1]).unsqueeze(0).to(device)\n",
        "        logits = model(seq_t)\n",
        "        topk_idx = torch.topk(logits, TOPK, dim=1).indices[0].cpu().numpy()\n",
        "        top_items = [id2item[i] for i in topk_idx if i in id2item]\n",
        "        for it in top_items:\n",
        "            submission_rows.append((user, it))\n",
        "\n",
        "sub = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub.to_csv('submission_transformer.csv', index=False)\n",
        "print(\"✅ submission_transformer.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T11:23:51.754163Z",
          "iopub.execute_input": "2025-10-18T11:23:51.754774Z",
          "iopub.status.idle": "2025-10-18T11:24:40.494183Z",
          "shell.execute_reply.started": "2025-10-18T11:23:51.754747Z",
          "shell.execute_reply": "2025-10-18T11:24:40.493054Z"
        },
        "id": "Phay42GTbgdm",
        "outputId": "7541b1d4-e819-4c58-9a15-0d32cf1f0584"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/307201193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 13.42 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.74 GiB is free. Process 2500 has 14.15 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 26.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ],
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 13.42 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.74 GiB is free. Process 2500 has 14.15 GiB memory in use. Of the allocated memory 13.83 GiB is allocated by PyTorch, and 26.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'my_model.pth')\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T15:38:53.294494Z",
          "iopub.execute_input": "2025-10-17T15:38:53.295294Z",
          "iopub.status.idle": "2025-10-17T15:38:54.178026Z",
          "shell.execute_reply.started": "2025-10-17T15:38:53.295269Z",
          "shell.execute_reply": "2025-10-17T15:38:54.177183Z"
        },
        "id": "Q0yVqL2Ubgdn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# two_tower_recommender.py\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "TRAIN_PATH = '/kaggle/input/reccomend/train_data.pq'\n",
        "SAMPLE_PATH = '/kaggle/input/asddbfd/sample_submission (1).csv'  # замените при необходимости\n",
        "OUTPUT_PATH = 'submission_two_tower.csv'\n",
        "\n",
        "EMB_DIM = 64        # размер эмбеддингов\n",
        "HIDDEN = 128        # размер MLP внутри башен\n",
        "BATCH_SIZE = 4096\n",
        "EPOCHS = 3\n",
        "LR = 1e-3\n",
        "TOP_ITEMS = 2000    # кандидаты (популярные) — можно увеличить\n",
        "TOPK = 20\n",
        "NUM_NEG = 1         # отрицательных на положительный\n",
        "SEED = 42\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ----------------- DATA LOAD -----------------\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_parquet(TRAIN_PATH)\n",
        "sample = pd.read_csv(SAMPLE_PATH)\n",
        "\n",
        "# users to predict (preserve order if needed)\n",
        "test_users_ordered = sample['user_id'].values  # длинный формат likely repeats users; we'll take unique in order\n",
        "_, idx = np.unique(test_users_ordered, return_index=True)\n",
        "test_users = test_users_ordered[np.sort(idx)]\n",
        "\n",
        "# ----------------- CANDIDATE REDUCTION -----------------\n",
        "# топ популярных товаров для кандидатов (ускорение)\n",
        "popular_items = train['item_id'].value_counts().index.tolist()\n",
        "top_items = popular_items[:TOP_ITEMS]\n",
        "\n",
        "# ----------------- ID MAPPINGS -----------------\n",
        "all_users = train['user_id'].unique().tolist()\n",
        "# include test users even if they didn't appear in training pairs mapping — but prompt said they do\n",
        "for u in test_users:\n",
        "    if u not in all_users:\n",
        "        all_users.append(u)\n",
        "all_items = list({i for i in train['item_id'].unique() if i in top_items})  # limit items to top_items\n",
        "# ensure top_items are present in item list\n",
        "all_items_set = set(all_items)\n",
        "for it in top_items:\n",
        "    if it not in all_items_set:\n",
        "        all_items.append(it)\n",
        "\n",
        "user2id = {u: idx for idx, u in enumerate(all_users)}\n",
        "id2user = {v:k for k,v in user2id.items()}\n",
        "item2id = {i: idx for idx, i in enumerate(all_items)}\n",
        "id2item = {v:k for k,v in item2id.items()}\n",
        "\n",
        "n_users = len(user2id)\n",
        "n_items = len(item2id)\n",
        "print(f\"n_users={n_users}, n_items={n_items}\")\n",
        "\n",
        "# ----------------- BUILD USER HISTORIES -----------------\n",
        "user_pos = train.groupby('user_id')['item_id'].apply(list).to_dict()\n",
        "# filter histories to items inside item2id (candidates)\n",
        "user_pos_filtered = {u: [i for i in lst if i in item2id] for u, lst in user_pos.items()}\n",
        "\n",
        "# ----------------- TRAINING PAIR DATASET -----------------\n",
        "pairs = []\n",
        "for u, items in user_pos_filtered.items():\n",
        "    if not items:\n",
        "        continue\n",
        "    # each positive item can produce NUM_NEG negative samples\n",
        "    for pos in set(items):\n",
        "        for _ in range(NUM_NEG):\n",
        "            # sample negative from top_items but exclude user's positives\n",
        "            neg = random.choice(top_items)\n",
        "            # if neg not in item2id, skip (could happen if top_items larger than our filtered)\n",
        "            if neg not in item2id:\n",
        "                continue\n",
        "            if neg in items:\n",
        "                continue\n",
        "            pairs.append((u, pos, neg))\n",
        "\n",
        "print(f\"Pairs for training: {len(pairs)}\")\n",
        "\n",
        "pairs_df = pd.DataFrame(pairs, columns=['user', 'pos', 'neg'])\n",
        "pairs_df['u_id'] = pairs_df['user'].map(user2id)\n",
        "pairs_df['pos_id'] = pairs_df['pos'].map(item2id)\n",
        "pairs_df['neg_id'] = pairs_df['neg'].map(item2id)\n",
        "pairs_df = pairs_df.dropna().astype(int)\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.u = df['u_id'].values\n",
        "        self.pos = df['pos_id'].values\n",
        "        self.neg = df['neg_id'].values\n",
        "    def __len__(self):\n",
        "        return len(self.u)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.u[idx], self.pos[idx], self.neg[idx]\n",
        "\n",
        "train_loader = DataLoader(PairDataset(pairs_df), batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ----------------- MODEL -----------------\n",
        "class Tower(nn.Module):\n",
        "    def __init__(self, n_entities, emb_dim, hidden):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(n_entities, emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, emb_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, idx):\n",
        "        x = self.emb(idx)\n",
        "        x = self.mlp(x)\n",
        "        # L2-normalize for stable dot product / cosine-like scoring\n",
        "        x = x / (x.norm(p=2, dim=1, keepdim=True) + 1e-8)\n",
        "        return x\n",
        "\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_dim=64, hidden=128):\n",
        "        super().__init__()\n",
        "        self.user_tower = Tower(n_users, emb_dim, hidden)\n",
        "        self.item_tower = Tower(n_items, emb_dim, hidden)\n",
        "\n",
        "    def forward(self, u_idx, i_idx=None, j_idx=None):\n",
        "        u_emb = self.user_tower(u_idx)  # (B, E)\n",
        "        if i_idx is not None:\n",
        "            i_emb = self.item_tower(i_idx)\n",
        "        else:\n",
        "            i_emb = None\n",
        "        if j_idx is not None:\n",
        "            j_emb = self.item_tower(j_idx)\n",
        "        else:\n",
        "            j_emb = None\n",
        "        return u_emb, i_emb, j_emb\n",
        "\n",
        "model = TwoTowerModel(n_users=n_users, n_items=n_items, emb_dim=EMB_DIM, hidden=HIDDEN).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# BPR-style loss using logits difference + softplus\n",
        "def bpr_loss(u_emb, pos_emb, neg_emb):\n",
        "    # u_emb: (B, E), pos_emb/neg_emb: (B, E)\n",
        "    pos_scores = (u_emb * pos_emb).sum(dim=1)   # (B,)\n",
        "    neg_scores = (u_emb * neg_emb).sum(dim=1)\n",
        "    x = pos_scores - neg_scores\n",
        "    # softplus(-x) encourages pos>neg; equivalently use -log(sigmoid(x))\n",
        "    return torch.nn.functional.softplus(-x).mean()\n",
        "\n",
        "# ----------------- TRAINING LOOP -----------------\n",
        "print(\"Start training...\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        u_ids, pos_ids, neg_ids = batch\n",
        "        u_ids = u_ids.to(device).long()\n",
        "        pos_ids = pos_ids.to(device).long()\n",
        "        neg_ids = neg_ids.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        u_emb, pos_emb, neg_emb = model(u_ids, pos_ids, neg_ids)\n",
        "        loss = bpr_loss(u_emb, pos_emb, neg_emb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} avg loss: {avg_loss:.6f}\")\n",
        "\n",
        "# ----------------- GENERATE RECOMMENDATIONS -----------------\n",
        "# Precompute all item embeddings (for candidate set)\n",
        "print(\"Computing item embeddings...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # item indices 0..n_items-1\n",
        "    all_item_idx = torch.arange(n_items, device=device).long()\n",
        "    item_embs = model.item_tower.emb(all_item_idx)\n",
        "    item_embs = model.item_tower.mlp(item_embs)\n",
        "    item_embs = item_embs / (item_embs.norm(p=2, dim=1, keepdim=True) + 1e-8)  # (n_items, E)\n",
        "    # to cpu for faster topk if GPU memory limited (we'll keep on device to use torch.topk)\n",
        "    item_embs_t = item_embs  # keep on device\n",
        "\n",
        "# We'll produce recommendations for unique test users in order\n",
        "print(\"Generating submission...\")\n",
        "submission_rows = []\n",
        "unique_test_users = test_users  # ordered unique users from earlier\n",
        "\n",
        "batch_size_predict = 1024\n",
        "for i in tqdm(range(0, len(unique_test_users), batch_size_predict)):\n",
        "    batch_users = unique_test_users[i:i+batch_size_predict]\n",
        "    u_idx = []\n",
        "    for u in batch_users:\n",
        "        u_idx.append(user2id.get(u, None))\n",
        "    # if some users missing mapping (unlikely), fallback to popular items\n",
        "    valid_mask = [idx is not None and idx < n_users for idx in u_idx]\n",
        "    # create tensor of user indices (fill invalid with 0)\n",
        "    u_idx_tensor = torch.tensor([x if x is not None else 0 for x in u_idx], device=device).long()\n",
        "    with torch.no_grad():\n",
        "        u_emb_batch = model.user_tower.emb(u_idx_tensor)\n",
        "        u_emb_batch = model.user_tower.mlp(u_emb_batch)\n",
        "        u_emb_batch = u_emb_batch / (u_emb_batch.norm(p=2, dim=1, keepdim=True) + 1e-8)  # (B, E)\n",
        "        # scores = u_emb_batch @ item_embs_t.T  -> (B, n_items)\n",
        "        scores = torch.matmul(u_emb_batch, item_embs_t.t())  # (B, n_items)\n",
        "        topk_vals, topk_idxs = torch.topk(scores, k=min(TOPK, n_items), dim=1)\n",
        "\n",
        "    for bi, uid in enumerate(batch_users):\n",
        "        if not valid_mask[bi]:\n",
        "            # fallback: top popular items\n",
        "            recs = top_items[:TOPK]\n",
        "        else:\n",
        "            idxs = topk_idxs[bi].cpu().numpy().tolist()\n",
        "            recs = [id2item[idx] for idx in idxs]\n",
        "        # append to long-format (one row per recommendation)\n",
        "        for it in recs:\n",
        "            submission_rows.append((uid, it))\n",
        "\n",
        "sub_df = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub_df.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"Saved {OUTPUT_PATH} with {len(sub_df)} rows\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T10:44:02.854317Z",
          "iopub.execute_input": "2025-10-18T10:44:02.854496Z",
          "iopub.status.idle": "2025-10-18T11:16:53.146277Z",
          "execution_failed": "2025-10-18T11:22:22.759Z"
        },
        "id": "sRIJqACKbgdn",
        "outputId": "769430be-0a20-46d2-928d-0d580fd59d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading data...\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_37/4168239699.py\", line None, in <cell line: 0>\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n    start = lineno - 1 - context//2\n            ~~~~~~~^~~\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_37/4168239699.py\", line None, in <cell line: 0>\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n    self.showtraceback(running_compiled_code=True)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(etype,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n    return FormattedTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n    return VerboseTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n    return len(records), 0\n           ^^^^^^^^^^^^\nTypeError: object of type 'NoneType' has no len()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n    start = lineno - 1 - context//2\n            ~~~~~~~^~~\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_37/4168239699.py\", line None, in <cell line: 0>\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n    self.showtraceback(running_compiled_code=True)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(etype,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n    return FormattedTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n    return VerboseTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n    return len(records), 0\n           ^^^^^^^^^^^^\nTypeError: object of type 'NoneType' has no len()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n    self.showtraceback()\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(etype,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n    return FormattedTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n    return VerboseTB.structured_traceback(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n    return len(records), 0\n           ^^^^^^^^^^^^\nTypeError: object of type 'NoneType' has no len()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n    start = lineno - 1 - context//2\n            ~~~~~~~^~~\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}