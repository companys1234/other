{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13367894,
          "sourceType": "datasetVersion",
          "datasetId": 8480207
        },
        {
          "sourceId": 13368115,
          "sourceType": "datasetVersion",
          "datasetId": 8480372
        },
        {
          "sourceId": 13369296,
          "sourceType": "datasetVersion",
          "datasetId": 8481309
        },
        {
          "sourceId": 13422019,
          "sourceType": "datasetVersion",
          "datasetId": 8518808
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "3 task",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "recsys_pointwise.py\n",
        "\n",
        "Простой pipeline для задачи рекомендаций (pointwise). Делает следующее:\n",
        "1. Читает train_data.pq и sample_submission.csv\n",
        "2. Делит по времени: последние 7 дней как target (positive) для обучения/валидации\n",
        "3. Генерирует кандидатов: история пользователя, популярные товары, случайные негативы\n",
        "4. Строит признаки (user/item statistics, recency, user-item count)\n",
        "5. Обучает LightGBM классификатор (pointwise) для предсказания вероятности клика\n",
        "6. Генерирует топ-20 рекомендаций для пользователей из sample_submission\n",
        "7. Сохраняет два варианта сабмишна: \"submission_long.csv\" (one row per user-item)\n",
        "   и \"submission_space.csv\" (one row per user with space-separated 20 item_ids)\n",
        "\n",
        "Требования: pandas, numpy, pyarrow (для чтения parquet), scikit-learn, lightgbm\n",
        "\n",
        "Запуск:\n",
        "python recsys_pointwise.py --train train_data.pq --sample sample_submission.csv --out_dir ./output\n",
        "\n",
        "Примечание: это минимальный, но практичный baseline. Его легко улучшать: больше фич,\n",
        "больше кандидатов, продвинутые модели (табличные NN / embedding + dot-product),\n",
        "специфичная валидация по времени и т.д.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# lightgbm may not be installed in user's env; try import and give friendly error\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception as e:\n",
        "    lgb = None\n",
        "\n",
        "\n",
        "def read_data(train_path, sample_path):\n",
        "    print(\"Reading train parquet...\")\n",
        "    df = pd.read_parquet(train_path)\n",
        "    print(f\"Loaded interactions: {len(df):,}\")\n",
        "    sample = pd.read_csv(sample_path)\n",
        "    print(f\"Loaded sample submission: {len(sample):,} rows\")\n",
        "    return df, sample\n",
        "\n",
        "\n",
        "def time_split_make_labels(df, days_for_target=7):\n",
        "    # assume 'date' is integer day index\n",
        "    max_day = int(df['date'].max())\n",
        "    split_day = max_day - days_for_target\n",
        "    print(f\"Max day in data: {max_day}, using last {days_for_target} days as target (days > {split_day})\")\n",
        "\n",
        "    train_df = df[df['date'] <= split_day].copy()\n",
        "    target_df = df[df['date'] > split_day].copy()\n",
        "    print(f\"Train interactions: {len(train_df):,}, target interactions: {len(target_df):,}\")\n",
        "    return train_df, target_df, split_day\n",
        "\n",
        "\n",
        "def make_stats(train_df):\n",
        "    print(\"Building basic user/item statistics...\")\n",
        "    user_clicks = train_df.groupby('user_id').size().rename('user_total_clicks')\n",
        "    user_nunique_items = train_df.groupby('user_id')['item_id'].nunique().rename('user_nunique_items')\n",
        "    item_clicks = train_df.groupby('item_id').size().rename('item_total_clicks')\n",
        "    item_last_day = train_df.groupby('item_id')['date'].max().rename('item_last_day')\n",
        "\n",
        "    user_stats = pd.concat([user_clicks, user_nunique_items], axis=1).reset_index()\n",
        "    item_stats = pd.concat([item_clicks, item_last_day], axis=1).reset_index()\n",
        "\n",
        "    return user_stats, item_stats\n",
        "\n",
        "\n",
        "def generate_candidates(train_df, sample_users, item_stats, topk_popular=500, max_user_history=200, random_neg_per_user=200, seed=42):\n",
        "    \"\"\"\n",
        "    Candidate generation strategy:\n",
        "    - for each user: include their historical items (most recent first up to max_user_history)\n",
        "    - add top popular items (global)\n",
        "    - add some random negatives sampled from popular pool\n",
        "    Returns dict: user_id -> set(item_ids)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    # top popular items globally\n",
        "    popular_items = item_stats.sort_values('item_total_clicks', ascending=False)['item_id'].values\n",
        "    popular_top = popular_items[:topk_popular]\n",
        "\n",
        "    # user history (most recent first)\n",
        "    user_history = train_df.sort_values(['user_id', 'date'], ascending=[True, False]).groupby('user_id')['item_id'].apply(list)\n",
        "\n",
        "    candidates = {}\n",
        "    pop_pool = list(popular_top)\n",
        "\n",
        "    for uid in sample_users:\n",
        "        cands = []\n",
        "        if uid in user_history.index:\n",
        "            # take last N\n",
        "            hist = user_history.loc[uid][:max_user_history]\n",
        "            cands.extend(hist)\n",
        "        # add popular\n",
        "        cands.extend(pop_pool[:200])\n",
        "        # random negatives from popular pool\n",
        "        neg = rng.choice(pop_pool, size=min(random_neg_per_user, len(pop_pool)), replace=False).tolist()\n",
        "        cands.extend(neg)\n",
        "        candidates[uid] = set(cands)\n",
        "\n",
        "    print(f\"Generated candidates for {len(candidates)} users; avg candidates per user ~ {np.mean([len(v) for v in candidates.values()]):.1f}\")\n",
        "    return candidates\n",
        "\n",
        "\n",
        "def build_feature_table(train_df, target_df, candidates, user_stats, item_stats, split_day):\n",
        "    \"\"\"\n",
        "    Build training table: for each user-item candidate produce features and label (1 if in target_df)\n",
        "    \"\"\"\n",
        "    print(\"Building feature table (this may use memory)...\")\n",
        "    # map stats\n",
        "    user_stats = user_stats.set_index('user_id')\n",
        "    item_stats = item_stats.set_index('item_id')\n",
        "\n",
        "    # build a set of positive pairs (from target_df)\n",
        "    pos_pairs = set(zip(target_df['user_id'], target_df['item_id']))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for i, (uid, items) in enumerate(candidates.items()):\n",
        "        if (i+1) % 10000 == 0:\n",
        "            print(f\"Processed {i+1} users for features...\")\n",
        "        # prefetch user stats\n",
        "        if uid in user_stats.index:\n",
        "            u_total = int(user_stats.at[uid, 'user_total_clicks'])\n",
        "            u_nitems = int(user_stats.at[uid, 'user_nunique_items'])\n",
        "        else:\n",
        "            u_total = 0\n",
        "            u_nitems = 0\n",
        "\n",
        "        # get user's last interaction day from train_df (if present)\n",
        "        user_last_day = train_df[train_df['user_id'] == uid]['date'].max() if uid in train_df['user_id'].values else np.nan\n",
        "\n",
        "        for iid in items:\n",
        "            if iid in item_stats.index:\n",
        "                i_total = int(item_stats.at[iid, 'item_total_clicks'])\n",
        "                i_last = int(item_stats.at[iid, 'item_last_day'])\n",
        "            else:\n",
        "                i_total = 0\n",
        "                i_last = -999\n",
        "\n",
        "            # user-item historical count in train\n",
        "            ui_count = ((train_df['user_id'] == uid) & (train_df['item_id'] == iid)).sum()\n",
        "\n",
        "            recency = split_day - i_last  # how many days since last item click in train\n",
        "\n",
        "            label = 1 if (uid, iid) in pos_pairs else 0\n",
        "\n",
        "            rows.append((uid, iid, u_total, u_nitems, i_total, ui_count, recency, label))\n",
        "\n",
        "    feats = pd.DataFrame(rows, columns=['user_id','item_id','user_total_clicks','user_nunique_items','item_total_clicks','ui_count','recency','label'])\n",
        "    print(f\"Feature table size: {len(feats):,}\")\n",
        "    return feats\n",
        "\n",
        "\n",
        "def train_pointwise_model(feats, num_boost_round=1000, early_stopping_rounds=50):\n",
        "    if lgb is None:\n",
        "        raise ImportError(\"lightgbm is required to train the model. Please install it (pip install lightgbm)\")\n",
        "\n",
        "    X = feats[['user_total_clicks','user_nunique_items','item_total_clicks','ui_count','recency']]\n",
        "    y = feats['label']\n",
        "\n",
        "    # simple stratified split maintaining label distribution\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    dval = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 63,\n",
        "        'seed': 42,\n",
        "    }\n",
        "\n",
        "    print(\"Training LightGBM...\")\n",
        "    model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], valid_names=['train','val'],\n",
        "                      num_boost_round=num_boost_round, early_stopping_rounds=early_stopping_rounds, verbose_eval=50)\n",
        "\n",
        "    # evaluate AP@20 in a rough way on validation users\n",
        "    print(\"Evaluating on validation set (approx AP) ...\")\n",
        "    val_preds = model.predict(X_val)\n",
        "    try:\n",
        "        ap = average_precision_score(y_val, val_preds)\n",
        "        print(f\"Validation average precision (binary): {ap:.6f}\")\n",
        "    except Exception:\n",
        "        print(\"Couldn't compute average_precision_score (maybe all labels same)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_topk(model, candidates, user_stats, item_stats, split_day, topk=20):\n",
        "    print(\"Scoring candidates and producing top-k per user...\")\n",
        "    rows = []\n",
        "    user_stats = user_stats.set_index('user_id')\n",
        "    item_stats = item_stats.set_index('item_id')\n",
        "\n",
        "    for i, (uid, items) in enumerate(candidates.items()):\n",
        "        if (i+1) % 10000 == 0:\n",
        "            print(f\"Scored {i+1} users...\")\n",
        "        feats = []\n",
        "        iids = []\n",
        "        for iid in items:\n",
        "            if uid in user_stats.index:\n",
        "                u_total = int(user_stats.at[uid, 'user_total_clicks'])\n",
        "                u_nitems = int(user_stats.at[uid, 'user_nunique_items'])\n",
        "            else:\n",
        "                u_total = 0\n",
        "                u_nitems = 0\n",
        "            if iid in item_stats.index:\n",
        "                i_total = int(item_stats.at[iid, 'item_total_clicks'])\n",
        "                i_last = int(item_stats.at[iid, 'item_last_day'])\n",
        "            else:\n",
        "                i_total = 0\n",
        "                i_last = -999\n",
        "            ui_count = 0  # we don't keep full train_df here for speed\n",
        "            recency = split_day - i_last\n",
        "            feats.append([u_total, u_nitems, i_total, ui_count, recency])\n",
        "            iids.append(iid)\n",
        "        X = np.array(feats)\n",
        "        scores = model.predict(X)\n",
        "        top_idx = np.argsort(-scores)[:topk]\n",
        "        top_items = [iids[idx] for idx in top_idx]\n",
        "        rows.append((uid, top_items))\n",
        "    print(\"Done scoring all users\")\n",
        "    return rows\n",
        "\n",
        "\n",
        "def save_submission_long(rows, out_path):\n",
        "    # rows: list of (user_id, [item_ids])\n",
        "    out_rows = []\n",
        "    for uid, items in rows:\n",
        "        for iid in items:\n",
        "            out_rows.append((uid, iid))\n",
        "    sub_df = pd.DataFrame(out_rows, columns=['user_id','item_id'])\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    print(f\"Saved long-format submission to {out_path}\")\n",
        "\n",
        "\n",
        "def save_submission_space(rows, out_path):\n",
        "    out_rows = []\n",
        "    for uid, items in rows:\n",
        "        out_rows.append((uid, ' '.join(map(str, items))))\n",
        "    sub_df = pd.DataFrame(out_rows, columns=['user_id','predictions'])\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    print(f\"Saved space-separated submission to {out_path}\")\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    df, sample = read_data(args.train, args.sample)\n",
        "    train_df, target_df, split_day = time_split_make_labels(df, days_for_target=args.days_target)\n",
        "\n",
        "    user_stats, item_stats = make_stats(train_df)\n",
        "\n",
        "    sample_users = sample['user_id'].unique().tolist()\n",
        "\n",
        "    candidates = generate_candidates(train_df, sample_users, item_stats,\n",
        "                                     topk_popular=args.topk_popular,\n",
        "                                     max_user_history=args.max_user_history,\n",
        "                                     random_neg_per_user=args.random_neg)\n",
        "\n",
        "    feats = build_feature_table(train_df, target_df, candidates, user_stats, item_stats, split_day)\n",
        "\n",
        "    # balance down negative samples for faster training (optional)\n",
        "    pos = feats[feats['label'] == 1]\n",
        "    neg = feats[feats['label'] == 0]\n",
        "    if len(neg) > 5 * len(pos):\n",
        "        neg = neg.sample(n=min(len(neg), 5 * len(pos)), random_state=42)\n",
        "    feats_bal = pd.concat([pos, neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    model = train_pointwise_model(feats_bal, num_boost_round=args.num_boost_round, early_stopping_rounds=args.early_stop)\n",
        "\n",
        "    # predict\n",
        "    rows = predict_topk(model, candidates, user_stats, item_stats, split_day, topk=args.topk)\n",
        "\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    save_submission_long(rows, os.path.join(args.out_dir, 'submission_long.csv'))\n",
        "    save_submission_space(rows, os.path.join(args.out_dir, 'submission_space.csv'))\n",
        "\n",
        "    print(\"All done.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train', type=str, required=True, help='train_data.pq')\n",
        "    parser.add_argument('--sample', type=str, required=True, help='sample_submission.csv')\n",
        "    parser.add_argument('--out_dir', type=str, default='./output')\n",
        "    parser.add_argument('--days_target', type=int, default=7)\n",
        "    parser.add_argument('--topk_popular', type=int, default=500)\n",
        "    parser.add_argument('--max_user_history', type=int, default=200)\n",
        "    parser.add_argument('--random_neg', type=int, default=200)\n",
        "    parser.add_argument('--topk', type=int, default=20)\n",
        "    parser.add_argument('--num_boost_round', type=int, default=1000)\n",
        "    parser.add_argument('--early_stop', type=int, default=50)\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "metadata": {
        "trusted": true,
        "id": "EHViKcU5cErg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "recsys_pointwise_kaggle.py\n",
        "\n",
        "Pointwise baseline для Kaggle Notebook.\n",
        "1. Читает train_data.pq и sample_submission.csv (должны быть в /kaggle/input/...)\n",
        "2. Делит по времени: последние 7 дней — как целевой период\n",
        "3. Генерирует кандидатов: история пользователя, популярные товары, случайные негативы\n",
        "4. Строит простые признаки (user/item статистики, recency)\n",
        "5. Обучает LightGBM (binary pointwise)\n",
        "6. Предсказывает топ-20 item_id для каждого user_id\n",
        "7. Сохраняет submission.csv в /kaggle/working/\n",
        "\n",
        "Требования: pandas, numpy, pyarrow, lightgbm, scikit-learn\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# === CONFIG ===\n",
        "TRAIN_PATH = '/kaggle/input/reccomend/train_data.pq'  # изменить под структуру датасета\n",
        "SAMPLE_PATH = '/kaggle/input/asddbfd/sample_submission (1).csv'\n",
        "OUT_PATH = '/kaggle/working/submission.csv'\n",
        "\n",
        "DAYS_TARGET = 7\n",
        "TOPK_POPULAR = 500\n",
        "MAX_USER_HISTORY = 200\n",
        "RANDOM_NEG = 200\n",
        "TOPK = 20\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "# === STEP 1: LOAD DATA ===\n",
        "print('Reading data...')\n",
        "df = pd.read_parquet(TRAIN_PATH)\n",
        "sample = pd.read_csv(SAMPLE_PATH)\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "# === STEP 2: TIME SPLIT ===\n",
        "max_day = int(df['date'].max())\n",
        "split_day = max_day - DAYS_TARGET\n",
        "train_df = df[df['date'] <= split_day].copy()\n",
        "target_df = df[df['date'] > split_day].copy()\n",
        "print(f\"Train days ≤ {split_day}, target days > {split_day}\")\n",
        "\n",
        "# === STEP 3: BASIC STATS ===\n",
        "user_clicks = train_df.groupby('user_id').size().rename('user_total_clicks')\n",
        "user_nunique_items = train_df.groupby('user_id')['item_id'].nunique().rename('user_nunique_items')\n",
        "item_clicks = train_df.groupby('item_id').size().rename('item_total_clicks')\n",
        "item_last_day = train_df.groupby('item_id')['date'].max().rename('item_last_day')\n",
        "\n",
        "user_stats = pd.concat([user_clicks, user_nunique_items], axis=1).reset_index()\n",
        "item_stats = pd.concat([item_clicks, item_last_day], axis=1).reset_index()\n",
        "\n",
        "# === STEP 4: CANDIDATES ===\n",
        "np.random.seed(SEED)\n",
        "popular_items = item_stats.sort_values('item_total_clicks', ascending=False)['item_id'].values[:TOPK_POPULAR]\n",
        "user_history = train_df.sort_values(['user_id','date'], ascending=[True, False]).groupby('user_id')['item_id'].apply(list)\n",
        "\n",
        "candidates = {}\n",
        "for uid in sample['user_id'].unique():\n",
        "    cands = []\n",
        "    if uid in user_history.index:\n",
        "        cands.extend(user_history.loc[uid][:MAX_USER_HISTORY])\n",
        "    cands.extend(popular_items[:200])\n",
        "    cands.extend(np.random.choice(popular_items, size=min(RANDOM_NEG, len(popular_items)), replace=False))\n",
        "    candidates[uid] = list(set(cands))\n",
        "\n",
        "print(f\"Generated candidates for {len(candidates)} users\")\n",
        "\n",
        "# === STEP 5: FEATURE TABLE ===\n",
        "user_stats = user_stats.set_index('user_id')\n",
        "item_stats = item_stats.set_index('item_id')\n",
        "\n",
        "pos_pairs = set(zip(target_df['user_id'], target_df['item_id']))\n",
        "\n",
        "rows = []\n",
        "for uid, items in candidates.items():\n",
        "    if uid in user_stats.index:\n",
        "        u_total = user_stats.at[uid, 'user_total_clicks']\n",
        "        u_nitems = user_stats.at[uid, 'user_nunique_items']\n",
        "    else:\n",
        "        u_total, u_nitems = 0, 0\n",
        "    for iid in items:\n",
        "        if iid in item_stats.index:\n",
        "            i_total = item_stats.at[iid, 'item_total_clicks']\n",
        "            i_last = item_stats.at[iid, 'item_last_day']\n",
        "        else:\n",
        "            i_total, i_last = 0, -999\n",
        "        recency = split_day - i_last\n",
        "        ui_count = ((train_df['user_id'] == uid) & (train_df['item_id'] == iid)).sum()\n",
        "        label = 1 if (uid, iid) in pos_pairs else 0\n",
        "        rows.append((uid, iid, u_total, u_nitems, i_total, ui_count, recency, label))\n",
        "\n",
        "feats = pd.DataFrame(rows, columns=['user_id','item_id','user_total_clicks','user_nunique_items','item_total_clicks','ui_count','recency','label'])\n",
        "print(f\"Feature table shape: {feats.shape}\")\n",
        "\n",
        "# Balance for faster training\n",
        "pos = feats[feats['label'] == 1]\n",
        "neg = feats[feats['label'] == 0]\n",
        "if len(neg) > 5 * len(pos):\n",
        "    neg = neg.sample(n=5 * len(pos), random_state=SEED)\n",
        "feats_bal = pd.concat([pos, neg]).sample(frac=1, random_state=SEED)\n",
        "\n",
        "# === STEP 6: TRAIN MODEL (RankNet) ===\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Подготовка данных\n",
        "X = feats_bal[['user_total_clicks','user_nunique_items','item_total_clicks','ui_count','recency']]\n",
        "y = feats_bal['label']\n",
        "groups = feats_bal.groupby('user_id').size().values  # количество кандидатов на каждого пользователя\n",
        "\n",
        "# Разделение по пользователям, а не по строкам\n",
        "unique_users = feats_bal['user_id'].unique()\n",
        "train_users, val_users = train_test_split(unique_users, test_size=0.15, random_state=SEED)\n",
        "\n",
        "train_mask = feats_bal['user_id'].isin(train_users)\n",
        "val_mask = feats_bal['user_id'].isin(val_users)\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val, y_val = X[val_mask], y[val_mask]\n",
        "\n",
        "group_train = feats_bal[train_mask].groupby('user_id').size().values\n",
        "group_val = feats_bal[val_mask].groupby('user_id').size().values\n",
        "\n",
        "# LightGBM Datasets\n",
        "dtrain = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
        "dval = lgb.Dataset(X_val, label=y_val, group=group_val)\n",
        "\n",
        "params = {\n",
        "    'objective': 'rank_xendcg',  # RankNet / LambdaRank / Xendcg (все поддерживаются)\n",
        "    'metric': 'ndcg',\n",
        "    'ndcg_eval_at': [5, 10],\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 63,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'boosting': 'gbdt',\n",
        "    'verbosity': -1,\n",
        "    'seed': SEED\n",
        "}\n",
        "\n",
        "EPOCHS = 1\n",
        "pbar = tqdm(total=EPOCHS, desc=\"Training RankNet\", position=0, leave=True)\n",
        "\n",
        "def tqdm_callback(env):\n",
        "    pbar.update(1)\n",
        "    if env.iteration % 10 == 0:\n",
        "        ndcg_val = env.evaluation_result_list[1][2]\n",
        "        pbar.set_postfix({'iter': env.iteration, 'val_ndcg': f\"{ndcg_val:.4f}\"})\n",
        "\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    valid_sets=[dtrain, dval],\n",
        "    valid_names=['train','val'],\n",
        "    num_boost_round=EPOCHS,\n",
        "    early_stopping_rounds=30,\n",
        "    verbose_eval=False,\n",
        "    callbacks=[tqdm_callback]\n",
        ")\n",
        "\n",
        "pbar.close()\n",
        "print(\"Best iteration:\", model.best_iteration)\n",
        "\n",
        "\n",
        "\n",
        "# === STEP 7: PREDICT TOPK ===\n",
        "rows_pred = []\n",
        "for uid, items in candidates.items():\n",
        "    feats_user = []\n",
        "    iids = []\n",
        "    if uid in user_stats.index:\n",
        "        u_total = user_stats.at[uid, 'user_total_clicks']\n",
        "        u_nitems = user_stats.at[uid, 'user_nunique_items']\n",
        "    else:\n",
        "        u_total, u_nitems = 0, 0\n",
        "    for iid in items:\n",
        "        if iid in item_stats.index:\n",
        "            i_total = item_stats.at[iid, 'item_total_clicks']\n",
        "            i_last = item_stats.at[iid, 'item_last_day']\n",
        "        else:\n",
        "            i_total, i_last = 0, -999\n",
        "        recency = split_day - i_last\n",
        "        feats_user.append([u_total, u_nitems, i_total, 0, recency])\n",
        "        iids.append(iid)\n",
        "    X_pred = np.array(feats_user)\n",
        "    scores = model.predict(X_pred)\n",
        "    top_items = [iids[i] for i in np.argsort(-scores)[:TOPK]]\n",
        "    rows_pred.append((uid, ' '.join(map(str, top_items))))\n",
        "\n",
        "submission = pd.DataFrame(rows_pred, columns=['user_id','predictions'])\n",
        "submission.to_csv(OUT_PATH, index=False)\n",
        "print(f'Saved submission to {OUT_PATH}')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-13T12:36:42.513782Z",
          "iopub.execute_input": "2025-10-13T12:36:42.514503Z",
          "iopub.status.idle": "2025-10-13T12:40:37.102607Z",
          "shell.execute_reply.started": "2025-10-13T12:36:42.514479Z",
          "shell.execute_reply": "2025-10-13T12:40:37.101648Z"
        },
        "id": "UX_rtzz_cErh",
        "outputId": "5f20ab46-462f-4302-b4c9-b99f1103391a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Reading data...\nTrain shape: (8777975, 3), Sample: (5864600, 2)\nTrain days ≤ 39, target days > 39\nGenerated candidates for 293230 users\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/513811776.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mi_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mrecency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_day\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mui_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_pairs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_nitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mui_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6117\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6119\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         if not is_cmp and (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TEST_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# === CONFIG ===\n",
        "TRAIN_PATH = '/kaggle/input/reccomend/train_data.pq'\n",
        "SAMPLE_PATH = '/kaggle/input/asddbfd/sample_submission (1).csv'\n",
        "OUT_PATH = '/kaggle/working/submission.csv'\n",
        "SEED = 42\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DAYS_TARGET = 7\n",
        "TOPK = 20\n",
        "TOPK_POPULAR = 3000\n",
        "MAX_USER_HISTORY = 20\n",
        "MAX_USERS = 50000      # ⚠️ обучаем только на части пользователей\n",
        "CANDS_PER_USER = 200\n",
        "RANDOM_NEG = 50\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# === STEP 1: LOAD ===\n",
        "print(\"Reading data...\")\n",
        "df = pd.read_parquet(TRAIN_PATH)\n",
        "sample = pd.read_csv(SAMPLE_PATH)\n",
        "print(f\"Train shape: {df.shape}, Sample: {sample.shape}\")\n",
        "\n",
        "# === STEP 2: TIME SPLIT ===\n",
        "max_day = int(df['date'].max())\n",
        "split_day = max_day - DAYS_TARGET\n",
        "train_df = df[df['date'] <= split_day]\n",
        "target_df = df[df['date'] > split_day]\n",
        "\n",
        "print(f\"Train ≤ {split_day}, Target > {split_day}\")\n",
        "\n",
        "# === STEP 3: BASIC STATS ===\n",
        "user_clicks = train_df.groupby('user_id').size().rename('user_total_clicks')\n",
        "user_nunique_items = train_df.groupby('user_id')['item_id'].nunique().rename('user_nunique_items')\n",
        "item_clicks = train_df.groupby('item_id').size().rename('item_total_clicks')\n",
        "item_last_day = train_df.groupby('item_id')['date'].max().rename('item_last_day')\n",
        "\n",
        "user_stats = pd.concat([user_clicks, user_nunique_items], axis=1)\n",
        "item_stats = pd.concat([item_clicks, item_last_day], axis=1)\n",
        "\n",
        "# ограничим пользователей\n",
        "active_users = user_clicks.sort_values(ascending=False).head(MAX_USERS).index\n",
        "sample = sample[sample['user_id'].isin(active_users)]\n",
        "print(f\"Using {len(sample)} most active users for training\")\n",
        "\n",
        "# === STEP 4: CANDIDATES (быстро) ===\n",
        "popular_items = item_stats.sort_values('item_total_clicks', ascending=False)['item_id'].values[:TOPK_POPULAR]\n",
        "user_history = (\n",
        "    train_df[train_df['user_id'].isin(active_users)]\n",
        "    .sort_values(['user_id', 'date'], ascending=[True, False])\n",
        "    .groupby('user_id')['item_id']\n",
        "    .apply(lambda x: x.iloc[:MAX_USER_HISTORY].tolist())\n",
        ")\n",
        "\n",
        "def get_candidates(uid):\n",
        "    cands = []\n",
        "    if uid in user_history:\n",
        "        cands += user_history[uid]\n",
        "    cands += list(popular_items[:100])\n",
        "    if len(cands) < CANDS_PER_USER:\n",
        "        cands += list(np.random.choice(popular_items, size=CANDS_PER_USER - len(cands), replace=False))\n",
        "    return list(set(cands))\n",
        "\n",
        "print(\"Generating candidates (subset)...\")\n",
        "candidates = {uid: get_candidates(uid) for uid in tqdm(sample['user_id'])}\n",
        "\n",
        "# === STEP 5: FEATURE TABLE (векторизовано) ===\n",
        "def build_features(uids, cands_dict):\n",
        "    feats = []\n",
        "    for uid in uids:\n",
        "        items = cands_dict[uid]\n",
        "        u_total = user_stats.at[uid, 'user_total_clicks'] if uid in user_stats.index else 0\n",
        "        u_nitems = user_stats.at[uid, 'user_nunique_items'] if uid in user_stats.index else 0\n",
        "        user_part = np.array([[uid, iid, u_total, u_nitems] for iid in items])\n",
        "        feats.append(user_part)\n",
        "    feats = np.vstack(feats)\n",
        "    feats = pd.DataFrame(feats, columns=['user_id', 'item_id', 'user_total_clicks', 'user_nunique_items'])\n",
        "    feats['item_total_clicks'] = item_stats.reindex(feats['item_id'].values)['item_total_clicks'].fillna(0).values\n",
        "    feats['recency'] = split_day - item_stats.reindex(feats['item_id'].values)['item_last_day'].fillna(-999).values\n",
        "    feats['ui_count'] = (\n",
        "        train_df[['user_id','item_id']].value_counts().reindex(list(zip(feats['user_id'], feats['item_id'])))\n",
        "        .fillna(0).values\n",
        "    )\n",
        "    feats['label'] = feats.apply(lambda r: 1 if (r['user_id'], r['item_id']) in set(zip(target_df['user_id'], target_df['item_id'])) else 0, axis=1)\n",
        "    return feats\n",
        "\n",
        "print(\"Building compact feature table...\")\n",
        "feats = build_features(sample['user_id'].values, candidates)\n",
        "print(f\"Feature table: {feats.shape}\")\n",
        "\n",
        "# Ограничим количество примеров для быстрой тренировки\n",
        "feats_small = feats.sample(n=min(100_000, len(feats)), random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# === STEP 6: MAKE PAIRS ===\n",
        "def make_pairs_fast(df):\n",
        "    pairs = []\n",
        "    grouped = df.groupby('user_id')\n",
        "    for uid, g in grouped:\n",
        "        pos = g[g['label'] == 1]\n",
        "        neg = g[g['label'] == 0]\n",
        "        if len(pos) == 0 or len(neg) == 0:\n",
        "            continue\n",
        "        neg_sample = neg.sample(min(RANDOM_NEG, len(neg)), random_state=SEED)\n",
        "        for _, p in pos.iterrows():\n",
        "            for _, n in neg_sample.iterrows():\n",
        "                pairs.append((p, n))\n",
        "    return pairs\n",
        "\n",
        "print(\"Generating pairs (subset)...\")\n",
        "pairs = make_pairs_fast(feats_small)\n",
        "print(f\"Pairs: {len(pairs)}\")\n",
        "\n",
        "def tensorize(rows):\n",
        "    X1 = np.array([[r[0]['user_total_clicks'], r[0]['user_nunique_items'], r[0]['item_total_clicks'], r[0]['ui_count'], r[0]['recency']] for r in rows], dtype=np.float32)\n",
        "    X2 = np.array([[r[1]['user_total_clicks'], r[1]['user_nunique_items'], r[1]['item_total_clicks'], r[1]['ui_count'], r[1]['recency']] for r in rows], dtype=np.float32)\n",
        "    return torch.tensor(X1), torch.tensor(X2)\n",
        "\n",
        "X1, X2 = tensorize(pairs)\n",
        "\n",
        "# === STEP 7: RANKNET MODEL ===\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = RankNet().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# === STEP 8: TRAIN LOOP (SUBSET) ===\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "pbar = tqdm(range(EPOCHS), desc=\"Training RankNet\")\n",
        "for epoch in pbar:\n",
        "    perm = torch.randperm(len(X1))\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for i in range(0, len(X1), BATCH_SIZE):\n",
        "        idx = perm[i:i+BATCH_SIZE]\n",
        "        x1 = X1[idx].to(DEVICE)\n",
        "        x2 = X2[idx].to(DEVICE)\n",
        "        s1 = model(x1)\n",
        "        s2 = model(x2)\n",
        "        p = torch.sigmoid(s1 - s2)\n",
        "        y = torch.ones_like(p)\n",
        "        loss = criterion(p, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    pbar.set_postfix({'loss': f'{total_loss:.4f}'})\n",
        "\n",
        "# === STEP 9: PREDICT TOPK ===\n",
        "print(\"Predicting for submission (subset)...\")\n",
        "model.eval()\n",
        "rows_pred = []\n",
        "with torch.no_grad():\n",
        "    for uid, items in tqdm(candidates.items()):\n",
        "        feats_user = []\n",
        "        iids = []\n",
        "        if uid in user_stats.index:\n",
        "            u_total = user_stats.at[uid, 'user_total_clicks']\n",
        "            u_nitems = user_stats.at[uid, 'user_nunique_items']\n",
        "        else:\n",
        "            u_total, u_nitems = 0, 0\n",
        "        for iid in items:\n",
        "            if iid in item_stats.index:\n",
        "                i_total = item_stats.at[iid, 'item_total_clicks']\n",
        "                i_last = item_stats.at[iid, 'item_last_day']\n",
        "            else:\n",
        "                i_total, i_last = 0, -999\n",
        "            recency = split_day - i_last\n",
        "            feats_user.append([u_total, u_nitems, i_total, 0, recency])\n",
        "            iids.append(iid)\n",
        "        X_pred = torch.tensor(feats_user, dtype=torch.float32).to(DEVICE)\n",
        "        scores = model(X_pred).squeeze().cpu().numpy()\n",
        "        top_items = [iids[i] for i in np.argsort(-scores)[:TOPK]]\n",
        "        rows_pred.append((uid, ' '.join(map(str, top_items))))\n",
        "\n",
        "submission = pd.DataFrame(rows_pred, columns=['user_id','predictions'])\n",
        "submission.to_csv(OUT_PATH, index=False)\n",
        "print(f'Saved submission to {OUT_PATH}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-13T12:47:23.413449Z",
          "iopub.execute_input": "2025-10-13T12:47:23.41375Z",
          "iopub.status.idle": "2025-10-13T12:47:28.544234Z",
          "shell.execute_reply.started": "2025-10-13T12:47:23.413729Z",
          "shell.execute_reply": "2025-10-13T12:47:28.543222Z"
        },
        "id": "LAyiEeI3cErj",
        "outputId": "38faa71a-111d-41fe-b24a-f2371857351e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Reading data...\nTrain shape: (8777975, 3), Sample: (5864600, 2)\nTrain ≤ 39, Target > 39\nUsing 353680 most active users for training\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'item_id'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/247481053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# === STEP 4: CANDIDATES (быстро) ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mpopular_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'item_total_clicks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTOPK_POPULAR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m user_history = (\n\u001b[1;32m     56\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'item_id'"
          ],
          "ename": "KeyError",
          "evalue": "'item_id'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек\n",
        "# !pip install pandas numpy scikit-surprise scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from surprise import SVDpp, Dataset, Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# === 1. Загрузка данных ===\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "# Попробуем безопасно загрузить файл отзывов\n",
        "reviews_df = pd.read_csv(\n",
        "    \"/kaggle/input/hshussu/reviews.txv/reviews.tsv\",\n",
        "    sep='\\t',\n",
        "    on_bad_lines='skip',  # пропуск строк с ошибками\n",
        "    quoting=3,            # ignore quotes\n",
        "    engine='python'       # более устойчивый парсер\n",
        ")\n",
        "\n",
        "print(reviews_df.head())\n",
        "\n",
        "\n",
        "# === 2. Подготовка признаков текстов ===\n",
        "# Заменяем None/NaN на пустую строку\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "\n",
        "# Объединяем все отзывы для каждого заведения\n",
        "reviews_agg = reviews_df.groupby(\"id\")['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "\n",
        "print(reviews_agg.head())\n",
        "\n",
        "# TF-IDF векторизация\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "reviews_tfidf = tfidf.fit_transform(reviews_agg[\"text\"])\n",
        "print(111)\n",
        "# Объединяем TF-IDF с базовыми признаками\n",
        "# Для простоты возьмём только категорию и географические признаки\n",
        "feature_cols = ['category', 'traffic_300m', 'traffic_1000m', 'mean_income_300m', 'mean_income_1000m']\n",
        "train_features = train_df[feature_cols].copy()\n",
        "test_features = test_df[feature_cols].copy()\n",
        "print(111)\n",
        "# Кодируем категориальные признаки\n",
        "train_features = pd.get_dummies(train_features, columns=['category'])\n",
        "test_features = pd.get_dummies(test_features, columns=['category'])\n",
        "print(111)\n",
        "# Выравниваем колонки\n",
        "train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# === 3. Подготовка данных для SVD++ ===\n",
        "# SVD++ в библиотеке surprise работает с \"user-item-rating\" форматом\n",
        "# Здесь заведем условного пользователя \"system\", чтобы использовать SVD++ для заведений\n",
        "svd_data = train_df[['id', 'target']].copy()\n",
        "svd_data['user'] = 'system'  # фиксированный \"пользователь\"\n",
        "print(111)\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(svd_data[['user', 'id', 'target']], reader)\n",
        "\n",
        "trainset = data.build_full_trainset()\n",
        "print(111)\n",
        "# === 4. Обучение SVD++ ===\n",
        "algo = SVDpp(n_factors=50, n_epochs=1, lr_all=0.005, reg_all=0.02,verbose=True)\n",
        "algo.fit(trainset)\n",
        "\n",
        "# === 5. Предсказание для тестового набора ===\n",
        "test_df['target'] = test_df['id'].apply(lambda x: algo.predict('system', x).est)\n",
        "\n",
        "# === 6. Сохранение результатов ===\n",
        "test_df[['id', 'target']].to_csv(\"submission.csv\", index=False)\n",
        "print(\"Результаты сохранены в submission.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-13T13:51:42.793554Z",
          "iopub.execute_input": "2025-10-13T13:51:42.79421Z",
          "iopub.status.idle": "2025-10-13T14:03:29.292821Z",
          "shell.execute_reply.started": "2025-10-13T13:51:42.79419Z",
          "shell.execute_reply": "2025-10-13T14:03:29.292132Z"
        },
        "id": "P9zJUSgncErk",
        "outputId": "d883b0c1-8f39-4b61-bc25-2150c4bd7f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "      id                                               text\n0  43591  Мармелад в целом неплохой, но цены завышены, м...\n1  43591  Не нравится, что товар выложен открыто, слишко...\n2  43591  Часто попадается сухой мармелад, дубовый впере...\n3  43591  Персонал был одет в костюмы пиратов, а ассорти...\n4  43591  Вкусный мармелад с широким ассортиментом форм,...\n                                                  id text\n0                   - ЧАСТЬ I (всё прошло хорошо) –\"     \n1   - широкий ассортимент пива, включающий не тол...     \n2                  Но цена могла бы быть чуть ниже.\"     \n3  !!!!! Здесь установлено современное оборудован...     \n4                        !!!!!! Плохой сервис !!!!!!     \n111\n111\n111\n111\n111\n processing epoch 0\nРезультаты сохранены в submission.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from surprise import SVD, Dataset, Reader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# === 1. Загрузка данных ===\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "\n",
        "# Попробуем безопасно загрузить файл отзывов\n",
        "reviews_df = pd.read_csv(\n",
        "    \"/kaggle/input/hshussu/reviews.txv/reviews.tsv\",\n",
        "    sep='\\t',\n",
        "    on_bad_lines='skip',\n",
        "    quoting=3,\n",
        "    engine='python'\n",
        ")\n",
        "\n",
        "# === 2. Подготовка текстов ===\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "reviews_agg = reviews_df.groupby(\"id\")['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "\n",
        "# TF-IDF векторизация\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "reviews_tfidf = tfidf.fit_transform(reviews_agg[\"text\"])\n",
        "\n",
        "# === 3. Подготовка данных для FunkSVD ===\n",
        "svd_data = train_df[['id', 'target']].copy()\n",
        "svd_data['user'] = 'system'  # фиктивный пользователь\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(svd_data[['user', 'id', 'target']], reader)\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# === 4. Обучение FunkSVD ===\n",
        "algo = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, verbose=True)\n",
        "algo.fit(trainset)\n",
        "\n",
        "# === 5. Предсказание ===\n",
        "test_df['target'] = test_df['id'].apply(lambda x: algo.predict('system', x).est)\n",
        "\n",
        "# === 6. Сохранение результатов ===\n",
        "test_df[['id', 'target']].to_csv(\"submission_funksvd.csv\", index=False)\n",
        "print(\"✅ Результаты сохранены в submission_funksvd.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-14T12:45:34.645414Z",
          "iopub.execute_input": "2025-10-14T12:45:34.646022Z",
          "iopub.status.idle": "2025-10-14T12:45:51.044453Z",
          "shell.execute_reply.started": "2025-10-14T12:45:34.645986Z",
          "shell.execute_reply": "2025-10-14T12:45:51.043654Z"
        },
        "id": "SMiIY9iEcErk",
        "outputId": "5d83d777-6c05-4bd6-fe30-47ece80f8bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Processing epoch 0\nProcessing epoch 1\nProcessing epoch 2\nProcessing epoch 3\nProcessing epoch 4\nProcessing epoch 5\nProcessing epoch 6\nProcessing epoch 7\nProcessing epoch 8\nProcessing epoch 9\nProcessing epoch 10\nProcessing epoch 11\nProcessing epoch 12\nProcessing epoch 13\nProcessing epoch 14\nProcessing epoch 15\nProcessing epoch 16\nProcessing epoch 17\nProcessing epoch 18\nProcessing epoch 19\n✅ Результаты сохранены в submission_funksvd.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# === 1. Загрузка данных ===\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "reviews_df = pd.read_csv(\n",
        "    \"/kaggle/input/hshussu/reviews.txv/reviews.tsv\",\n",
        "    sep='\\t',\n",
        "    on_bad_lines='skip',\n",
        "    quoting=3,\n",
        "    engine='python'\n",
        ")\n",
        "\n",
        "# === 2. Подготовка текстов ===\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "reviews_df['id'] = reviews_df['id'].astype(str)\n",
        "\n",
        "reviews_agg = reviews_df.groupby(\"id\")['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "\n",
        "# Приводим id к одному типу и в train/test\n",
        "train_df['id'] = train_df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "# Присоединяем текстовые признаки\n",
        "train_df = train_df.merge(reviews_agg, on='id', how='left')\n",
        "test_df = test_df.merge(reviews_agg, on='id', how='left')\n",
        "train_df['text'] = train_df['text'].fillna(\"\")\n",
        "test_df['text'] = test_df['text'].fillna(\"\")\n",
        "\n",
        "\n",
        "train_tfidf = tfidf.transform(train_df['text'])\n",
        "test_tfidf = tfidf.transform(test_df['text'])\n",
        "\n",
        "# === 3. Табличные признаки ===\n",
        "feature_cols = ['category', 'traffic_300m', 'traffic_1000m', 'mean_income_300m', 'mean_income_1000m']\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Убедимся, что только числовые колонки\n",
        "# === Кодирование категориальных признаков ===\n",
        "train_features = pd.get_dummies(train_df[feature_cols], columns=['category'])\n",
        "test_features = pd.get_dummies(test_df[feature_cols], columns=['category'])\n",
        "\n",
        "# === Выравнивание фич ===\n",
        "train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Приведение типов\n",
        "train_features = train_features.astype('float32')\n",
        "test_features = test_features.astype('float32')\n",
        "\n",
        "# TF-IDF признаки\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "train_tfidf = tfidf.fit_transform(train_df[\"text\"])\n",
        "test_tfidf = tfidf.transform(test_df[\"text\"])\n",
        "\n",
        "# Объединение TF-IDF + числовые признаки\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "X_train = hstack([csr_matrix(train_features.values), train_tfidf]).tocsr()\n",
        "X_test = hstack([csr_matrix(test_features.values), test_tfidf]).tocsr()\n",
        "\n",
        "y_train = train_df['target']\n",
        "\n",
        "\n",
        "\n",
        "# === 4. Обучение XGBoost ===\n",
        "model = XGBRegressor(\n",
        "    n_estimators=380,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist',\n",
        "    objective='reg:squarederror'\n",
        ")\n",
        "\n",
        "# Разделим немного для контроля качества\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=True)\n",
        "\n",
        "# === 5. Предсказание ===\n",
        "test_df['target'] = model.predict(X_test)\n",
        "test_df['target'] = np.clip(test_df['target'], 1, 5)  # диапазон [1,5]\n",
        "\n",
        "# === 6. Сохранение ===\n",
        "test_df[['id', 'target']].to_csv(\"submission_xgboost.csv\", index=False)\n",
        "print(\"✅ Результаты сохранены в submission_xgboost.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-14T13:30:54.367183Z",
          "iopub.execute_input": "2025-10-14T13:30:54.367476Z",
          "iopub.status.idle": "2025-10-14T13:32:39.137347Z",
          "shell.execute_reply.started": "2025-10-14T13:30:54.367455Z",
          "shell.execute_reply": "2025-10-14T13:32:39.136238Z"
        },
        "id": "zTAy5qsCcErl",
        "outputId": "bbc1069c-f0d3-46c6-d609-f4b43b8e605f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[0]\tvalidation_0-rmse:1.15075\n[1]\tvalidation_0-rmse:1.13537\n[2]\tvalidation_0-rmse:1.12082\n[3]\tvalidation_0-rmse:1.10700\n[4]\tvalidation_0-rmse:1.09465\n[5]\tvalidation_0-rmse:1.08336\n[6]\tvalidation_0-rmse:1.07250\n[7]\tvalidation_0-rmse:1.06349\n[8]\tvalidation_0-rmse:1.05397\n[9]\tvalidation_0-rmse:1.04489\n[10]\tvalidation_0-rmse:1.03743\n[11]\tvalidation_0-rmse:1.02972\n[12]\tvalidation_0-rmse:1.02339\n[13]\tvalidation_0-rmse:1.01622\n[14]\tvalidation_0-rmse:1.01083\n[15]\tvalidation_0-rmse:1.00505\n[16]\tvalidation_0-rmse:0.99941\n[17]\tvalidation_0-rmse:0.99408\n[18]\tvalidation_0-rmse:0.98919\n[19]\tvalidation_0-rmse:0.98465\n[20]\tvalidation_0-rmse:0.98027\n[21]\tvalidation_0-rmse:0.97606\n[22]\tvalidation_0-rmse:0.97250\n[23]\tvalidation_0-rmse:0.96903\n[24]\tvalidation_0-rmse:0.96576\n[25]\tvalidation_0-rmse:0.96236\n[26]\tvalidation_0-rmse:0.95914\n[27]\tvalidation_0-rmse:0.95682\n[28]\tvalidation_0-rmse:0.95471\n[29]\tvalidation_0-rmse:0.95234\n[30]\tvalidation_0-rmse:0.94980\n[31]\tvalidation_0-rmse:0.94725\n[32]\tvalidation_0-rmse:0.94489\n[33]\tvalidation_0-rmse:0.94260\n[34]\tvalidation_0-rmse:0.94084\n[35]\tvalidation_0-rmse:0.93842\n[36]\tvalidation_0-rmse:0.93646\n[37]\tvalidation_0-rmse:0.93472\n[38]\tvalidation_0-rmse:0.93343\n[39]\tvalidation_0-rmse:0.93194\n[40]\tvalidation_0-rmse:0.93029\n[41]\tvalidation_0-rmse:0.92898\n[42]\tvalidation_0-rmse:0.92744\n[43]\tvalidation_0-rmse:0.92584\n[44]\tvalidation_0-rmse:0.92444\n[45]\tvalidation_0-rmse:0.92289\n[46]\tvalidation_0-rmse:0.92159\n[47]\tvalidation_0-rmse:0.92052\n[48]\tvalidation_0-rmse:0.91917\n[49]\tvalidation_0-rmse:0.91812\n[50]\tvalidation_0-rmse:0.91667\n[51]\tvalidation_0-rmse:0.91522\n[52]\tvalidation_0-rmse:0.91421\n[53]\tvalidation_0-rmse:0.91294\n[54]\tvalidation_0-rmse:0.91183\n[55]\tvalidation_0-rmse:0.91071\n[56]\tvalidation_0-rmse:0.90962\n[57]\tvalidation_0-rmse:0.90907\n[58]\tvalidation_0-rmse:0.90807\n[59]\tvalidation_0-rmse:0.90744\n[60]\tvalidation_0-rmse:0.90640\n[61]\tvalidation_0-rmse:0.90566\n[62]\tvalidation_0-rmse:0.90512\n[63]\tvalidation_0-rmse:0.90410\n[64]\tvalidation_0-rmse:0.90339\n[65]\tvalidation_0-rmse:0.90253\n[66]\tvalidation_0-rmse:0.90195\n[67]\tvalidation_0-rmse:0.90146\n[68]\tvalidation_0-rmse:0.90096\n[69]\tvalidation_0-rmse:0.90028\n[70]\tvalidation_0-rmse:0.89961\n[71]\tvalidation_0-rmse:0.89932\n[72]\tvalidation_0-rmse:0.89880\n[73]\tvalidation_0-rmse:0.89837\n[74]\tvalidation_0-rmse:0.89791\n[75]\tvalidation_0-rmse:0.89729\n[76]\tvalidation_0-rmse:0.89659\n[77]\tvalidation_0-rmse:0.89603\n[78]\tvalidation_0-rmse:0.89564\n[79]\tvalidation_0-rmse:0.89523\n[80]\tvalidation_0-rmse:0.89471\n[81]\tvalidation_0-rmse:0.89417\n[82]\tvalidation_0-rmse:0.89371\n[83]\tvalidation_0-rmse:0.89305\n[84]\tvalidation_0-rmse:0.89241\n[85]\tvalidation_0-rmse:0.89173\n[86]\tvalidation_0-rmse:0.89126\n[87]\tvalidation_0-rmse:0.89073\n[88]\tvalidation_0-rmse:0.89031\n[89]\tvalidation_0-rmse:0.88976\n[90]\tvalidation_0-rmse:0.88938\n[91]\tvalidation_0-rmse:0.88911\n[92]\tvalidation_0-rmse:0.88885\n[93]\tvalidation_0-rmse:0.88848\n[94]\tvalidation_0-rmse:0.88809\n[95]\tvalidation_0-rmse:0.88776\n[96]\tvalidation_0-rmse:0.88742\n[97]\tvalidation_0-rmse:0.88695\n[98]\tvalidation_0-rmse:0.88682\n[99]\tvalidation_0-rmse:0.88667\n[100]\tvalidation_0-rmse:0.88650\n[101]\tvalidation_0-rmse:0.88622\n[102]\tvalidation_0-rmse:0.88585\n[103]\tvalidation_0-rmse:0.88564\n[104]\tvalidation_0-rmse:0.88546\n[105]\tvalidation_0-rmse:0.88521\n[106]\tvalidation_0-rmse:0.88486\n[107]\tvalidation_0-rmse:0.88445\n[108]\tvalidation_0-rmse:0.88422\n[109]\tvalidation_0-rmse:0.88391\n[110]\tvalidation_0-rmse:0.88369\n[111]\tvalidation_0-rmse:0.88339\n[112]\tvalidation_0-rmse:0.88302\n[113]\tvalidation_0-rmse:0.88279\n[114]\tvalidation_0-rmse:0.88263\n[115]\tvalidation_0-rmse:0.88253\n[116]\tvalidation_0-rmse:0.88251\n[117]\tvalidation_0-rmse:0.88231\n[118]\tvalidation_0-rmse:0.88217\n[119]\tvalidation_0-rmse:0.88201\n[120]\tvalidation_0-rmse:0.88179\n[121]\tvalidation_0-rmse:0.88159\n[122]\tvalidation_0-rmse:0.88153\n[123]\tvalidation_0-rmse:0.88133\n[124]\tvalidation_0-rmse:0.88125\n[125]\tvalidation_0-rmse:0.88116\n[126]\tvalidation_0-rmse:0.88100\n[127]\tvalidation_0-rmse:0.88093\n[128]\tvalidation_0-rmse:0.88085\n[129]\tvalidation_0-rmse:0.88080\n[130]\tvalidation_0-rmse:0.88056\n[131]\tvalidation_0-rmse:0.88046\n[132]\tvalidation_0-rmse:0.88041\n[133]\tvalidation_0-rmse:0.88015\n[134]\tvalidation_0-rmse:0.87990\n[135]\tvalidation_0-rmse:0.87993\n[136]\tvalidation_0-rmse:0.87983\n[137]\tvalidation_0-rmse:0.87968\n[138]\tvalidation_0-rmse:0.87933\n[139]\tvalidation_0-rmse:0.87926\n[140]\tvalidation_0-rmse:0.87900\n[141]\tvalidation_0-rmse:0.87891\n[142]\tvalidation_0-rmse:0.87872\n[143]\tvalidation_0-rmse:0.87864\n[144]\tvalidation_0-rmse:0.87857\n[145]\tvalidation_0-rmse:0.87830\n[146]\tvalidation_0-rmse:0.87810\n[147]\tvalidation_0-rmse:0.87802\n[148]\tvalidation_0-rmse:0.87783\n[149]\tvalidation_0-rmse:0.87765\n[150]\tvalidation_0-rmse:0.87768\n[151]\tvalidation_0-rmse:0.87751\n[152]\tvalidation_0-rmse:0.87736\n[153]\tvalidation_0-rmse:0.87723\n[154]\tvalidation_0-rmse:0.87712\n[155]\tvalidation_0-rmse:0.87717\n[156]\tvalidation_0-rmse:0.87708\n[157]\tvalidation_0-rmse:0.87701\n[158]\tvalidation_0-rmse:0.87681\n[159]\tvalidation_0-rmse:0.87666\n[160]\tvalidation_0-rmse:0.87644\n[161]\tvalidation_0-rmse:0.87648\n[162]\tvalidation_0-rmse:0.87625\n[163]\tvalidation_0-rmse:0.87600\n[164]\tvalidation_0-rmse:0.87591\n[165]\tvalidation_0-rmse:0.87587\n[166]\tvalidation_0-rmse:0.87586\n[167]\tvalidation_0-rmse:0.87587\n[168]\tvalidation_0-rmse:0.87572\n[169]\tvalidation_0-rmse:0.87585\n[170]\tvalidation_0-rmse:0.87576\n[171]\tvalidation_0-rmse:0.87559\n[172]\tvalidation_0-rmse:0.87556\n[173]\tvalidation_0-rmse:0.87540\n[174]\tvalidation_0-rmse:0.87524\n[175]\tvalidation_0-rmse:0.87508\n[176]\tvalidation_0-rmse:0.87476\n[177]\tvalidation_0-rmse:0.87469\n[178]\tvalidation_0-rmse:0.87449\n[179]\tvalidation_0-rmse:0.87429\n[180]\tvalidation_0-rmse:0.87414\n[181]\tvalidation_0-rmse:0.87398\n[182]\tvalidation_0-rmse:0.87393\n[183]\tvalidation_0-rmse:0.87391\n[184]\tvalidation_0-rmse:0.87401\n[185]\tvalidation_0-rmse:0.87392\n[186]\tvalidation_0-rmse:0.87390\n[187]\tvalidation_0-rmse:0.87384\n[188]\tvalidation_0-rmse:0.87375\n[189]\tvalidation_0-rmse:0.87369\n[190]\tvalidation_0-rmse:0.87364\n[191]\tvalidation_0-rmse:0.87356\n[192]\tvalidation_0-rmse:0.87348\n[193]\tvalidation_0-rmse:0.87343\n[194]\tvalidation_0-rmse:0.87313\n[195]\tvalidation_0-rmse:0.87319\n[196]\tvalidation_0-rmse:0.87319\n[197]\tvalidation_0-rmse:0.87311\n[198]\tvalidation_0-rmse:0.87304\n[199]\tvalidation_0-rmse:0.87299\n[200]\tvalidation_0-rmse:0.87302\n[201]\tvalidation_0-rmse:0.87313\n[202]\tvalidation_0-rmse:0.87303\n[203]\tvalidation_0-rmse:0.87296\n[204]\tvalidation_0-rmse:0.87293\n[205]\tvalidation_0-rmse:0.87282\n[206]\tvalidation_0-rmse:0.87280\n[207]\tvalidation_0-rmse:0.87274\n[208]\tvalidation_0-rmse:0.87268\n[209]\tvalidation_0-rmse:0.87256\n[210]\tvalidation_0-rmse:0.87267\n[211]\tvalidation_0-rmse:0.87260\n[212]\tvalidation_0-rmse:0.87254\n[213]\tvalidation_0-rmse:0.87251\n[214]\tvalidation_0-rmse:0.87247\n[215]\tvalidation_0-rmse:0.87238\n[216]\tvalidation_0-rmse:0.87231\n[217]\tvalidation_0-rmse:0.87234\n[218]\tvalidation_0-rmse:0.87229\n[219]\tvalidation_0-rmse:0.87216\n[220]\tvalidation_0-rmse:0.87209\n[221]\tvalidation_0-rmse:0.87213\n[222]\tvalidation_0-rmse:0.87209\n[223]\tvalidation_0-rmse:0.87199\n[224]\tvalidation_0-rmse:0.87186\n[225]\tvalidation_0-rmse:0.87191\n[226]\tvalidation_0-rmse:0.87192\n[227]\tvalidation_0-rmse:0.87194\n[228]\tvalidation_0-rmse:0.87191\n[229]\tvalidation_0-rmse:0.87195\n[230]\tvalidation_0-rmse:0.87192\n[231]\tvalidation_0-rmse:0.87183\n[232]\tvalidation_0-rmse:0.87178\n[233]\tvalidation_0-rmse:0.87176\n[234]\tvalidation_0-rmse:0.87178\n[235]\tvalidation_0-rmse:0.87185\n[236]\tvalidation_0-rmse:0.87189\n[237]\tvalidation_0-rmse:0.87181\n[238]\tvalidation_0-rmse:0.87183\n[239]\tvalidation_0-rmse:0.87175\n[240]\tvalidation_0-rmse:0.87166\n[241]\tvalidation_0-rmse:0.87160\n[242]\tvalidation_0-rmse:0.87159\n[243]\tvalidation_0-rmse:0.87156\n[244]\tvalidation_0-rmse:0.87157\n[245]\tvalidation_0-rmse:0.87152\n[246]\tvalidation_0-rmse:0.87147\n[247]\tvalidation_0-rmse:0.87139\n[248]\tvalidation_0-rmse:0.87146\n[249]\tvalidation_0-rmse:0.87120\n[250]\tvalidation_0-rmse:0.87115\n[251]\tvalidation_0-rmse:0.87110\n[252]\tvalidation_0-rmse:0.87107\n[253]\tvalidation_0-rmse:0.87099\n[254]\tvalidation_0-rmse:0.87082\n[255]\tvalidation_0-rmse:0.87059\n[256]\tvalidation_0-rmse:0.87058\n[257]\tvalidation_0-rmse:0.87058\n[258]\tvalidation_0-rmse:0.87046\n[259]\tvalidation_0-rmse:0.87041\n[260]\tvalidation_0-rmse:0.87034\n[261]\tvalidation_0-rmse:0.87029\n[262]\tvalidation_0-rmse:0.87016\n[263]\tvalidation_0-rmse:0.87014\n[264]\tvalidation_0-rmse:0.87005\n[265]\tvalidation_0-rmse:0.87007\n[266]\tvalidation_0-rmse:0.87000\n[267]\tvalidation_0-rmse:0.86983\n[268]\tvalidation_0-rmse:0.86979\n[269]\tvalidation_0-rmse:0.86974\n[270]\tvalidation_0-rmse:0.86969\n[271]\tvalidation_0-rmse:0.86953\n[272]\tvalidation_0-rmse:0.86945\n[273]\tvalidation_0-rmse:0.86949\n[274]\tvalidation_0-rmse:0.86949\n[275]\tvalidation_0-rmse:0.86945\n[276]\tvalidation_0-rmse:0.86929\n[277]\tvalidation_0-rmse:0.86929\n[278]\tvalidation_0-rmse:0.86925\n[279]\tvalidation_0-rmse:0.86914\n[280]\tvalidation_0-rmse:0.86910\n[281]\tvalidation_0-rmse:0.86906\n[282]\tvalidation_0-rmse:0.86901\n[283]\tvalidation_0-rmse:0.86902\n[284]\tvalidation_0-rmse:0.86889\n[285]\tvalidation_0-rmse:0.86898\n[286]\tvalidation_0-rmse:0.86893\n[287]\tvalidation_0-rmse:0.86879\n[288]\tvalidation_0-rmse:0.86871\n[289]\tvalidation_0-rmse:0.86874\n[290]\tvalidation_0-rmse:0.86868\n[291]\tvalidation_0-rmse:0.86875\n[292]\tvalidation_0-rmse:0.86873\n[293]\tvalidation_0-rmse:0.86857\n[294]\tvalidation_0-rmse:0.86849\n[295]\tvalidation_0-rmse:0.86846\n[296]\tvalidation_0-rmse:0.86845\n[297]\tvalidation_0-rmse:0.86847\n[298]\tvalidation_0-rmse:0.86851\n[299]\tvalidation_0-rmse:0.86855\n[300]\tvalidation_0-rmse:0.86849\n[301]\tvalidation_0-rmse:0.86859\n[302]\tvalidation_0-rmse:0.86861\n[303]\tvalidation_0-rmse:0.86854\n[304]\tvalidation_0-rmse:0.86847\n[305]\tvalidation_0-rmse:0.86845\n[306]\tvalidation_0-rmse:0.86848\n[307]\tvalidation_0-rmse:0.86841\n[308]\tvalidation_0-rmse:0.86840\n[309]\tvalidation_0-rmse:0.86840\n[310]\tvalidation_0-rmse:0.86840\n[311]\tvalidation_0-rmse:0.86841\n[312]\tvalidation_0-rmse:0.86832\n[313]\tvalidation_0-rmse:0.86838\n[314]\tvalidation_0-rmse:0.86829\n[315]\tvalidation_0-rmse:0.86822\n[316]\tvalidation_0-rmse:0.86814\n[317]\tvalidation_0-rmse:0.86812\n[318]\tvalidation_0-rmse:0.86811\n[319]\tvalidation_0-rmse:0.86802\n[320]\tvalidation_0-rmse:0.86800\n[321]\tvalidation_0-rmse:0.86794\n[322]\tvalidation_0-rmse:0.86794\n[323]\tvalidation_0-rmse:0.86793\n[324]\tvalidation_0-rmse:0.86791\n[325]\tvalidation_0-rmse:0.86790\n[326]\tvalidation_0-rmse:0.86786\n[327]\tvalidation_0-rmse:0.86771\n[328]\tvalidation_0-rmse:0.86776\n[329]\tvalidation_0-rmse:0.86779\n[330]\tvalidation_0-rmse:0.86779\n[331]\tvalidation_0-rmse:0.86786\n[332]\tvalidation_0-rmse:0.86789\n[333]\tvalidation_0-rmse:0.86783\n[334]\tvalidation_0-rmse:0.86785\n[335]\tvalidation_0-rmse:0.86786\n[336]\tvalidation_0-rmse:0.86780\n[337]\tvalidation_0-rmse:0.86784\n[338]\tvalidation_0-rmse:0.86780\n[339]\tvalidation_0-rmse:0.86784\n[340]\tvalidation_0-rmse:0.86787\n[341]\tvalidation_0-rmse:0.86793\n[342]\tvalidation_0-rmse:0.86794\n[343]\tvalidation_0-rmse:0.86788\n[344]\tvalidation_0-rmse:0.86788\n[345]\tvalidation_0-rmse:0.86790\n[346]\tvalidation_0-rmse:0.86784\n[347]\tvalidation_0-rmse:0.86792\n✅ Результаты сохранены в submission_xgboost.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============ 1. Загрузка данных ============\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "reviews_df = pd.read_csv(\n",
        "    \"/kaggle/input/hshussu/reviews.txv/reviews.tsv\",\n",
        "    sep='\\t',\n",
        "    on_bad_lines='skip',\n",
        "    quoting=3,\n",
        "    engine='python'\n",
        ")\n",
        "\n",
        "# ============ 2. Подготовка текстов ============\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "reviews_df['id'] = reviews_df['id'].astype(str)\n",
        "train_df['id'] = train_df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "reviews_agg = reviews_df.groupby(\"id\")['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "\n",
        "train_df = train_df.merge(reviews_agg, on='id', how='left')\n",
        "test_df = test_df.merge(reviews_agg, on='id', how='left')\n",
        "\n",
        "train_df['text'] = train_df['text'].fillna(\"\")\n",
        "test_df['text'] = test_df['text'].fillna(\"\")\n",
        "\n",
        "# ============ 3. TF-IDF ============\n",
        "tfidf = TfidfVectorizer(max_features=512)\n",
        "train_tfidf = tfidf.fit_transform(train_df[\"text\"]).astype(np.float32)\n",
        "test_tfidf = tfidf.transform(test_df[\"text\"]).astype(np.float32)\n",
        "\n",
        "# ============ 4. Табличные признаки ============\n",
        "feature_cols = ['category', 'traffic_300m', 'traffic_1000m', 'mean_income_300m', 'mean_income_1000m']\n",
        "train_features = pd.get_dummies(train_df[feature_cols], columns=['category'])\n",
        "test_features = pd.get_dummies(test_df[feature_cols], columns=['category'])\n",
        "train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_num = scaler.fit_transform(train_features)\n",
        "test_num = scaler.transform(test_features)\n",
        "\n",
        "# ============ 5. Torch Dataset ============\n",
        "class RecDataset(Dataset):\n",
        "    def __init__(self, tfidf, num, target=None):\n",
        "        self.tfidf = torch.tensor(tfidf.toarray(), dtype=torch.float32)\n",
        "        self.num = torch.tensor(num, dtype=torch.float32)\n",
        "        self.target = torch.tensor(target, dtype=torch.float32) if target is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.num)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.target is not None:\n",
        "            return self.tfidf[idx], self.num[idx], self.target[idx]\n",
        "        else:\n",
        "            return self.tfidf[idx], self.num[idx]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Создаем массив индексов\n",
        "idx = np.arange(train_df.shape[0])\n",
        "idx_train, idx_val = train_test_split(idx, test_size=0.1, random_state=42)\n",
        "\n",
        "# Разделяем TF-IDF\n",
        "X_tfidf_train = train_tfidf[idx_train]\n",
        "X_tfidf_val = train_tfidf[idx_val]\n",
        "\n",
        "# Разделяем числовые признаки\n",
        "X_num_train = train_num[idx_train]\n",
        "X_num_val = train_num[idx_val]\n",
        "\n",
        "# Разделяем целевую переменную\n",
        "y_train = train_df['target'].values[idx_train]\n",
        "y_val = train_df['target'].values[idx_val]\n",
        "\n",
        "# Создаем датасеты\n",
        "train_dataset = RecDataset(X_tfidf_train, X_num_train, y_train)\n",
        "val_dataset = RecDataset(X_tfidf_val, X_num_val, y_val)\n",
        "\n",
        "test_dataset = RecDataset(test_tfidf, test_num)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# ============ 6. Модель с Attention ============\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class HybridTransformer(nn.Module):\n",
        "    def __init__(self, text_dim, num_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.text_proj = nn.Linear(text_dim, hidden)\n",
        "        self.num_proj = nn.Linear(num_dim, hidden)\n",
        "        self.transformer = TransformerBlock(hidden, num_heads=4)\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, text, num):\n",
        "        text_emb = self.text_proj(text)\n",
        "        num_emb = self.num_proj(num)\n",
        "        # объединяем текст и табличные признаки в последовательность длиной 2\n",
        "        seq = torch.stack([text_emb, num_emb], dim=1)\n",
        "        out = self.transformer(seq)\n",
        "        # берём среднее по токенам\n",
        "        pooled = out.mean(dim=1)\n",
        "        return self.fc_out(pooled).squeeze(1)\n",
        "\n",
        "# ============ 7. Обучение ============\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HybridTransformer(text_dim=train_tfidf.shape[1], num_dim=train_num.shape[1]).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.L1Loss()  # MAE\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, num, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/5\"):\n",
        "        text, num, target = text.to(device), num.to(device), target.to(device)\n",
        "        pred = model(text, num)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Train loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ============ 8. Предсказание ============\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for text, num in DataLoader(test_dataset, batch_size=128):\n",
        "        text, num = text.to(device), num.to(device)\n",
        "        out = model(text, num)\n",
        "        preds.extend(out.cpu().numpy())\n",
        "\n",
        "test_df['target'] = np.clip(preds, 1, 5)\n",
        "test_df[['id', 'target']].to_csv(\"submission_transformer.csv\", index=False)\n",
        "print(\"✅ Результаты сохранены в submission_transformer.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-14T14:11:53.566507Z",
          "iopub.execute_input": "2025-10-14T14:11:53.568288Z",
          "iopub.status.idle": "2025-10-14T14:13:01.895994Z",
          "shell.execute_reply.started": "2025-10-14T14:11:53.568233Z",
          "shell.execute_reply": "2025-10-14T14:13:01.895226Z"
        },
        "id": "19QqHupjcErm",
        "outputId": "79bd0710-7ff7-4c51-90fc-e75b073b3b74"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/5: 100%|██████████| 579/579 [00:04<00:00, 119.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5551\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/5: 100%|██████████| 579/579 [00:04<00:00, 115.84it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4733\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/5: 100%|██████████| 579/579 [00:04<00:00, 115.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4641\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/5: 100%|██████████| 579/579 [00:04<00:00, 116.99it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4598\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/5: 100%|██████████| 579/579 [00:04<00:00, 118.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4597\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/5: 100%|██████████| 579/579 [00:04<00:00, 118.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4605\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/5: 100%|██████████| 579/579 [00:04<00:00, 119.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4541\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/5: 100%|██████████| 579/579 [00:04<00:00, 116.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4549\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/5: 100%|██████████| 579/579 [00:04<00:00, 116.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4523\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/5: 100%|██████████| 579/579 [00:05<00:00, 113.55it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4517\n✅ Результаты сохранены в submission_transformer.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === 1. Подготовка ===\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "reviews_df = pd.read_csv(\"/kaggle/input/hshussu/reviews.txv/reviews.tsv\", sep='\\t', on_bad_lines='skip', quoting=3, engine='python')\n",
        "\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "reviews_df = reviews_df.groupby(\"id\")[\"text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
        "train_df = train_df.merge(reviews_df, on=\"id\", how=\"left\").fillna(\"\")\n",
        "test_df = test_df.merge(reviews_df, on=\"id\", how=\"left\").fillna(\"\")\n",
        "\n",
        "# === 2. Числовые признаки ===\n",
        "num_cols = ['traffic_300m','traffic_1000m','mean_income_300m','mean_income_1000m']\n",
        "scaler = StandardScaler()\n",
        "train_num = scaler.fit_transform(train_df[num_cols])\n",
        "test_num = scaler.transform(test_df[num_cols])\n",
        "\n",
        "# === 3. Tokenizer ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class POIDataset(Dataset):\n",
        "    def __init__(self, texts, nums, targets=None):\n",
        "        self.texts = texts\n",
        "        self.nums = torch.tensor(nums, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32) if targets is not None else None\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
        "        out = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        out[\"num\"] = self.nums[idx]\n",
        "        if self.targets is not None:\n",
        "            out[\"target\"] = self.targets[idx]\n",
        "        return out\n",
        "\n",
        "# === 4. Модель ===\n",
        "class TransModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fc_num = nn.Linear(4, 64)\n",
        "        self.head = nn.Sequential(nn.Linear(64 + 768, 128), nn.ReLU(), nn.Linear(128, 1))\n",
        "    def forward(self, input_ids, attention_mask, num):\n",
        "        x_text = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:,0,:]\n",
        "        x_num = self.fc_num(num)\n",
        "        x = torch.cat([x_text, x_num], dim=1)\n",
        "        return self.head(x).squeeze(1)\n",
        "\n",
        "# === 5. Обучение ===\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_df[\"text\"].tolist(), train_num, train_df[\"target\"].values, test_size=0.1, random_state=42)\n",
        "train_ds = POIDataset(X_train, y_train, y_train)\n",
        "val_ds = POIDataset(X_val, y_val, y_val)\n",
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=8)\n",
        "\n",
        "model = TransModel().to(\"cuda\")\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    for batch in train_dl:\n",
        "        opt.zero_grad()\n",
        "        out = model(batch[\"input_ids\"].to(\"cuda\"), batch[\"attention_mask\"].to(\"cuda\"), batch[\"num\"].to(\"cuda\"))\n",
        "        loss = loss_fn(out, batch[\"target\"].to(\"cuda\"))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    print(f\"Epoch {epoch+1} done\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T13:52:48.276413Z",
          "iopub.execute_input": "2025-10-16T13:52:48.276766Z",
          "iopub.status.idle": "2025-10-16T13:53:17.397991Z",
          "shell.execute_reply.started": "2025-10-16T13:52:48.276729Z",
          "shell.execute_reply": "2025-10-16T13:53:17.395726Z"
        },
        "id": "8MqBKZ8fcErm",
        "outputId": "4517ee09-8b27-4833-9194-147e0ee0d292"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/1689553896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mreviews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;31m# If argument passed to validate,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m                 ):\n\u001b[0;32m-> 1508\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;31m# datetimelikes must match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to merge on int64 and object columns for key 'id'. If you wish to proceed you should use pd.concat"
          ],
          "ename": "ValueError",
          "evalue": "You are trying to merge on int64 and object columns for key 'id'. If you wish to proceed you should use pd.concat",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# ======================================================\n",
        "# 0. CLI аргументы\n",
        "# ======================================================\n",
        "\n",
        "# ======================================================\n",
        "# 1. Загрузка данных\n",
        "# ======================================================\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep=\"\\t\")\n",
        "reviews_df = pd.read_csv(\"/kaggle/input/hshussu/reviews.txv/reviews.tsv\", sep=\"\\t\",\n",
        "                         on_bad_lines=\"skip\", quoting=3, engine=\"python\")\n",
        "\n",
        "# агрегируем отзывы\n",
        "reviews_df[\"text\"] = reviews_df[\"text\"].fillna(\"\")\n",
        "reviews_agg = reviews_df.groupby(\"id\")[\"text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
        "\n",
        "# Приводим id к строковому типу везде\n",
        "train_df[\"id\"] = train_df[\"id\"].astype(str)\n",
        "test_df[\"id\"] = test_df[\"id\"].astype(str)\n",
        "reviews_agg[\"id\"] = reviews_agg[\"id\"].astype(str)\n",
        "\n",
        "# Теперь безопасно объединяем\n",
        "train_df = train_df.merge(reviews_agg, on=\"id\", how=\"left\").fillna(\"\")\n",
        "test_df = test_df.merge(reviews_agg, on=\"id\", how=\"left\").fillna(\"\")\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 2. Общие числовые признаки\n",
        "# ======================================================\n",
        "num_cols = [\"traffic_300m\", \"traffic_1000m\", \"mean_income_300m\", \"mean_income_1000m\"]\n",
        "for col in num_cols:\n",
        "    train_df[col] = train_df[col].fillna(train_df[col].median())\n",
        "    test_df[col] = test_df[col].fillna(train_df[col].median())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_num = scaler.fit_transform(train_df[num_cols])\n",
        "test_num = scaler.transform(test_df[num_cols])\n",
        "\n",
        "# ======================================================\n",
        "# 3. Модели\n",
        "# ======================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from huggingface_hub import login, hf_hub_download\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "\n",
        "\n",
        "class POIDataset(Dataset):\n",
        "    def __init__(self, texts, nums, targets=None):\n",
        "        self.texts = texts\n",
        "        self.nums = torch.tensor(nums, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32) if targets is not None else None\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(self.texts[idx], truncation=True, padding=\"max_length\",\n",
        "                        max_length=128, return_tensors=\"pt\")\n",
        "        out = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        out[\"num\"] = self.nums[idx]\n",
        "        if self.targets is not None:\n",
        "            out[\"target\"] = self.targets[idx]\n",
        "        return out\n",
        "\n",
        "class TransModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fc_num = nn.Linear(4, 64)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(768 + 64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, input_ids, attention_mask, num):\n",
        "        x_text = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        x_num = self.fc_num(num)\n",
        "        x = torch.cat([x_text, x_num], dim=1)\n",
        "        return self.head(x).squeeze(1)\n",
        "\n",
        "X_train = train_test_split(train_df[\"text\"].tolist(), train_num, train_df[\"target\"].values,test_size=0.1)\n",
        "y_train = train_test_split(train_df[\"text\"].tolist(), train_num, train_df[\"target\"].values,test_size=0.1)\n",
        "train_ds = POIDataset(X_train)\n",
        "val_ds = POIDataset(X_val, y_val)\n",
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=8)\n",
        "\n",
        "model = TransModel().to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dl:\n",
        "        opt.zero_grad()\n",
        "        out = model(batch[\"input_ids\"].to(device),\n",
        "                    batch[\"attention_mask\"].to(device),\n",
        "                    batch[\"num\"].to(device))\n",
        "        loss = loss_fn(out, batch[\"target\"].to(device))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# предсказание\n",
        "model.eval()\n",
        "preds = []\n",
        "test_ds = POIDataset(test_df[\"text\"].tolist(), test_num)\n",
        "test_dl = DataLoader(test_ds, batch_size=16)\n",
        "with torch.no_grad():\n",
        "    for batch in test_dl:\n",
        "        out = model(batch[\"input_ids\"].to(device),\n",
        "                    batch[\"attention_mask\"].to(device),\n",
        "                    batch[\"num\"].to(device))\n",
        "        preds.extend(out.cpu().numpy())\n",
        "test_df[\"target\"] = np.clip(preds, 1, 5)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 4. Сохранение результата\n",
        "# ======================================================\n",
        "sub = test_df[[\"id\", \"target\"]]\n",
        "sub.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ Saved submission.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T14:16:01.531978Z",
          "iopub.execute_input": "2025-10-16T14:16:01.532354Z",
          "iopub.status.idle": "2025-10-16T14:16:07.81428Z",
          "shell.execute_reply.started": "2025-10-16T14:16:01.53233Z",
          "shell.execute_reply": "2025-10-16T14:16:07.81261Z"
        },
        "id": "ATLKGSxhcErn",
        "outputId": "ae8a1621-c24e-49ff-eec6-b7c986e889c7"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/2901794650.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: POIDataset.__init__() missing 1 required positional argument: 'nums'"
          ],
          "ename": "TypeError",
          "evalue": "POIDataset.__init__() missing 1 required positional argument: 'nums'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Пути к данным ===\n",
        "TRAIN_PATH = \"/kaggle/input/hshussu/train.tsv\"\n",
        "TEST_PATH = \"/kaggle/input/hshussu/test.tsv\"\n",
        "REVIEWS_PATH = \"/kaggle/input/hshussu/reviews.txv/reviews.tsv\"\n",
        "\n",
        "# === Чтение данных ===\n",
        "train = pd.read_csv(TRAIN_PATH, sep='\\t')\n",
        "test = pd.read_csv(TEST_PATH, sep='\\t')\n",
        "reviews = pd.read_csv(REVIEWS_PATH, sep='\\t')\n",
        "\n",
        "print(train.shape, test.shape, reviews.shape)\n",
        "print(train.columns[:20])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T14:30:13.316419Z",
          "iopub.execute_input": "2025-10-16T14:30:13.316702Z",
          "iopub.status.idle": "2025-10-16T14:30:17.085615Z",
          "shell.execute_reply.started": "2025-10-16T14:30:13.316672Z",
          "shell.execute_reply": "2025-10-16T14:30:17.084106Z"
        },
        "id": "-9d3S_gacErn",
        "outputId": "1cd06e07-de65-4780-f620-470cc6c32969"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "(41105, 286) (9276, 285) (440082, 2)\nIndex(['id', 'name', 'coordinates', 'category', 'address', 'target',\n       'traffic_300m', 'homes_300m', 'works_300m', 'female_300m',\n       'train_ticket_order_300m', 'mortgage_300m', 'recipes_300m',\n       'online_shops_300m', 'manga_300m', 'children_goods_300m',\n       'language_courses_300m', 'commercial_real_estate_purchase_300m',\n       'grocery_stores_300m', 'preschool_300m'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Целевая переменная\n",
        "train['target']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T14:30:26.880397Z",
          "iopub.execute_input": "2025-10-16T14:30:26.880688Z",
          "iopub.status.idle": "2025-10-16T14:30:26.890978Z",
          "shell.execute_reply.started": "2025-10-16T14:30:26.880669Z",
          "shell.execute_reply": "2025-10-16T14:30:26.889973Z"
        },
        "id": "cenJqOgccErn",
        "outputId": "59f6d946-bcda-4ac9-a9a3-62ddf42d2829"
      },
      "outputs": [
        {
          "execution_count": 47,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0        4.1\n1        3.6\n2        3.5\n3        4.0\n4        4.2\n        ... \n41100    3.5\n41101    3.6\n41102    0.0\n41103    3.9\n41104    3.7\nName: target, Length: 41105, dtype: float64"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Соединяем отзывы\n",
        "reviews_grouped = reviews.groupby('id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "#train = train.merge(reviews_grouped, on='id', how='left')\n",
        "#test = test.merge(reviews_grouped, on='id', how='left')\n",
        "\n",
        "\n",
        "cat_cols = ['category']\n",
        "for c in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[c] = le.fit_transform(train[c].astype(str))\n",
        "    test[c] = le.transform(test[c].astype(str))\n",
        "\n",
        "# Целевая переменная\n",
        "train['target']\n",
        "#train = train.drop(columns=['target', 'name', 'address'])\n",
        "test['id']\n",
        "#test = test.drop(columns=['name', 'address'])\n",
        "\n",
        "# Удалим текст для табличных моделей\n",
        "tab_cols = [c for c in train.columns if c not in ['id', 'text']]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T14:42:16.646765Z",
          "iopub.execute_input": "2025-10-16T14:42:16.647237Z",
          "iopub.status.idle": "2025-10-16T14:42:17.373573Z",
          "shell.execute_reply.started": "2025-10-16T14:42:16.647167Z",
          "shell.execute_reply": "2025-10-16T14:42:17.371641Z"
        },
        "id": "V5DoLhXCcErn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Используем text_features_train/test из предыдущего шага\n",
        "X_tab = train[tab_cols].fillna(0).values\n",
        "# Убедимся, что список признаков совпадает между train и test\n",
        "tab_cols = [c for c in train.columns if c not in ['id', 'text']]\n",
        "common_cols = [c for c in tab_cols if c in test.columns]\n",
        "\n",
        "X_tab = train[common_cols].fillna(0).values\n",
        "X_test_tab = test[common_cols].fillna(0).values\n",
        "\n",
        "X_test_tab = test[tab_cols].fillna(0).values\n",
        "\n",
        "X_train = np.hstack([X_tab, text_features_train])\n",
        "X_test = np.hstack([X_test_tab, text_features_test])\n",
        "\n",
        "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Torch Dataset\n",
        "class TabTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = TabTextDataset(X_train_t, y_train_t)\n",
        "val_ds = TabTextDataset(X_val_t, y_val_t)\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=256)\n",
        "\n",
        "# Модель\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).squeeze(1)\n",
        "\n",
        "model = MLP(X_train_t.shape[1])\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "        opt.zero_grad()\n",
        "        loss = loss_fn(model(xb), yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    model.eval()\n",
        "    val_loss = np.mean([loss_fn(model(xb), yb).item() for xb, yb in val_dl])\n",
        "    print(f\"Epoch {epoch+1}: val_MAE = {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T14:44:33.326276Z",
          "iopub.execute_input": "2025-10-16T14:44:33.32664Z",
          "iopub.status.idle": "2025-10-16T14:44:34.964724Z",
          "shell.execute_reply.started": "2025-10-16T14:44:33.326618Z",
          "shell.execute_reply": "2025-10-16T14:44:34.962486Z"
        },
        "id": "kjseDSGAcErn",
        "outputId": "73337a0a-fac9-4b2e-856d-04b4f2aa5934"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/519676683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_test_tab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcommon_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_test_tab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtab_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['name', 'address', 'target'] not in index\""
          ],
          "ename": "KeyError",
          "evalue": "\"['name', 'address', 'target'] not in index\"",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "#  MLP baseline for POI/Recommendation Task\n",
        "# =======================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ========== 1. Dataset ==========\n",
        "\n",
        "class RecDataset(Dataset):\n",
        "    def __init__(self, X_tfidf, X_num, y=None):\n",
        "        self.tfidf = torch.tensor(X_tfidf, dtype=torch.float32)\n",
        "        self.num = torch.tensor(X_num, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tfidf)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return self.tfidf[idx], self.num[idx], self.y[idx]\n",
        "        return self.tfidf[idx], self.num[idx]\n",
        "\n",
        "# ========== 2. Model ==========\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, tfidf_dim, num_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc_tfidf = nn.Sequential(\n",
        "            nn.Linear(tfidf_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc_num = nn.Sequential(\n",
        "            nn.Linear(num_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_tfidf, x_num):\n",
        "        t = self.fc_tfidf(x_tfidf)\n",
        "        n = self.fc_num(x_num)\n",
        "        x = torch.cat([t, n], dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ========== 3. Train function ==========\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_tfidf, x_num, y in loader:\n",
        "        x_tfidf, x_num, y = x_tfidf.to(device), x_num.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x_tfidf, x_num).squeeze()\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * len(y)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_epoch(model, loader, device):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_tfidf, x_num, y in loader:\n",
        "            x_tfidf, x_num = x_tfidf.to(device), x_num.to(device)\n",
        "            y_pred = model(x_tfidf, x_num).squeeze().cpu().numpy()\n",
        "            preds.extend(y_pred)\n",
        "            targets.extend(y.numpy())\n",
        "    return roc_auc_score(targets, preds)\n",
        "\n",
        "# ========== 4. Example usage ==========\n",
        "\n",
        "# допустим, у нас уже есть матрицы:\n",
        "# train_tfidf, train_num, train_df[\"target\"]\n",
        "\n",
        "# Разделяем\n",
        "X_train_tfidf, X_val_tfidf, X_train_num, X_val_num, y_train, y_val = train_test_split(\n",
        "    train_tfidf, train_num, train_df[\"target\"], test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_num = scaler.fit_transform(X_train_num)\n",
        "X_val_num = scaler.transform(X_val_num)\n",
        "\n",
        "train_ds = RecDataset(X_train_tfidf, X_train_num, y_train)\n",
        "val_ds = RecDataset(X_val_tfidf, X_val_num, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = MLPModel(tfidf_dim=X_train_tfidf.shape[1], num_dim=X_train_num.shape[1]).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# ========== 5. Training loop ==========\n",
        "\n",
        "best_auc = 0\n",
        "for epoch in range(10):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_auc = eval_epoch(model, val_loader, device)\n",
        "    best_auc = max(best_auc, val_auc)\n",
        "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_auc={val_auc:.4f}\")\n",
        "\n",
        "print(f\"✅ Best ROC-AUC: {best_auc:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T14:48:47.097302Z",
          "iopub.execute_input": "2025-10-16T14:48:47.097664Z",
          "iopub.status.idle": "2025-10-16T14:48:47.144937Z",
          "shell.execute_reply.started": "2025-10-16T14:48:47.097645Z",
          "shell.execute_reply": "2025-10-16T14:48:47.142991Z"
        },
        "id": "ldZtlJvrcEro",
        "outputId": "b8cb9401-1243-4483-d033-4c74f384b922"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/2234775961.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# допустим, у нас уже есть матрицы:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# train_tfidf, train_num, train_df[\"target\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mtrain_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;31m# Разделяем\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m X_train_tfidf, X_val_tfidf, X_train_num, X_val_num, y_train, y_val = train_test_split(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'tfidf' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 💡 MLP Model for Recommendation Task\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# 1. Load Data\n",
        "# ===============================\n",
        "\n",
        "# Пример: поменяй пути на свои файлы\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\")\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\")\n",
        "\n",
        "print(\"✅ Train shape:\", train_df.shape)\n",
        "print(\"✅ Test shape:\", test_df.shape)\n",
        "\n",
        "# Проверим столбцы\n",
        "print(\"Columns:\", list(train_df.columns))\n",
        "\n",
        "# ===============================\n",
        "# 2. Feature Engineering\n",
        "# ===============================\n",
        "\n",
        "# Пример: допустим, есть текстовые поля 'title', 'description'\n",
        "TEXT_COLS = [\"title\", \"description\"]\n",
        "NUM_COLS = [\"price\", \"rating\", \"num_reviews\"]  # замени на реальные числовые фичи\n",
        "\n",
        "# Заполним пропуски\n",
        "for col in TEXT_COLS:\n",
        "    train_df[col] = train_df[col].fillna(\"\")\n",
        "    test_df[col] = test_df[col].fillna(\"\")\n",
        "for col in NUM_COLS:\n",
        "    train_df[col] = train_df[col].fillna(0)\n",
        "    test_df[col] = test_df[col].fillna(0)\n",
        "\n",
        "# Объединим тексты в один\n",
        "train_texts = (train_df[TEXT_COLS[0]] + \" \" + train_df[TEXT_COLS[1]]).values\n",
        "test_texts = (test_df[TEXT_COLS[0]] + \" \" + test_df[TEXT_COLS[1]]).values\n",
        "\n",
        "# ===============================\n",
        "# 3. TF-IDF Features\n",
        "# ===============================\n",
        "print(\"🔧 Building TF-IDF features...\")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
        "train_tfidf = tfidf.fit_transform(train_texts)\n",
        "test_tfidf = tfidf.transform(test_texts)\n",
        "\n",
        "# Можно добавить SVD для ускорения (уменьшает размерность TF-IDF)\n",
        "svd = TruncatedSVD(n_components=256, random_state=42)\n",
        "train_tfidf = svd.fit_transform(train_tfidf)\n",
        "test_tfidf = svd.transform(test_tfidf)\n",
        "\n",
        "# ===============================\n",
        "# 4. Numeric Features\n",
        "# ===============================\n",
        "scaler = StandardScaler()\n",
        "train_num = scaler.fit_transform(train_df[NUM_COLS])\n",
        "test_num = scaler.transform(test_df[NUM_COLS])\n",
        "\n",
        "# ===============================\n",
        "# 5. Train/Validation Split\n",
        "# ===============================\n",
        "X_train_tfidf, X_val_tfidf, X_train_num, X_val_num, y_train, y_val = train_test_split(\n",
        "    train_tfidf, train_num, train_df[\"target\"], test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 6. Dataset and Dataloader\n",
        "# ===============================\n",
        "\n",
        "class RecDataset(Dataset):\n",
        "    def __init__(self, X_tfidf, X_num, y=None):\n",
        "        self.tfidf = torch.tensor(X_tfidf, dtype=torch.float32)\n",
        "        self.num = torch.tensor(X_num, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tfidf)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return self.tfidf[idx], self.num[idx], self.y[idx]\n",
        "        return self.tfidf[idx], self.num[idx]\n",
        "\n",
        "train_ds = RecDataset(X_train_tfidf, X_train_num, y_train)\n",
        "val_ds = RecDataset(X_val_tfidf, X_val_num, y_val)\n",
        "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n",
        "\n",
        "# ===============================\n",
        "# 7. MLP Model\n",
        "# ===============================\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, tfidf_dim, num_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc_tfidf = nn.Sequential(\n",
        "            nn.Linear(tfidf_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc_num = nn.Sequential(\n",
        "            nn.Linear(num_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_tfidf, x_num):\n",
        "        t = self.fc_tfidf(x_tfidf)\n",
        "        n = self.fc_num(x_num)\n",
        "        x = torch.cat([t, n], dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ===============================\n",
        "# 8. Training\n",
        "# ===============================\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = MLPModel(tfidf_dim=X_train_tfidf.shape[1], num_dim=X_train_num.shape[1]).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_tfidf, x_num, y in loader:\n",
        "        x_tfidf, x_num, y = x_tfidf.to(device), x_num.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x_tfidf, x_num).squeeze()\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * len(y)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_tfidf, x_num, y in loader:\n",
        "            x_tfidf, x_num = x_tfidf.to(device), x_num.to(device)\n",
        "            y_pred = model(x_tfidf, x_num).squeeze().cpu().numpy()\n",
        "            preds.extend(y_pred)\n",
        "            targets.extend(y.numpy())\n",
        "    return roc_auc_score(targets, preds)\n",
        "\n",
        "# ===============================\n",
        "# 9. Run Training\n",
        "# ===============================\n",
        "best_auc = 0\n",
        "for epoch in range(10):\n",
        "    train_loss = train_epoch(model, train_loader)\n",
        "    val_auc = eval_epoch(model, val_loader)\n",
        "    best_auc = max(best_auc, val_auc)\n",
        "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_auc={val_auc:.4f}\")\n",
        "\n",
        "print(f\"✅ Best ROC-AUC: {best_auc:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 10. Predict on Test\n",
        "# ===============================\n",
        "test_ds = RecDataset(test_tfidf, test_num)\n",
        "test_loader = DataLoader(test_ds, batch_size=1024, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for x_tfidf, x_num in test_loader:\n",
        "        x_tfidf, x_num = x_tfidf.to(device), x_num.to(device)\n",
        "        y_pred = model(x_tfidf, x_num).squeeze().cpu().numpy()\n",
        "        preds.extend(y_pred)\n",
        "\n",
        "submission = pd.DataFrame({\"id\": test_df[\"id\"], \"target\": preds})\n",
        "submission.to_csv(\"submission_mlp.csv\", index=False)\n",
        "print(\"💾 Saved submission_mlp.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-16T14:53:07.740516Z",
          "iopub.execute_input": "2025-10-16T14:53:07.740959Z",
          "iopub.status.idle": "2025-10-16T14:53:07.847947Z",
          "shell.execute_reply.started": "2025-10-16T14:53:07.740928Z",
          "shell.execute_reply": "2025-10-16T14:53:07.84655Z"
        },
        "id": "pSE5Fyt9cEro",
        "outputId": "b7bed825-194f-46b9-b9f4-3e6e777ff13b"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/3557929584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Пример: поменяй пути на свои файлы\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/hshussu/train.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/hshussu/test.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 6 fields in line 60, saw 7\n"
          ],
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 6 fields in line 60, saw 7\n",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ========== 1. Загрузка данных ==========\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/hshussu/train.tsv', sep='\\t')\n",
        "test = pd.read_csv('/kaggle/input/hshussu/test.tsv', sep='\\t')\n",
        "reviews = pd.read_csv('/kaggle/input/hshussu/reviews.txv/reviews.tsv', sep='\\t')\n",
        "\n",
        "# ========== 2. Объединение отзывов ==========\n",
        "reviews_grouped = reviews.groupby('id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "train = train.merge(reviews_grouped, on='id', how='left')\n",
        "test = test.merge(reviews_grouped, on='id', how='left')\n",
        "train['text'] = train['text'].fillna('')\n",
        "test['text'] = test['text'].fillna('')\n",
        "\n",
        "# ========== 3. Кодирование категориальных признаков ==========\n",
        "cat_cols = ['category']\n",
        "for c in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[c] = le.fit_transform(train[c].astype(str))\n",
        "    test[c] = le.transform(test[c].astype(str))\n",
        "\n",
        "# ========== 4. Формирование фич ==========\n",
        "target = train['target'].clip(1, 5)  # диапазон 1–5\n",
        "drop_cols = ['id', 'name', 'address', 'target', 'coordinates']\n",
        "num_cols = [c for c in train.columns if c not in drop_cols + ['text']]\n",
        "\n",
        "# стандартизация\n",
        "scaler = StandardScaler()\n",
        "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
        "test[num_cols] = scaler.transform(test[num_cols])\n",
        "\n",
        "# TF-IDF текстов\n",
        "tfidf = TfidfVectorizer(max_features=8000)\n",
        "svd = TruncatedSVD(n_components=128, random_state=42)\n",
        "text_features_train = svd.fit_transform(tfidf.fit_transform(train['text']))\n",
        "text_features_test = svd.transform(tfidf.transform(test['text']))\n",
        "\n",
        "# Объединяем фичи\n",
        "train_tab = torch.tensor(train[num_cols].values, dtype=torch.float32)\n",
        "test_tab = torch.tensor(test[num_cols].values, dtype=torch.float32)\n",
        "train_text = torch.tensor(text_features_train, dtype=torch.float32)\n",
        "test_text = torch.tensor(text_features_test, dtype=torch.float32)\n",
        "train_target = torch.tensor(target.values, dtype=torch.float32)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T10:47:07.787367Z",
          "iopub.execute_input": "2025-10-18T10:47:07.788297Z",
          "iopub.status.idle": "2025-10-18T10:47:30.734983Z",
          "shell.execute_reply.started": "2025-10-18T10:47:07.788271Z",
          "shell.execute_reply": "2025-10-18T10:47:30.733942Z"
        },
        "id": "kpKTaGQ-cEro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 5. Dataset ==========\n",
        "class POIDataset(Dataset):\n",
        "    def __init__(self, tab, text, y=None):\n",
        "        self.tab = tab\n",
        "        self.text = text\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.tab)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.tab[idx], self.text[idx]\n",
        "        return self.tab[idx], self.text[idx], self.y[idx]\n",
        "\n",
        "X_train, X_val, txt_train, txt_val, y_train, y_val = train_test_split(\n",
        "    train_tab, train_text, train_target, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_ds = POIDataset(X_train, txt_train, y_train)\n",
        "val_ds = POIDataset(X_val, txt_val, y_val)\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=256)\n",
        "\n",
        "# ========== 6. Модель ==========\n",
        "from transformers import AutoModel\n",
        "\n",
        "class POIModelHF(nn.Module):\n",
        "    def __init__(self, num_tab_features, model_name=\"bert-base-uncased\", hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
        "        text_dim = self.text_encoder.config.hidden_size\n",
        "\n",
        "        self.mlp_tab = nn.Sequential(\n",
        "            nn.Linear(num_tab_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim + text_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tab, text_inputs):\n",
        "        # text_inputs — dict с input_ids и attention_mask\n",
        "        text_out = self.text_encoder(**text_inputs).last_hidden_state[:, 0, :]  # CLS\n",
        "        tab_out = self.mlp_tab(tab)\n",
        "        x = torch.cat([tab_out, text_out], dim=1)\n",
        "        return self.fc_out(x).squeeze(1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = POIModelHF(num_tab_features=train_tab.shape[1]).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# ========== 7. Обучение ==========\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for tab, text, y in train_dl:\n",
        "        tab, text, y = tab.to(device), text.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        preds = model(tab, text)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    # валидация\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for tab, text, y in val_dl:\n",
        "            tab, text, y = tab.to(device), text.to(device), y.to(device)\n",
        "            preds = model(tab, text)\n",
        "            val_losses.append(criterion(preds, y).item())\n",
        "    print(f\"Epoch {epoch+1}: train={total_loss/len(train_dl):.4f}, val={np.mean(val_losses):.4f}\")\n",
        "\n",
        "# ========== 8. Предсказания ==========\n",
        "test_ds = POIDataset(test_tab, test_text)\n",
        "test_dl = DataLoader(test_ds, batch_size=256)\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for tab, text in tqdm(test_dl):\n",
        "        tab, text = tab.to(device), text.to(device)\n",
        "        out = model(tab, text)\n",
        "        preds.append(out.cpu().numpy())\n",
        "preds = np.concatenate(preds)\n",
        "preds = np.clip(preds, 1, 5)  # диапазон [1,5]\n",
        "\n",
        "# ========== 9. Сохранение ==========\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': preds\n",
        "})\n",
        "submission.to_csv('submission_poi_transformer.csv', index=False)\n",
        "print(\"✅ submission_poi_transformer.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T11:34:09.657237Z",
          "iopub.execute_input": "2025-10-18T11:34:09.657944Z",
          "iopub.status.idle": "2025-10-18T11:34:12.483282Z",
          "shell.execute_reply.started": "2025-10-18T11:34:09.657905Z",
          "shell.execute_reply": "2025-10-18T11:34:12.481997Z"
        },
        "id": "1-9V5a0DcEro",
        "outputId": "3dfbc20b-9aae-4b94-e2d1-2255bfe1579d",
        "colab": {
          "referenced_widgets": [
            "9157665d4b5c4d1ead158631c47a5dde",
            "21995bfdeb8d4c9bab5562defb5095a8"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9157665d4b5c4d1ead158631c47a5dde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21995bfdeb8d4c9bab5562defb5095a8"
            }
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/3189527924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_37/3189527924.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tab, text_inputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# text_inputs — dict с input_ids и attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtext_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtab_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_tab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtab_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n) argument after ** must be a mapping, not Tensor"
          ],
          "ename": "TypeError",
          "evalue": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n) argument after ** must be a mapping, not Tensor",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === 1. Загружаем данные ===\n",
        "train = pd.read_parquet('/kaggle/input/hshussu/train.tsv')\n",
        "test = pd.read_parquet('/kaggle/input/hshussu/test.tsv')  # если есть\n",
        "\n",
        "# === 2. Модель (можно поменять название в model_name) ===\n",
        "model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# === 3. Получаем эмбеддинги ===\n",
        "def get_embeddings(texts, batch_size=256):\n",
        "    all_embs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i:i+batch_size].fillna(\"\").tolist()\n",
        "        emb = model.encode(batch, show_progress_bar=False, convert_to_numpy=True, batch_size=batch_size)\n",
        "        all_embs.append(emb)\n",
        "    return np.vstack(all_embs)\n",
        "\n",
        "train_embs = get_embeddings(train['text'])\n",
        "test_embs = get_embeddings(test['text'])\n",
        "\n",
        "print(train_embs.shape, test_embs.shape)  # например (1_000_000, 384)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T14:18:01.873121Z",
          "iopub.execute_input": "2025-10-17T14:18:01.873974Z",
          "iopub.status.idle": "2025-10-17T14:20:16.506146Z",
          "shell.execute_reply.started": "2025-10-17T14:18:01.873945Z",
          "shell.execute_reply": "2025-10-17T14:20:16.504682Z"
        },
        "id": "lPsmLDOhcErp",
        "outputId": "6bb3825c-3908-4ea9-8dfa-34110c14479a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m506.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2025-10-17 14:19:57.451237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760710797.718844      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760710797.786366      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/2355007226.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# === 1. Загружаем данные ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/hshussu/train.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/hshussu/test.tsv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# если есть\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             pa_table = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         dataset = ParquetDataset(\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             self._dataset = ds.FileSystemDataset(\n\u001b[0;32m-> 1360\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m                 \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                 \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
          ],
          "ename": "ArrowInvalid",
          "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"DeepPavlov/rubert-base-cased\"  # или \"intfloat/multilingual-e5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.eval()\n",
        "\n",
        "def get_embeddings_hf(texts, batch_size=64):\n",
        "    embs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = [t if isinstance(t, str) else \"\" for t in texts[i:i+batch_size]]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors='pt').to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Берем [CLS] токен (или среднее по токенам)\n",
        "            cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        embs.append(cls_emb)\n",
        "    return np.vstack(embs)\n",
        "\n",
        "train_embs = get_embeddings_hf(train['text'])\n",
        "test_embs = get_embeddings_hf(test['text'])\n",
        "print(train_embs.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T14:23:15.441982Z",
          "iopub.execute_input": "2025-10-17T14:23:15.442316Z",
          "iopub.status.idle": "2025-10-17T16:56:18.186514Z",
          "shell.execute_reply.started": "2025-10-17T14:23:15.442291Z",
          "shell.execute_reply": "2025-10-17T16:56:18.181433Z"
        },
        "id": "9emi7NLccErp",
        "outputId": "48ae3c47-ca64-424d-dd71-501cadae901d",
        "colab": {
          "referenced_widgets": [
            "ef5bb58d11b148b29ba96b9792a6e136",
            "b6887ffd0f154d879f13f8ae1b5569d6",
            "0e1a5b874a2f443fbc63e56b438e994e",
            "f1e8290feadf4065a9c260d75b4ad234",
            "e656314fd4ff483fab4b370faaa7e3fc",
            "137b050809ea4483bf8d13d7cde948b0"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef5bb58d11b148b29ba96b9792a6e136"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6887ffd0f154d879f13f8ae1b5569d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e1a5b874a2f443fbc63e56b438e994e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1e8290feadf4065a9c260d75b4ad234"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e656314fd4ff483fab4b370faaa7e3fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "137b050809ea4483bf8d13d7cde948b0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n  0%|          | 0/643 [00:00<?, ?it/s]\u001b[A\n  0%|          | 1/643 [00:22<3:57:59, 22.24s/it]\u001b[A\n  0%|          | 2/643 [00:42<3:43:47, 20.95s/it]\u001b[A\n  0%|          | 3/643 [01:00<3:31:47, 19.86s/it]\u001b[A\n  1%|          | 4/643 [01:21<3:34:26, 20.14s/it]\u001b[A\n  1%|          | 5/643 [01:40<3:30:50, 19.83s/it]\u001b[A\n  1%|          | 6/643 [01:53<3:06:34, 17.57s/it]\u001b[A\n  1%|          | 7/643 [02:04<2:43:42, 15.44s/it]\u001b[A\n  1%|          | 8/643 [02:16<2:29:24, 14.12s/it]\u001b[A\n  1%|▏         | 9/643 [02:27<2:19:56, 13.24s/it]\u001b[A\n  2%|▏         | 10/643 [02:38<2:13:19, 12.64s/it]\u001b[A\n  2%|▏         | 11/643 [02:50<2:08:53, 12.24s/it]\u001b[A\n  2%|▏         | 12/643 [03:01<2:05:14, 11.91s/it]\u001b[A\n  2%|▏         | 13/643 [03:12<2:02:55, 11.71s/it]\u001b[A\n  2%|▏         | 14/643 [03:23<2:00:31, 11.50s/it]\u001b[A\n  2%|▏         | 15/643 [03:34<1:58:55, 11.36s/it]\u001b[A\n  2%|▏         | 16/643 [03:45<1:58:04, 11.30s/it]\u001b[A\n  3%|▎         | 17/643 [03:56<1:57:27, 11.26s/it]\u001b[A\n  3%|▎         | 18/643 [04:08<1:57:02, 11.24s/it]\u001b[A\n  3%|▎         | 19/643 [04:19<1:56:57, 11.25s/it]\u001b[A\n  3%|▎         | 20/643 [04:32<2:04:03, 11.95s/it]\u001b[A\n  3%|▎         | 21/643 [04:44<2:01:36, 11.73s/it]\u001b[A\n  3%|▎         | 22/643 [04:55<1:59:35, 11.55s/it]\u001b[A\n  4%|▎         | 23/643 [05:06<1:58:09, 11.43s/it]\u001b[A\n  4%|▎         | 24/643 [05:18<2:01:00, 11.73s/it]\u001b[A\n  4%|▍         | 25/643 [05:37<2:23:20, 13.92s/it]\u001b[A\n  4%|▍         | 26/643 [05:49<2:15:54, 13.22s/it]\u001b[A\n  4%|▍         | 27/643 [06:01<2:10:39, 12.73s/it]\u001b[A\n  4%|▍         | 28/643 [06:12<2:07:23, 12.43s/it]\u001b[A\n  5%|▍         | 29/643 [06:24<2:04:23, 12.16s/it]\u001b[A\n  5%|▍         | 30/643 [06:35<2:02:16, 11.97s/it]\u001b[A\n  5%|▍         | 31/643 [06:47<2:00:45, 11.84s/it]\u001b[A\n  5%|▍         | 32/643 [06:58<1:59:38, 11.75s/it]\u001b[A\n  5%|▌         | 33/643 [07:10<1:58:48, 11.69s/it]\u001b[A\n  5%|▌         | 34/643 [07:22<1:58:18, 11.66s/it]\u001b[A\n  5%|▌         | 35/643 [07:33<1:58:11, 11.66s/it]\u001b[A\n  6%|▌         | 36/643 [07:45<1:58:04, 11.67s/it]\u001b[A\n  6%|▌         | 37/643 [07:57<1:57:41, 11.65s/it]\u001b[A\n  6%|▌         | 38/643 [08:08<1:57:11, 11.62s/it]\u001b[A\n  6%|▌         | 39/643 [08:20<1:57:40, 11.69s/it]\u001b[A\n  6%|▌         | 40/643 [08:32<1:57:59, 11.74s/it]\u001b[A\n  6%|▋         | 41/643 [08:44<1:57:48, 11.74s/it]\u001b[A\n  7%|▋         | 42/643 [08:55<1:57:45, 11.76s/it]\u001b[A\n  7%|▋         | 43/643 [09:07<1:57:10, 11.72s/it]\u001b[A\n  7%|▋         | 44/643 [09:19<1:56:36, 11.68s/it]\u001b[A\n  7%|▋         | 45/643 [09:30<1:56:16, 11.67s/it]\u001b[A\n  7%|▋         | 46/643 [09:42<1:56:10, 11.68s/it]\u001b[A\n  7%|▋         | 47/643 [09:54<1:55:57, 11.67s/it]\u001b[A\n  7%|▋         | 48/643 [10:05<1:56:09, 11.71s/it]\u001b[A\n  8%|▊         | 49/643 [10:17<1:56:20, 11.75s/it]\u001b[A\n  8%|▊         | 50/643 [10:29<1:56:26, 11.78s/it]\u001b[A\n  8%|▊         | 51/643 [10:41<1:55:56, 11.75s/it]\u001b[A\n  8%|▊         | 52/643 [10:52<1:55:45, 11.75s/it]\u001b[A\n  8%|▊         | 53/643 [11:04<1:55:21, 11.73s/it]\u001b[A\n  8%|▊         | 54/643 [11:16<1:55:10, 11.73s/it]\u001b[A\n  9%|▊         | 55/643 [11:28<1:54:45, 11.71s/it]\u001b[A\n  9%|▊         | 56/643 [11:40<1:55:23, 11.79s/it]\u001b[A\n  9%|▉         | 57/643 [11:51<1:54:28, 11.72s/it]\u001b[A\n  9%|▉         | 58/643 [12:03<1:54:00, 11.69s/it]\u001b[A\n  9%|▉         | 59/643 [12:14<1:53:10, 11.63s/it]\u001b[A\n  9%|▉         | 60/643 [12:26<1:52:43, 11.60s/it]\u001b[A\n  9%|▉         | 61/643 [12:37<1:52:06, 11.56s/it]\u001b[A\n 10%|▉         | 62/643 [12:49<1:52:32, 11.62s/it]\u001b[A\n 10%|▉         | 63/643 [13:01<1:52:08, 11.60s/it]\u001b[A\n 10%|▉         | 64/643 [13:12<1:51:39, 11.57s/it]\u001b[A\n 10%|█         | 65/643 [13:23<1:51:06, 11.53s/it]\u001b[A\n 10%|█         | 66/643 [13:35<1:50:49, 11.52s/it]\u001b[A\n 10%|█         | 67/643 [13:46<1:50:22, 11.50s/it]\u001b[A\n 11%|█         | 68/643 [13:58<1:50:11, 11.50s/it]\u001b[A\n 11%|█         | 69/643 [14:09<1:49:59, 11.50s/it]\u001b[A\n 11%|█         | 70/643 [14:21<1:49:39, 11.48s/it]\u001b[A\n 11%|█         | 71/643 [14:32<1:49:43, 11.51s/it]\u001b[A\n 11%|█         | 72/643 [14:44<1:49:32, 11.51s/it]\u001b[A\n 11%|█▏        | 73/643 [14:56<1:49:31, 11.53s/it]\u001b[A\n 12%|█▏        | 74/643 [15:07<1:49:34, 11.56s/it]\u001b[A\n 12%|█▏        | 75/643 [15:19<1:49:12, 11.54s/it]\u001b[A\n 12%|█▏        | 76/643 [15:30<1:48:55, 11.53s/it]\u001b[A\n 12%|█▏        | 77/643 [15:54<2:23:39, 15.23s/it]\u001b[A\n 12%|█▏        | 78/643 [16:06<2:15:38, 14.40s/it]\u001b[A\n 12%|█▏        | 79/643 [16:18<2:07:32, 13.57s/it]\u001b[A\n 12%|█▏        | 80/643 [16:30<2:01:57, 13.00s/it]\u001b[A\n 13%|█▎        | 81/643 [16:42<1:59:07, 12.72s/it]\u001b[A\n 13%|█▎        | 82/643 [16:54<1:55:58, 12.40s/it]\u001b[A\n 13%|█▎        | 83/643 [17:05<1:53:48, 12.19s/it]\u001b[A\n 13%|█▎        | 84/643 [17:17<1:52:21, 12.06s/it]\u001b[A\n 13%|█▎        | 85/643 [17:29<1:51:40, 12.01s/it]\u001b[A\n 13%|█▎        | 86/643 [17:40<1:50:25, 11.89s/it]\u001b[A\n 14%|█▎        | 87/643 [17:52<1:49:34, 11.82s/it]\u001b[A\n 14%|█▎        | 88/643 [18:04<1:48:28, 11.73s/it]\u001b[A\n 14%|█▍        | 89/643 [18:15<1:47:43, 11.67s/it]\u001b[A\n 14%|█▍        | 90/643 [18:27<1:47:12, 11.63s/it]\u001b[A\n 14%|█▍        | 91/643 [18:38<1:46:58, 11.63s/it]\u001b[A\n 14%|█▍        | 92/643 [18:50<1:47:01, 11.65s/it]\u001b[A\n 14%|█▍        | 93/643 [19:02<1:47:08, 11.69s/it]\u001b[A\n 15%|█▍        | 94/643 [19:13<1:46:32, 11.64s/it]\u001b[A\n 15%|█▍        | 95/643 [19:25<1:45:43, 11.58s/it]\u001b[A\n 15%|█▍        | 96/643 [19:36<1:45:37, 11.59s/it]\u001b[A\n 15%|█▌        | 97/643 [19:48<1:45:22, 11.58s/it]\u001b[A\n 15%|█▌        | 98/643 [19:59<1:44:47, 11.54s/it]\u001b[A\n 15%|█▌        | 99/643 [20:11<1:44:36, 11.54s/it]\u001b[A\n 16%|█▌        | 100/643 [20:22<1:44:12, 11.52s/it]\u001b[A\n 16%|█▌        | 101/643 [20:34<1:45:03, 11.63s/it]\u001b[A\n 16%|█▌        | 102/643 [20:46<1:45:37, 11.71s/it]\u001b[A\n 16%|█▌        | 103/643 [20:58<1:44:49, 11.65s/it]\u001b[A\n 16%|█▌        | 104/643 [21:09<1:44:14, 11.60s/it]\u001b[A\n 16%|█▋        | 105/643 [21:21<1:43:59, 11.60s/it]\u001b[A\n 16%|█▋        | 106/643 [21:32<1:43:57, 11.62s/it]\u001b[A\n 17%|█▋        | 107/643 [21:44<1:43:14, 11.56s/it]\u001b[A\n 17%|█▋        | 108/643 [21:55<1:42:40, 11.52s/it]\u001b[A\n 17%|█▋        | 109/643 [22:07<1:42:15, 11.49s/it]\u001b[A\n 17%|█▋        | 110/643 [22:18<1:41:49, 11.46s/it]\u001b[A\n 17%|█▋        | 111/643 [22:30<1:41:41, 11.47s/it]\u001b[A\n 17%|█▋        | 112/643 [22:41<1:41:07, 11.43s/it]\u001b[A\n 18%|█▊        | 113/643 [22:52<1:41:02, 11.44s/it]\u001b[A\n 18%|█▊        | 114/643 [23:04<1:40:43, 11.42s/it]\u001b[A\n 18%|█▊        | 115/643 [23:15<1:40:37, 11.43s/it]\u001b[A\n 18%|█▊        | 116/643 [23:27<1:40:24, 11.43s/it]\u001b[A\n 18%|█▊        | 117/643 [23:38<1:40:09, 11.42s/it]\u001b[A\n 18%|█▊        | 118/643 [23:50<1:40:04, 11.44s/it]\u001b[A\n 19%|█▊        | 119/643 [24:01<1:40:06, 11.46s/it]\u001b[A\n 19%|█▊        | 120/643 [24:13<1:40:14, 11.50s/it]\u001b[A\n 19%|█▉        | 121/643 [24:24<1:40:18, 11.53s/it]\u001b[A\n 19%|█▉        | 122/643 [24:36<1:40:11, 11.54s/it]\u001b[A\n 19%|█▉        | 123/643 [24:47<1:40:24, 11.59s/it]\u001b[A\n 19%|█▉        | 124/643 [24:59<1:40:42, 11.64s/it]\u001b[A\n 19%|█▉        | 125/643 [25:11<1:40:55, 11.69s/it]\u001b[A\n 20%|█▉        | 126/643 [25:23<1:40:59, 11.72s/it]\u001b[A\n 20%|█▉        | 127/643 [25:35<1:40:51, 11.73s/it]\u001b[A\n 20%|█▉        | 128/643 [25:46<1:40:40, 11.73s/it]\u001b[A\n 20%|██        | 129/643 [26:00<1:45:09, 12.28s/it]\u001b[A\n 20%|██        | 130/643 [26:22<2:09:38, 15.16s/it]\u001b[A\n 20%|██        | 131/643 [26:34<2:01:30, 14.24s/it]\u001b[A\n 21%|██        | 132/643 [26:46<1:55:42, 13.59s/it]\u001b[A\n 21%|██        | 133/643 [26:58<1:52:03, 13.18s/it]\u001b[A\n 21%|██        | 134/643 [27:13<1:56:00, 13.67s/it]\u001b[A\n 21%|██        | 135/643 [27:26<1:53:39, 13.42s/it]\u001b[A\n 21%|██        | 136/643 [27:38<1:50:34, 13.09s/it]\u001b[A\n 21%|██▏       | 137/643 [27:50<1:47:57, 12.80s/it]\u001b[A\n 21%|██▏       | 138/643 [28:02<1:45:49, 12.57s/it]\u001b[A\n 22%|██▏       | 139/643 [28:14<1:43:40, 12.34s/it]\u001b[A\n 22%|██▏       | 140/643 [28:26<1:42:02, 12.17s/it]\u001b[A\n 22%|██▏       | 141/643 [28:38<1:41:34, 12.14s/it]\u001b[A\n 22%|██▏       | 142/643 [28:50<1:40:32, 12.04s/it]\u001b[A\n 22%|██▏       | 143/643 [29:02<1:39:45, 11.97s/it]\u001b[A\n 22%|██▏       | 144/643 [29:13<1:39:15, 11.94s/it]\u001b[A\n 23%|██▎       | 145/643 [29:25<1:38:46, 11.90s/it]\u001b[A\n 23%|██▎       | 146/643 [29:37<1:38:46, 11.92s/it]\u001b[A\n 23%|██▎       | 147/643 [29:49<1:37:55, 11.85s/it]\u001b[A\n 23%|██▎       | 148/643 [30:01<1:37:20, 11.80s/it]\u001b[A\n 23%|██▎       | 149/643 [30:12<1:36:42, 11.75s/it]\u001b[A\n 23%|██▎       | 150/643 [30:24<1:35:59, 11.68s/it]\u001b[A\n 23%|██▎       | 151/643 [30:35<1:35:32, 11.65s/it]\u001b[A\n 24%|██▎       | 152/643 [30:47<1:35:13, 11.64s/it]\u001b[A\n 24%|██▍       | 153/643 [30:59<1:34:57, 11.63s/it]\u001b[A\n 24%|██▍       | 154/643 [31:10<1:34:27, 11.59s/it]\u001b[A\n 24%|██▍       | 155/643 [31:22<1:34:06, 11.57s/it]\u001b[A\n 24%|██▍       | 156/643 [31:33<1:34:01, 11.59s/it]\u001b[A\n 24%|██▍       | 157/643 [31:45<1:33:54, 11.59s/it]\u001b[A\n 25%|██▍       | 158/643 [31:56<1:33:36, 11.58s/it]\u001b[A\n 25%|██▍       | 159/643 [32:08<1:33:19, 11.57s/it]\u001b[A\n 25%|██▍       | 160/643 [32:19<1:33:00, 11.55s/it]\u001b[A\n 25%|██▌       | 161/643 [32:31<1:33:01, 11.58s/it]\u001b[A\n 25%|██▌       | 162/643 [32:43<1:34:13, 11.75s/it]\u001b[A\n 25%|██▌       | 163/643 [32:55<1:33:42, 11.71s/it]\u001b[A\n 26%|██▌       | 164/643 [33:06<1:33:01, 11.65s/it]\u001b[A\n 26%|██▌       | 165/643 [33:18<1:32:21, 11.59s/it]\u001b[A\n 26%|██▌       | 166/643 [33:29<1:32:14, 11.60s/it]\u001b[A\n 26%|██▌       | 167/643 [33:41<1:31:56, 11.59s/it]\u001b[A\n 26%|██▌       | 168/643 [33:52<1:31:14, 11.53s/it]\u001b[A\n 26%|██▋       | 169/643 [34:11<1:49:01, 13.80s/it]\u001b[A\n 26%|██▋       | 170/643 [34:30<2:00:12, 15.25s/it]\u001b[A\n 27%|██▋       | 171/643 [34:50<2:12:03, 16.79s/it]\u001b[A\n 27%|██▋       | 172/643 [35:03<2:02:22, 15.59s/it]\u001b[A\n 27%|██▋       | 173/643 [35:15<1:52:27, 14.36s/it]\u001b[A\n 27%|██▋       | 174/643 [35:26<1:45:30, 13.50s/it]\u001b[A\n 27%|██▋       | 175/643 [35:38<1:40:35, 12.90s/it]\u001b[A\n 27%|██▋       | 176/643 [35:49<1:37:11, 12.49s/it]\u001b[A\n 28%|██▊       | 177/643 [36:01<1:34:32, 12.17s/it]\u001b[A\n 28%|██▊       | 178/643 [36:12<1:32:47, 11.97s/it]\u001b[A\n 28%|██▊       | 179/643 [36:28<1:40:46, 13.03s/it]\u001b[A\n 28%|██▊       | 180/643 [36:45<1:51:00, 14.38s/it]\u001b[A\n 28%|██▊       | 181/643 [36:57<1:44:54, 13.62s/it]\u001b[A\n 28%|██▊       | 182/643 [37:09<1:40:41, 13.11s/it]\u001b[A\n 28%|██▊       | 183/643 [37:21<1:37:43, 12.75s/it]\u001b[A\n 29%|██▊       | 184/643 [37:33<1:36:03, 12.56s/it]\u001b[A\n 29%|██▉       | 185/643 [37:45<1:34:29, 12.38s/it]\u001b[A\n 29%|██▉       | 186/643 [37:57<1:33:26, 12.27s/it]\u001b[A\n 29%|██▉       | 187/643 [38:09<1:32:23, 12.16s/it]\u001b[A\n 29%|██▉       | 188/643 [38:21<1:31:26, 12.06s/it]\u001b[A\n 29%|██▉       | 189/643 [38:32<1:30:31, 11.96s/it]\u001b[A\n 30%|██▉       | 190/643 [38:44<1:29:59, 11.92s/it]\u001b[A\n 30%|██▉       | 191/643 [38:56<1:29:41, 11.91s/it]\u001b[A\n 30%|██▉       | 192/643 [39:08<1:29:30, 11.91s/it]\u001b[A\n 30%|███       | 193/643 [39:20<1:29:23, 11.92s/it]\u001b[A\n 30%|███       | 194/643 [39:32<1:29:23, 11.95s/it]\u001b[A\n 30%|███       | 195/643 [39:44<1:29:07, 11.94s/it]\u001b[A\n 30%|███       | 196/643 [39:56<1:28:47, 11.92s/it]\u001b[A\n 31%|███       | 197/643 [40:08<1:28:18, 11.88s/it]\u001b[A\n 31%|███       | 198/643 [40:19<1:27:48, 11.84s/it]\u001b[A\n 31%|███       | 199/643 [40:31<1:27:12, 11.78s/it]\u001b[A\n 31%|███       | 200/643 [40:43<1:26:50, 11.76s/it]\u001b[A\n 31%|███▏      | 201/643 [40:54<1:26:43, 11.77s/it]\u001b[A\n 31%|███▏      | 202/643 [41:06<1:26:17, 11.74s/it]\u001b[A\n 32%|███▏      | 203/643 [41:18<1:26:00, 11.73s/it]\u001b[A\n 32%|███▏      | 204/643 [41:30<1:26:43, 11.85s/it]\u001b[A\n 32%|███▏      | 205/643 [41:42<1:26:28, 11.85s/it]\u001b[A\n 32%|███▏      | 206/643 [41:53<1:25:43, 11.77s/it]\u001b[A\n 32%|███▏      | 207/643 [42:05<1:24:58, 11.69s/it]\u001b[A\n 32%|███▏      | 208/643 [42:16<1:24:23, 11.64s/it]\u001b[A\n 33%|███▎      | 209/643 [42:28<1:23:51, 11.59s/it]\u001b[A\n 33%|███▎      | 210/643 [42:39<1:23:23, 11.56s/it]\u001b[A\n 33%|███▎      | 211/643 [42:51<1:23:03, 11.54s/it]\u001b[A\n 33%|███▎      | 212/643 [43:02<1:22:46, 11.52s/it]\u001b[A\n 33%|███▎      | 213/643 [43:14<1:22:36, 11.53s/it]\u001b[A\n 33%|███▎      | 214/643 [43:25<1:22:31, 11.54s/it]\u001b[A\n 33%|███▎      | 215/643 [43:37<1:22:21, 11.55s/it]\u001b[A\n 34%|███▎      | 216/643 [43:49<1:21:58, 11.52s/it]\u001b[A\n 34%|███▎      | 217/643 [44:00<1:21:50, 11.53s/it]\u001b[A\n 34%|███▍      | 218/643 [44:11<1:21:18, 11.48s/it]\u001b[A\n 34%|███▍      | 219/643 [44:23<1:20:37, 11.41s/it]\u001b[A\n 34%|███▍      | 220/643 [44:34<1:20:09, 11.37s/it]\u001b[A\n 34%|███▍      | 221/643 [44:47<1:22:50, 11.78s/it]\u001b[A\n 35%|███▍      | 222/643 [44:58<1:22:42, 11.79s/it]\u001b[A\n 35%|███▍      | 223/643 [45:10<1:21:47, 11.68s/it]\u001b[A\n 35%|███▍      | 224/643 [45:21<1:21:06, 11.61s/it]\u001b[A\n 35%|███▍      | 225/643 [45:33<1:20:57, 11.62s/it]\u001b[A\n 35%|███▌      | 226/643 [45:44<1:20:28, 11.58s/it]\u001b[A\n 35%|███▌      | 227/643 [45:56<1:20:08, 11.56s/it]\u001b[A\n 35%|███▌      | 228/643 [46:08<1:19:51, 11.55s/it]\u001b[A\n 36%|███▌      | 229/643 [46:19<1:19:39, 11.55s/it]\u001b[A\n 36%|███▌      | 230/643 [46:31<1:19:36, 11.57s/it]\u001b[A\n 36%|███▌      | 231/643 [46:42<1:19:13, 11.54s/it]\u001b[A\n 36%|███▌      | 232/643 [46:54<1:18:53, 11.52s/it]\u001b[A\n 36%|███▌      | 233/643 [47:05<1:18:39, 11.51s/it]\u001b[A\n 36%|███▋      | 234/643 [47:17<1:18:28, 11.51s/it]\u001b[A\n 37%|███▋      | 235/643 [47:28<1:18:24, 11.53s/it]\u001b[A\n 37%|███▋      | 236/643 [47:40<1:18:12, 11.53s/it]\u001b[A\n 37%|███▋      | 237/643 [47:51<1:18:04, 11.54s/it]\u001b[A\n 37%|███▋      | 238/643 [48:03<1:17:53, 11.54s/it]\u001b[A\n 37%|███▋      | 239/643 [48:14<1:17:31, 11.51s/it]\u001b[A\n 37%|███▋      | 240/643 [48:26<1:17:20, 11.51s/it]\u001b[A\n 37%|███▋      | 241/643 [48:37<1:17:05, 11.51s/it]\u001b[A\n 38%|███▊      | 242/643 [48:49<1:16:52, 11.50s/it]\u001b[A\n 38%|███▊      | 243/643 [49:00<1:16:38, 11.50s/it]\u001b[A\n 38%|███▊      | 244/643 [49:12<1:16:35, 11.52s/it]\u001b[A\n 38%|███▊      | 245/643 [49:23<1:16:19, 11.51s/it]\u001b[A\n 38%|███▊      | 246/643 [49:35<1:16:05, 11.50s/it]\u001b[A\n 38%|███▊      | 247/643 [49:46<1:15:43, 11.47s/it]\u001b[A\n 39%|███▊      | 248/643 [49:58<1:15:29, 11.47s/it]\u001b[A\n 39%|███▊      | 249/643 [50:09<1:15:18, 11.47s/it]\u001b[A\n 39%|███▉      | 250/643 [50:21<1:15:02, 11.46s/it]\u001b[A\n 39%|███▉      | 251/643 [50:32<1:14:46, 11.45s/it]\u001b[A\n 39%|███▉      | 252/643 [50:43<1:14:27, 11.42s/it]\u001b[A\n 39%|███▉      | 253/643 [50:55<1:14:20, 11.44s/it]\u001b[A\n 40%|███▉      | 254/643 [51:06<1:14:05, 11.43s/it]\u001b[A\n 40%|███▉      | 255/643 [51:18<1:13:50, 11.42s/it]\u001b[A\n 40%|███▉      | 256/643 [51:29<1:13:53, 11.46s/it]\u001b[A\n 40%|███▉      | 257/643 [51:41<1:13:42, 11.46s/it]\u001b[A\n 40%|████      | 258/643 [51:52<1:13:51, 11.51s/it]\u001b[A\n 40%|████      | 259/643 [52:04<1:13:33, 11.49s/it]\u001b[A\n 40%|████      | 260/643 [52:15<1:13:22, 11.49s/it]\u001b[A\n 41%|████      | 261/643 [52:27<1:13:14, 11.50s/it]\u001b[A\n 41%|████      | 262/643 [52:38<1:13:27, 11.57s/it]\u001b[A\n 41%|████      | 263/643 [52:50<1:13:20, 11.58s/it]\u001b[A\n 41%|████      | 264/643 [53:02<1:13:22, 11.61s/it]\u001b[A\n 41%|████      | 265/643 [53:13<1:13:11, 11.62s/it]\u001b[A\n 41%|████▏     | 266/643 [53:25<1:13:05, 11.63s/it]\u001b[A\n 42%|████▏     | 267/643 [53:37<1:13:20, 11.70s/it]\u001b[A\n 42%|████▏     | 268/643 [53:49<1:13:35, 11.77s/it]\u001b[A\n 42%|████▏     | 269/643 [54:01<1:13:25, 11.78s/it]\u001b[A\n 42%|████▏     | 270/643 [54:12<1:13:08, 11.76s/it]\u001b[A\n 42%|████▏     | 271/643 [54:24<1:12:36, 11.71s/it]\u001b[A\n 42%|████▏     | 272/643 [54:36<1:12:39, 11.75s/it]\u001b[A\n 42%|████▏     | 273/643 [54:48<1:12:23, 11.74s/it]\u001b[A\n 43%|████▎     | 274/643 [55:01<1:16:18, 12.41s/it]\u001b[A\n 43%|████▎     | 275/643 [55:13<1:14:40, 12.18s/it]\u001b[A\n 43%|████▎     | 276/643 [55:25<1:13:12, 11.97s/it]\u001b[A\n 43%|████▎     | 277/643 [55:36<1:12:04, 11.82s/it]\u001b[A\n 43%|████▎     | 278/643 [55:48<1:11:21, 11.73s/it]\u001b[A\n 43%|████▎     | 279/643 [55:59<1:10:52, 11.68s/it]\u001b[A\n 44%|████▎     | 280/643 [56:11<1:10:32, 11.66s/it]\u001b[A\n 44%|████▎     | 281/643 [56:23<1:11:06, 11.79s/it]\u001b[A\n 44%|████▍     | 282/643 [56:34<1:10:18, 11.69s/it]\u001b[A\n 44%|████▍     | 283/643 [56:46<1:09:45, 11.63s/it]\u001b[A\n 44%|████▍     | 284/643 [56:57<1:09:14, 11.57s/it]\u001b[A\n 44%|████▍     | 285/643 [57:09<1:08:57, 11.56s/it]\u001b[A\n 44%|████▍     | 286/643 [57:20<1:08:32, 11.52s/it]\u001b[A\n 45%|████▍     | 287/643 [57:32<1:08:09, 11.49s/it]\u001b[A\n 45%|████▍     | 288/643 [57:43<1:08:06, 11.51s/it]\u001b[A\n 45%|████▍     | 289/643 [57:55<1:08:01, 11.53s/it]\u001b[A\n 45%|████▌     | 290/643 [58:06<1:07:58, 11.56s/it]\u001b[A\n 45%|████▌     | 291/643 [58:18<1:07:58, 11.59s/it]\u001b[A\n 45%|████▌     | 292/643 [58:30<1:07:46, 11.58s/it]\u001b[A\n 46%|████▌     | 293/643 [58:41<1:07:37, 11.59s/it]\u001b[A\n 46%|████▌     | 294/643 [58:53<1:07:31, 11.61s/it]\u001b[A\n 46%|████▌     | 295/643 [59:04<1:07:15, 11.60s/it]\u001b[A\n 46%|████▌     | 296/643 [59:16<1:06:52, 11.56s/it]\u001b[A\n 46%|████▌     | 297/643 [59:28<1:06:48, 11.59s/it]\u001b[A\n 46%|████▋     | 298/643 [59:39<1:06:31, 11.57s/it]\u001b[A\n 47%|████▋     | 299/643 [59:51<1:06:08, 11.54s/it]\u001b[A\n 47%|████▋     | 300/643 [1:00:02<1:05:57, 11.54s/it]\u001b[A\n 47%|████▋     | 301/643 [1:00:14<1:05:35, 11.51s/it]\u001b[A\n 47%|████▋     | 302/643 [1:00:25<1:05:31, 11.53s/it]\u001b[A\n 47%|████▋     | 303/643 [1:00:37<1:05:20, 11.53s/it]\u001b[A\n 47%|████▋     | 304/643 [1:00:48<1:05:09, 11.53s/it]\u001b[A\n 47%|████▋     | 305/643 [1:01:00<1:04:58, 11.54s/it]\u001b[A\n 48%|████▊     | 306/643 [1:01:11<1:04:39, 11.51s/it]\u001b[A\n 48%|████▊     | 307/643 [1:01:23<1:04:31, 11.52s/it]\u001b[A\n 48%|████▊     | 308/643 [1:01:34<1:04:20, 11.52s/it]\u001b[A\n 48%|████▊     | 309/643 [1:01:46<1:04:08, 11.52s/it]\u001b[A\n 48%|████▊     | 310/643 [1:01:57<1:03:50, 11.50s/it]\u001b[A\n 48%|████▊     | 311/643 [1:02:09<1:03:32, 11.48s/it]\u001b[A\n 49%|████▊     | 312/643 [1:02:20<1:03:28, 11.51s/it]\u001b[A\n 49%|████▊     | 313/643 [1:02:32<1:03:19, 11.51s/it]\u001b[A\n 49%|████▉     | 314/643 [1:02:43<1:03:07, 11.51s/it]\u001b[A\n 49%|████▉     | 315/643 [1:02:55<1:02:50, 11.50s/it]\u001b[A\n 49%|████▉     | 316/643 [1:03:06<1:02:36, 11.49s/it]\u001b[A\n 49%|████▉     | 317/643 [1:03:18<1:02:25, 11.49s/it]\u001b[A\n 49%|████▉     | 318/643 [1:03:29<1:02:15, 11.49s/it]\u001b[A\n 50%|████▉     | 319/643 [1:03:41<1:02:01, 11.49s/it]\u001b[A\n 50%|████▉     | 320/643 [1:03:52<1:01:54, 11.50s/it]\u001b[A\n 50%|████▉     | 321/643 [1:04:04<1:01:44, 11.50s/it]\u001b[A\n 50%|█████     | 322/643 [1:04:15<1:01:34, 11.51s/it]\u001b[A\n 50%|█████     | 323/643 [1:04:27<1:01:25, 11.52s/it]\u001b[A\n 50%|█████     | 324/643 [1:04:38<1:01:19, 11.54s/it]\u001b[A\n 51%|█████     | 325/643 [1:04:50<1:00:57, 11.50s/it]\u001b[A\n 51%|█████     | 326/643 [1:05:01<1:01:05, 11.56s/it]\u001b[A\n 51%|█████     | 327/643 [1:05:13<1:00:51, 11.56s/it]\u001b[A\n 51%|█████     | 328/643 [1:05:25<1:00:53, 11.60s/it]\u001b[A\n 51%|█████     | 329/643 [1:05:36<1:00:41, 11.60s/it]\u001b[A\n 51%|█████▏    | 330/643 [1:05:48<1:00:12, 11.54s/it]\u001b[A\n 51%|█████▏    | 331/643 [1:05:59<59:52, 11.51s/it]  \u001b[A\n 52%|█████▏    | 332/643 [1:06:11<59:35, 11.50s/it]\u001b[A\n 52%|█████▏    | 333/643 [1:06:22<59:22, 11.49s/it]\u001b[A\n 52%|█████▏    | 334/643 [1:06:34<59:08, 11.48s/it]\u001b[A\n 52%|█████▏    | 335/643 [1:06:45<58:44, 11.44s/it]\u001b[A\n 52%|█████▏    | 336/643 [1:06:56<58:29, 11.43s/it]\u001b[A\n 52%|█████▏    | 337/643 [1:07:08<58:16, 11.43s/it]\u001b[A\n 53%|█████▎    | 338/643 [1:07:19<58:06, 11.43s/it]\u001b[A\n 53%|█████▎    | 339/643 [1:07:31<58:02, 11.46s/it]\u001b[A\n 53%|█████▎    | 340/643 [1:07:42<58:16, 11.54s/it]\u001b[A\n 53%|█████▎    | 341/643 [1:07:54<58:39, 11.65s/it]\u001b[A\n 53%|█████▎    | 342/643 [1:08:06<58:10, 11.60s/it]\u001b[A\n 53%|█████▎    | 343/643 [1:08:17<57:49, 11.57s/it]\u001b[A\n 53%|█████▎    | 344/643 [1:08:29<57:27, 11.53s/it]\u001b[A\n 54%|█████▎    | 345/643 [1:08:40<56:56, 11.46s/it]\u001b[A\n 54%|█████▍    | 346/643 [1:08:51<56:37, 11.44s/it]\u001b[A\n 54%|█████▍    | 347/643 [1:09:03<56:24, 11.43s/it]\u001b[A\n 54%|█████▍    | 348/643 [1:09:14<56:13, 11.44s/it]\u001b[A\n 54%|█████▍    | 349/643 [1:09:26<55:57, 11.42s/it]\u001b[A\n 54%|█████▍    | 350/643 [1:09:37<55:47, 11.43s/it]\u001b[A\n 55%|█████▍    | 351/643 [1:09:49<55:33, 11.42s/it]\u001b[A\n 55%|█████▍    | 352/643 [1:10:00<55:16, 11.40s/it]\u001b[A\n 55%|█████▍    | 353/643 [1:10:11<54:55, 11.36s/it]\u001b[A\n 55%|█████▌    | 354/643 [1:10:23<54:48, 11.38s/it]\u001b[A\n 55%|█████▌    | 355/643 [1:10:34<54:36, 11.38s/it]\u001b[A\n 55%|█████▌    | 356/643 [1:10:45<54:28, 11.39s/it]\u001b[A\n 56%|█████▌    | 357/643 [1:10:57<54:16, 11.39s/it]\u001b[A\n 56%|█████▌    | 358/643 [1:11:09<54:40, 11.51s/it]\u001b[A\n 56%|█████▌    | 359/643 [1:11:20<54:15, 11.46s/it]\u001b[A\n 56%|█████▌    | 360/643 [1:11:31<54:00, 11.45s/it]\u001b[A\n 56%|█████▌    | 361/643 [1:11:43<53:43, 11.43s/it]\u001b[A\n 56%|█████▋    | 362/643 [1:11:54<53:28, 11.42s/it]\u001b[A\n 56%|█████▋    | 363/643 [1:12:05<53:12, 11.40s/it]\u001b[A\n 57%|█████▋    | 364/643 [1:12:17<52:59, 11.40s/it]\u001b[A\n 57%|█████▋    | 365/643 [1:12:28<52:43, 11.38s/it]\u001b[A\n 57%|█████▋    | 366/643 [1:12:40<52:30, 11.38s/it]\u001b[A\n 57%|█████▋    | 367/643 [1:12:51<52:15, 11.36s/it]\u001b[A\n 57%|█████▋    | 368/643 [1:13:06<56:48, 12.40s/it]\u001b[A\n 57%|█████▋    | 369/643 [1:13:18<57:08, 12.51s/it]\u001b[A\n 58%|█████▊    | 370/643 [1:13:30<55:35, 12.22s/it]\u001b[A\n 58%|█████▊    | 371/643 [1:13:42<54:32, 12.03s/it]\u001b[A\n 58%|█████▊    | 372/643 [1:13:53<54:04, 11.97s/it]\u001b[A\n 58%|█████▊    | 373/643 [1:14:05<53:22, 11.86s/it]\u001b[A\n 58%|█████▊    | 374/643 [1:14:17<52:47, 11.77s/it]\u001b[A\n 58%|█████▊    | 375/643 [1:14:28<52:19, 11.72s/it]\u001b[A\n 58%|█████▊    | 376/643 [1:14:40<52:07, 11.71s/it]\u001b[A\n 59%|█████▊    | 377/643 [1:14:51<51:30, 11.62s/it]\u001b[A\n 59%|█████▉    | 378/643 [1:15:03<51:01, 11.55s/it]\u001b[A\n 59%|█████▉    | 379/643 [1:15:14<50:36, 11.50s/it]\u001b[A\n 59%|█████▉    | 380/643 [1:15:26<50:24, 11.50s/it]\u001b[A\n 59%|█████▉    | 381/643 [1:15:37<50:15, 11.51s/it]\u001b[A\n 59%|█████▉    | 382/643 [1:15:49<50:08, 11.53s/it]\u001b[A\n 60%|█████▉    | 383/643 [1:16:00<49:59, 11.54s/it]\u001b[A\n 60%|█████▉    | 384/643 [1:16:12<49:47, 11.53s/it]\u001b[A\n 60%|█████▉    | 385/643 [1:16:23<49:22, 11.48s/it]\u001b[A\n 60%|██████    | 386/643 [1:16:34<48:55, 11.42s/it]\u001b[A\n 60%|██████    | 387/643 [1:16:46<48:33, 11.38s/it]\u001b[A\n 60%|██████    | 388/643 [1:16:57<48:18, 11.37s/it]\u001b[A\n 60%|██████    | 389/643 [1:17:08<48:02, 11.35s/it]\u001b[A\n 61%|██████    | 390/643 [1:17:20<47:53, 11.36s/it]\u001b[A\n 61%|██████    | 391/643 [1:17:31<47:49, 11.39s/it]\u001b[A\n 61%|██████    | 392/643 [1:17:43<47:44, 11.41s/it]\u001b[A\n 61%|██████    | 393/643 [1:17:54<47:24, 11.38s/it]\u001b[A\n 61%|██████▏   | 394/643 [1:18:05<47:07, 11.35s/it]\u001b[A\n 61%|██████▏   | 395/643 [1:18:17<46:52, 11.34s/it]\u001b[A\n 62%|██████▏   | 396/643 [1:18:28<46:47, 11.36s/it]\u001b[A\n 62%|██████▏   | 397/643 [1:18:39<46:30, 11.34s/it]\u001b[A\n 62%|██████▏   | 398/643 [1:18:51<46:29, 11.39s/it]\u001b[A\n 62%|██████▏   | 399/643 [1:19:02<46:12, 11.36s/it]\u001b[A\n 62%|██████▏   | 400/643 [1:19:13<46:04, 11.38s/it]\u001b[A\n 62%|██████▏   | 401/643 [1:19:25<45:56, 11.39s/it]\u001b[A\n 63%|██████▎   | 402/643 [1:19:36<46:02, 11.46s/it]\u001b[A\n 63%|██████▎   | 403/643 [1:19:48<45:40, 11.42s/it]\u001b[A\n 63%|██████▎   | 404/643 [1:19:59<45:17, 11.37s/it]\u001b[A\n 63%|██████▎   | 405/643 [1:20:10<44:58, 11.34s/it]\u001b[A\n 63%|██████▎   | 406/643 [1:20:22<44:54, 11.37s/it]\u001b[A\n 63%|██████▎   | 407/643 [1:20:33<44:41, 11.36s/it]\u001b[A\n 63%|██████▎   | 408/643 [1:20:44<44:25, 11.34s/it]\u001b[A\n 64%|██████▎   | 409/643 [1:20:56<44:14, 11.34s/it]\u001b[A\n 64%|██████▍   | 410/643 [1:21:07<44:03, 11.35s/it]\u001b[A\n 64%|██████▍   | 411/643 [1:21:19<43:59, 11.38s/it]\u001b[A\n 64%|██████▍   | 412/643 [1:21:30<43:57, 11.42s/it]\u001b[A\n 64%|██████▍   | 413/643 [1:21:42<43:47, 11.42s/it]\u001b[A\n 64%|██████▍   | 414/643 [1:21:53<43:37, 11.43s/it]\u001b[A\n 65%|██████▍   | 415/643 [1:22:05<43:35, 11.47s/it]\u001b[A\n 65%|██████▍   | 416/643 [1:22:16<43:29, 11.49s/it]\u001b[A\n 65%|██████▍   | 417/643 [1:22:28<43:25, 11.53s/it]\u001b[A\n 65%|██████▌   | 418/643 [1:22:42<46:43, 12.46s/it]\u001b[A\n 65%|██████▌   | 419/643 [1:22:55<46:46, 12.53s/it]\u001b[A\n 65%|██████▌   | 420/643 [1:23:07<46:03, 12.39s/it]\u001b[A\n 65%|██████▌   | 421/643 [1:23:19<45:21, 12.26s/it]\u001b[A\n 66%|██████▌   | 422/643 [1:23:31<44:54, 12.19s/it]\u001b[A\n 66%|██████▌   | 423/643 [1:23:43<44:32, 12.15s/it]\u001b[A\n 66%|██████▌   | 424/643 [1:23:55<44:26, 12.17s/it]\u001b[A\n 66%|██████▌   | 425/643 [1:24:07<44:05, 12.14s/it]\u001b[A\n 66%|██████▋   | 426/643 [1:24:20<44:01, 12.17s/it]\u001b[A\n 66%|██████▋   | 427/643 [1:24:32<43:31, 12.09s/it]\u001b[A\n 67%|██████▋   | 428/643 [1:24:43<43:02, 12.01s/it]\u001b[A\n 67%|██████▋   | 429/643 [1:24:55<42:43, 11.98s/it]\u001b[A\n 67%|██████▋   | 430/643 [1:25:07<42:27, 11.96s/it]\u001b[A\n 67%|██████▋   | 431/643 [1:25:22<44:51, 12.69s/it]\u001b[A\n 67%|██████▋   | 432/643 [1:25:34<44:24, 12.63s/it]\u001b[A\n 67%|██████▋   | 433/643 [1:25:46<43:28, 12.42s/it]\u001b[A\n 67%|██████▋   | 434/643 [1:25:58<42:39, 12.25s/it]\u001b[A\n 68%|██████▊   | 435/643 [1:26:10<41:54, 12.09s/it]\u001b[A\n 68%|██████▊   | 436/643 [1:26:21<41:22, 11.99s/it]\u001b[A\n 68%|██████▊   | 437/643 [1:26:33<41:08, 11.98s/it]\u001b[A\n 68%|██████▊   | 438/643 [1:26:45<40:48, 11.94s/it]\u001b[A\n 68%|██████▊   | 439/643 [1:26:57<40:19, 11.86s/it]\u001b[A\n 68%|██████▊   | 440/643 [1:27:08<39:55, 11.80s/it]\u001b[A\n 69%|██████▊   | 441/643 [1:27:20<39:32, 11.74s/it]\u001b[A\n 69%|██████▊   | 442/643 [1:27:32<39:26, 11.77s/it]\u001b[A\n 69%|██████▉   | 443/643 [1:27:44<39:14, 11.77s/it]\u001b[A\n 69%|██████▉   | 444/643 [1:27:56<39:04, 11.78s/it]\u001b[A\n 69%|██████▉   | 445/643 [1:28:07<38:45, 11.75s/it]\u001b[A\n 69%|██████▉   | 446/643 [1:28:19<38:29, 11.72s/it]\u001b[A\n 70%|██████▉   | 447/643 [1:28:30<38:07, 11.67s/it]\u001b[A\n 70%|██████▉   | 448/643 [1:28:42<37:55, 11.67s/it]\u001b[A\n 70%|██████▉   | 449/643 [1:28:54<37:47, 11.69s/it]\u001b[A\n 70%|██████▉   | 450/643 [1:29:05<37:34, 11.68s/it]\u001b[A\n 70%|███████   | 451/643 [1:29:17<37:17, 11.65s/it]\u001b[A\n 70%|███████   | 452/643 [1:29:29<37:00, 11.62s/it]\u001b[A\n 70%|███████   | 453/643 [1:29:40<36:49, 11.63s/it]\u001b[A\n 71%|███████   | 454/643 [1:29:52<36:40, 11.64s/it]\u001b[A\n 71%|███████   | 455/643 [1:30:03<36:21, 11.61s/it]\u001b[A\n 71%|███████   | 456/643 [1:30:15<36:09, 11.60s/it]\u001b[A\n 71%|███████   | 457/643 [1:30:26<35:50, 11.56s/it]\u001b[A\n 71%|███████   | 458/643 [1:30:38<35:34, 11.54s/it]\u001b[A\n 71%|███████▏  | 459/643 [1:30:50<35:28, 11.57s/it]\u001b[A\n 72%|███████▏  | 460/643 [1:31:01<35:24, 11.61s/it]\u001b[A\n 72%|███████▏  | 461/643 [1:31:13<35:31, 11.71s/it]\u001b[A\n 72%|███████▏  | 462/643 [1:31:25<35:42, 11.84s/it]\u001b[A\n 72%|███████▏  | 463/643 [1:31:38<35:48, 11.94s/it]\u001b[A\n 72%|███████▏  | 464/643 [1:31:50<35:40, 11.96s/it]\u001b[A\n 72%|███████▏  | 465/643 [1:32:02<35:29, 11.96s/it]\u001b[A\n 72%|███████▏  | 466/643 [1:32:13<35:00, 11.87s/it]\u001b[A\n 73%|███████▎  | 467/643 [1:32:25<34:38, 11.81s/it]\u001b[A\n 73%|███████▎  | 468/643 [1:32:37<34:18, 11.76s/it]\u001b[A\n 73%|███████▎  | 469/643 [1:32:48<33:58, 11.71s/it]\u001b[A\n 73%|███████▎  | 470/643 [1:33:00<33:36, 11.66s/it]\u001b[A\n 73%|███████▎  | 471/643 [1:33:11<33:21, 11.64s/it]\u001b[A\n 73%|███████▎  | 472/643 [1:33:23<33:18, 11.69s/it]\u001b[A\n 74%|███████▎  | 473/643 [1:33:35<33:23, 11.79s/it]\u001b[A\n 74%|███████▎  | 474/643 [1:33:47<33:08, 11.77s/it]\u001b[A\n 74%|███████▍  | 475/643 [1:33:59<32:56, 11.76s/it]\u001b[A\n 74%|███████▍  | 476/643 [1:34:10<32:40, 11.74s/it]\u001b[A\n 74%|███████▍  | 477/643 [1:34:22<32:27, 11.73s/it]\u001b[A\n 74%|███████▍  | 478/643 [1:34:34<32:18, 11.75s/it]\u001b[A\n 74%|███████▍  | 479/643 [1:34:45<32:03, 11.73s/it]\u001b[A\n 75%|███████▍  | 480/643 [1:34:57<31:57, 11.76s/it]\u001b[A\n 75%|███████▍  | 481/643 [1:35:09<31:45, 11.76s/it]\u001b[A\n 75%|███████▍  | 482/643 [1:35:24<33:52, 12.63s/it]\u001b[A\n 75%|███████▌  | 483/643 [1:35:36<33:11, 12.45s/it]\u001b[A\n 75%|███████▌  | 484/643 [1:35:47<32:15, 12.17s/it]\u001b[A\n 75%|███████▌  | 485/643 [1:35:59<31:35, 11.99s/it]\u001b[A\n 76%|███████▌  | 486/643 [1:36:10<31:04, 11.88s/it]\u001b[A\n 76%|███████▌  | 487/643 [1:36:22<30:48, 11.85s/it]\u001b[A\n 76%|███████▌  | 488/643 [1:36:34<30:34, 11.83s/it]\u001b[A\n 76%|███████▌  | 489/643 [1:36:46<30:15, 11.79s/it]\u001b[A\n 76%|███████▌  | 490/643 [1:36:57<29:51, 11.71s/it]\u001b[A\n 76%|███████▋  | 491/643 [1:37:09<29:34, 11.67s/it]\u001b[A\n 77%|███████▋  | 492/643 [1:37:21<29:27, 11.71s/it]\u001b[A\n 77%|███████▋  | 493/643 [1:37:33<29:39, 11.86s/it]\u001b[A\n 77%|███████▋  | 494/643 [1:37:45<29:30, 11.88s/it]\u001b[A\n 77%|███████▋  | 495/643 [1:37:57<29:22, 11.91s/it]\u001b[A\n 77%|███████▋  | 496/643 [1:38:09<29:08, 11.90s/it]\u001b[A\n 77%|███████▋  | 497/643 [1:38:20<28:49, 11.84s/it]\u001b[A\n 77%|███████▋  | 498/643 [1:38:33<29:05, 12.04s/it]\u001b[A\n 78%|███████▊  | 499/643 [1:38:46<29:37, 12.34s/it]\u001b[A\n 78%|███████▊  | 500/643 [1:38:58<29:05, 12.21s/it]\u001b[A\n 78%|███████▊  | 501/643 [1:39:10<28:47, 12.16s/it]\u001b[A\n 78%|███████▊  | 502/643 [1:39:22<28:24, 12.09s/it]\u001b[A\n 78%|███████▊  | 503/643 [1:39:34<28:27, 12.20s/it]\u001b[A\n 78%|███████▊  | 504/643 [1:39:46<28:20, 12.23s/it]\u001b[A\n 79%|███████▊  | 505/643 [1:39:59<28:20, 12.32s/it]\u001b[A\n 79%|███████▊  | 506/643 [1:40:11<27:46, 12.17s/it]\u001b[A\n 79%|███████▉  | 507/643 [1:40:23<27:20, 12.06s/it]\u001b[A\n 79%|███████▉  | 508/643 [1:40:34<26:56, 11.97s/it]\u001b[A\n 79%|███████▉  | 509/643 [1:40:46<26:36, 11.91s/it]\u001b[A\n 79%|███████▉  | 510/643 [1:40:58<26:16, 11.85s/it]\u001b[A\n 79%|███████▉  | 511/643 [1:41:09<25:56, 11.79s/it]\u001b[A\n 80%|███████▉  | 512/643 [1:41:21<25:38, 11.75s/it]\u001b[A\n 80%|███████▉  | 513/643 [1:41:33<25:24, 11.72s/it]\u001b[A\n 80%|███████▉  | 514/643 [1:41:45<25:11, 11.71s/it]\u001b[A\n 80%|████████  | 515/643 [1:41:56<24:51, 11.65s/it]\u001b[A\n 80%|████████  | 516/643 [1:42:07<24:25, 11.54s/it]\u001b[A\n 80%|████████  | 517/643 [1:42:19<24:04, 11.46s/it]\u001b[A\n 81%|████████  | 518/643 [1:42:30<23:45, 11.41s/it]\u001b[A\n 81%|████████  | 519/643 [1:42:41<23:28, 11.36s/it]\u001b[A\n 81%|████████  | 520/643 [1:42:52<23:14, 11.34s/it]\u001b[A\n 81%|████████  | 521/643 [1:43:04<23:15, 11.44s/it]\u001b[A\n 81%|████████  | 522/643 [1:43:16<23:06, 11.46s/it]\u001b[A\n 81%|████████▏ | 523/643 [1:43:27<22:55, 11.46s/it]\u001b[A\n 81%|████████▏ | 524/643 [1:43:39<22:50, 11.52s/it]\u001b[A\n 82%|████████▏ | 525/643 [1:43:50<22:41, 11.53s/it]\u001b[A\n 82%|████████▏ | 526/643 [1:44:02<22:36, 11.59s/it]\u001b[A\n 82%|████████▏ | 527/643 [1:44:13<22:21, 11.56s/it]\u001b[A\n 82%|████████▏ | 528/643 [1:44:25<22:02, 11.50s/it]\u001b[A\n 82%|████████▏ | 529/643 [1:44:36<21:53, 11.53s/it]\u001b[A\n 82%|████████▏ | 530/643 [1:44:48<21:35, 11.46s/it]\u001b[A\n 83%|████████▎ | 531/643 [1:44:59<21:27, 11.49s/it]\u001b[A\n 83%|████████▎ | 532/643 [1:45:11<21:17, 11.51s/it]\u001b[A\n 83%|████████▎ | 533/643 [1:45:23<21:18, 11.62s/it]\u001b[A\n 83%|████████▎ | 534/643 [1:45:35<21:22, 11.77s/it]\u001b[A\n 83%|████████▎ | 535/643 [1:45:47<21:11, 11.77s/it]\u001b[A\n 83%|████████▎ | 536/643 [1:45:58<20:55, 11.74s/it]\u001b[A\n 84%|████████▎ | 537/643 [1:46:10<20:38, 11.68s/it]\u001b[A\n 84%|████████▎ | 538/643 [1:46:21<20:18, 11.60s/it]\u001b[A\n 84%|████████▍ | 539/643 [1:46:33<20:13, 11.66s/it]\u001b[A\n 84%|████████▍ | 540/643 [1:46:45<20:05, 11.70s/it]\u001b[A\n 84%|████████▍ | 541/643 [1:46:57<19:54, 11.71s/it]\u001b[A\n 84%|████████▍ | 542/643 [1:47:08<19:41, 11.69s/it]\u001b[A\n 84%|████████▍ | 543/643 [1:47:20<19:27, 11.68s/it]\u001b[A\n 85%|████████▍ | 544/643 [1:47:33<20:10, 12.23s/it]\u001b[A\n 85%|████████▍ | 545/643 [1:47:49<21:24, 13.10s/it]\u001b[A\n 85%|████████▍ | 546/643 [1:48:00<20:20, 12.59s/it]\u001b[A\n 85%|████████▌ | 547/643 [1:48:11<19:33, 12.23s/it]\u001b[A\n 85%|████████▌ | 548/643 [1:48:23<19:04, 12.05s/it]\u001b[A\n 85%|████████▌ | 549/643 [1:48:35<18:42, 11.94s/it]\u001b[A\n 86%|████████▌ | 550/643 [1:48:46<18:17, 11.80s/it]\u001b[A\n 86%|████████▌ | 551/643 [1:48:58<18:06, 11.81s/it]\u001b[A\n 86%|████████▌ | 552/643 [1:49:10<17:48, 11.75s/it]\u001b[A\n 86%|████████▌ | 553/643 [1:49:21<17:28, 11.65s/it]\u001b[A\n 86%|████████▌ | 554/643 [1:49:32<17:12, 11.61s/it]\u001b[A\n 86%|████████▋ | 555/643 [1:49:44<17:03, 11.63s/it]\u001b[A\n 86%|████████▋ | 556/643 [1:49:56<16:52, 11.63s/it]\u001b[A\n 87%|████████▋ | 557/643 [1:50:08<16:42, 11.66s/it]\u001b[A\n 87%|████████▋ | 558/643 [1:50:19<16:32, 11.68s/it]\u001b[A\n 87%|████████▋ | 559/643 [1:50:31<16:16, 11.62s/it]\u001b[A\n 87%|████████▋ | 560/643 [1:50:42<16:02, 11.59s/it]\u001b[A\n 87%|████████▋ | 561/643 [1:50:54<15:55, 11.65s/it]\u001b[A\n 87%|████████▋ | 562/643 [1:51:06<15:48, 11.71s/it]\u001b[A\n 88%|████████▊ | 563/643 [1:51:18<15:40, 11.75s/it]\u001b[A\n 88%|████████▊ | 564/643 [1:51:29<15:28, 11.75s/it]\u001b[A\n 88%|████████▊ | 565/643 [1:51:41<15:10, 11.67s/it]\u001b[A\n 88%|████████▊ | 566/643 [1:51:53<14:57, 11.66s/it]\u001b[A\n 88%|████████▊ | 567/643 [1:52:05<14:57, 11.81s/it]\u001b[A\n 88%|████████▊ | 568/643 [1:52:16<14:35, 11.67s/it]\u001b[A\n 88%|████████▊ | 569/643 [1:52:27<14:14, 11.54s/it]\u001b[A\n 89%|████████▊ | 570/643 [1:52:39<14:00, 11.51s/it]\u001b[A\n 89%|████████▉ | 571/643 [1:52:51<13:53, 11.57s/it]\u001b[A\n 89%|████████▉ | 572/643 [1:53:02<13:41, 11.57s/it]\u001b[A\n 89%|████████▉ | 573/643 [1:53:14<13:28, 11.55s/it]\u001b[A\n 89%|████████▉ | 574/643 [1:53:25<13:14, 11.51s/it]\u001b[A\n 89%|████████▉ | 575/643 [1:53:36<13:01, 11.49s/it]\u001b[A\n 90%|████████▉ | 576/643 [1:53:48<12:45, 11.42s/it]\u001b[A\n 90%|████████▉ | 577/643 [1:53:59<12:32, 11.40s/it]\u001b[A\n 90%|████████▉ | 578/643 [1:54:11<12:23, 11.43s/it]\u001b[A\n 90%|█████████ | 579/643 [1:54:22<12:11, 11.44s/it]\u001b[A\n 90%|█████████ | 580/643 [1:54:33<11:55, 11.36s/it]\u001b[A\n 90%|█████████ | 581/643 [1:54:44<11:40, 11.29s/it]\u001b[A\n 91%|█████████ | 582/643 [1:54:56<11:27, 11.27s/it]\u001b[A\n 91%|█████████ | 583/643 [1:55:07<11:20, 11.34s/it]\u001b[A\n 91%|█████████ | 584/643 [1:55:19<11:12, 11.41s/it]\u001b[A\n 91%|█████████ | 585/643 [1:55:30<11:06, 11.48s/it]\u001b[A\n 91%|█████████ | 586/643 [1:55:42<10:57, 11.54s/it]\u001b[A\n 91%|█████████▏| 587/643 [1:55:53<10:44, 11.51s/it]\u001b[A\n 91%|█████████▏| 588/643 [1:56:05<10:31, 11.49s/it]\u001b[A\n 92%|█████████▏| 589/643 [1:56:16<10:19, 11.48s/it]\u001b[A\n 92%|█████████▏| 590/643 [1:56:28<10:05, 11.43s/it]\u001b[A\n 92%|█████████▏| 591/643 [1:56:39<09:53, 11.42s/it]\u001b[A\n 92%|█████████▏| 592/643 [1:56:51<09:47, 11.51s/it]\u001b[A\n 92%|█████████▏| 593/643 [1:57:02<09:38, 11.58s/it]\u001b[A\n 92%|█████████▏| 594/643 [1:57:14<09:30, 11.64s/it]\u001b[A\n 93%|█████████▎| 595/643 [1:57:26<09:17, 11.61s/it]\u001b[A\n 93%|█████████▎| 596/643 [1:57:37<09:05, 11.62s/it]\u001b[A\n 93%|█████████▎| 597/643 [1:57:49<08:55, 11.65s/it]\u001b[A\n 93%|█████████▎| 598/643 [1:58:01<08:48, 11.74s/it]\u001b[A\n 93%|█████████▎| 599/643 [1:58:13<08:39, 11.81s/it]\u001b[A\n 93%|█████████▎| 600/643 [1:58:25<08:27, 11.81s/it]\u001b[A\n 93%|█████████▎| 601/643 [1:58:37<08:17, 11.85s/it]\u001b[A\n 94%|█████████▎| 602/643 [1:58:48<08:03, 11.80s/it]\u001b[A\n 94%|█████████▍| 603/643 [1:59:00<07:49, 11.74s/it]\u001b[A\n 94%|█████████▍| 604/643 [1:59:12<07:39, 11.78s/it]\u001b[A\n 94%|█████████▍| 605/643 [1:59:24<07:26, 11.75s/it]\u001b[A\n 94%|█████████▍| 606/643 [1:59:35<07:13, 11.73s/it]\u001b[A\n 94%|█████████▍| 607/643 [1:59:54<08:17, 13.83s/it]\u001b[A\n 95%|█████████▍| 608/643 [2:00:08<08:01, 13.74s/it]\u001b[A\n 95%|█████████▍| 609/643 [2:00:19<07:25, 13.11s/it]\u001b[A\n 95%|█████████▍| 610/643 [2:00:31<07:03, 12.82s/it]\u001b[A\n 95%|█████████▌| 611/643 [2:00:44<06:47, 12.73s/it]\u001b[A\n 95%|█████████▌| 612/643 [2:00:56<06:32, 12.68s/it]\u001b[A\n 95%|█████████▌| 613/643 [2:01:09<06:17, 12.58s/it]\u001b[A\n 95%|█████████▌| 614/643 [2:01:21<06:04, 12.58s/it]\u001b[A\n 96%|█████████▌| 615/643 [2:01:34<05:52, 12.57s/it]\u001b[A\n 96%|█████████▌| 616/643 [2:01:46<05:37, 12.52s/it]\u001b[A\n 96%|█████████▌| 617/643 [2:01:58<05:19, 12.29s/it]\u001b[A\n 96%|█████████▌| 618/643 [2:02:10<05:04, 12.17s/it]\u001b[A\n 96%|█████████▋| 619/643 [2:02:22<04:50, 12.08s/it]\u001b[A\n 96%|█████████▋| 620/643 [2:02:33<04:34, 11.93s/it]\u001b[A\n 97%|█████████▋| 621/643 [2:02:45<04:19, 11.79s/it]\u001b[A\n 97%|█████████▋| 622/643 [2:02:57<04:07, 11.79s/it]\u001b[A\n 97%|█████████▋| 623/643 [2:03:08<03:55, 11.77s/it]\u001b[A\n 97%|█████████▋| 624/643 [2:03:20<03:42, 11.72s/it]\u001b[A\n 97%|█████████▋| 625/643 [2:03:32<03:30, 11.68s/it]\u001b[A\n 97%|█████████▋| 626/643 [2:03:43<03:19, 11.72s/it]\u001b[A\n 98%|█████████▊| 627/643 [2:03:55<03:08, 11.76s/it]\u001b[A\n 98%|█████████▊| 628/643 [2:04:07<02:57, 11.82s/it]\u001b[A\n 98%|█████████▊| 629/643 [2:04:19<02:45, 11.84s/it]\u001b[A\n 98%|█████████▊| 630/643 [2:04:31<02:35, 11.93s/it]\u001b[A\n 98%|█████████▊| 631/643 [2:04:43<02:22, 11.89s/it]\u001b[A\n 98%|█████████▊| 632/643 [2:04:55<02:10, 11.89s/it]\u001b[A\n 98%|█████████▊| 633/643 [2:05:07<01:58, 11.86s/it]\u001b[A\n 99%|█████████▊| 634/643 [2:05:18<01:46, 11.79s/it]\u001b[A\n 99%|█████████▉| 635/643 [2:05:30<01:34, 11.78s/it]\u001b[A\n 99%|█████████▉| 636/643 [2:05:42<01:23, 11.89s/it]\u001b[A\n 99%|█████████▉| 637/643 [2:05:54<01:11, 11.96s/it]\u001b[A\n 99%|█████████▉| 638/643 [2:06:06<00:59, 11.98s/it]\u001b[A\n 99%|█████████▉| 639/643 [2:06:18<00:47, 11.94s/it]\u001b[A\n100%|█████████▉| 640/643 [2:06:30<00:35, 11.86s/it]\u001b[A\n100%|█████████▉| 641/643 [2:06:42<00:23, 11.88s/it]\u001b[A\n100%|█████████▉| 642/643 [2:06:54<00:11, 11.89s/it]\u001b[A\n100%|██████████| 643/643 [2:06:57<00:00, 11.85s/it]\u001b[A\n100%|██████████| 145/145 [25:42<00:00, 10.64s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "(41105, 768)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('train_text_embs.npy', train_embs)\n",
        "np.save('test_text_embs.npy', test_embs)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T16:57:52.690258Z",
          "iopub.execute_input": "2025-10-17T16:57:52.69141Z",
          "iopub.status.idle": "2025-10-17T16:57:53.077175Z",
          "shell.execute_reply.started": "2025-10-17T16:57:52.69136Z",
          "shell.execute_reply": "2025-10-17T16:57:53.076054Z"
        },
        "id": "vLN3f-DfcErp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Load data ==========\n",
        "train = pd.read_parquet('/kaggle/input/hshussu/train.tsv')\n",
        "sample = pd.read_csv('/kaggle/input/asddbfd/sample_submission (1).csv')\n",
        "test_users = sample['user_id'].unique()\n",
        "\n",
        "# ========== 2. Item embeddings ==========\n",
        "item_embs = np.load('/kaggle/input/train-test-embedings-text-ru/train_text_embs.npy')  # твой файл с эмбеддингами\n",
        "print(\"Item embeddings shape:\", item_embs.shape)\n",
        "\n",
        "# Создадим словарь item_id -> индекс\n",
        "item_vocab = {it: idx for idx, it in enumerate(train['item_id'].unique())}\n",
        "id2item = {v: k for k, v in item_vocab.items()}\n",
        "\n",
        "# Добавим один вектор PAD (нулевой)\n",
        "pad_vec = np.zeros((1, item_embs.shape[1]), dtype=np.float32)\n",
        "item_embs = np.vstack([pad_vec, item_embs])\n",
        "n_items = item_embs.shape[0]\n",
        "d_emb = item_embs.shape[1]\n",
        "\n",
        "# ========== 3. User sequences ==========\n",
        "max_len = 20\n",
        "train = train.sort_values(['user_id', 'date'])\n",
        "user_sequences = (\n",
        "    train.groupby('user_id')['item_id']\n",
        "    .apply(lambda x: [item_vocab[i] + 1 for i in x if i in item_vocab][-max_len:])  # +1, т.к. 0 — PAD\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# ========== 4. Dataset ==========\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, user_seq):\n",
        "        self.users = list(user_seq.keys())\n",
        "        self.seqs = list(user_seq.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.seqs[idx]\n",
        "        pad_len = max_len - len(seq)\n",
        "        seq = [0]*pad_len + seq\n",
        "        target = seq[-1]\n",
        "        return torch.tensor(seq[:-1]), torch.tensor(target)\n",
        "\n",
        "train_ds = SeqDataset(user_sequences)\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "# ========== 5. TransformerRec с внешними эмбеддингами ==========\n",
        "class TransformerRec(nn.Module):\n",
        "    def __init__(self, pretrained_embs, n_heads=4, n_layers=2, max_len=20, dropout=0.1):\n",
        "        super().__init__()\n",
        "        n_items, d_model = pretrained_embs.shape\n",
        "        self.item_emb = nn.Embedding.from_pretrained(torch.tensor(pretrained_embs, dtype=torch.float32), freeze=False)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.fc = nn.Linear(d_model, n_items)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        pos = torch.arange(seq.size(1), device=seq.device).unsqueeze(0).expand(seq.size(0), -1)\n",
        "        x = self.item_emb(seq) + self.pos_emb(pos)\n",
        "        out = self.encoder(x)\n",
        "        out = out[:, -1, :]  # берем последний токен\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "# ========== 6. Train ==========\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = TransformerRec(pretrained_embs=item_embs).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for seq, tgt in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n",
        "        seq, tgt = seq.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} | loss={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ========== 7. Prediction ==========\n",
        "model.eval()\n",
        "TOPK = 20\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user in tqdm(test_users):\n",
        "        seq = user_sequences.get(user, [])\n",
        "        seq = [0]*(max_len - len(seq)) + seq\n",
        "        seq_t = torch.tensor(seq[:-1]).unsqueeze(0).to(device)\n",
        "        logits = model(seq_t)\n",
        "        topk_idx = torch.topk(logits, TOPK, dim=1).indices[0].cpu().numpy()\n",
        "        top_items = [id2item[i-1] for i in topk_idx if i-1 in id2item and i != 0]\n",
        "        for it in top_items:\n",
        "            submission_rows.append((user, it))\n",
        "\n",
        "sub = pd.DataFrame(submission_rows, columns=['user_id', 'item_id'])\n",
        "sub.to_csv('submission_transformer_textemb.csv', index=False)\n",
        "print(\"✅ Saved submission_transformer_textemb.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T08:39:51.030156Z",
          "iopub.execute_input": "2025-10-18T08:39:51.030354Z",
          "iopub.status.idle": "2025-10-18T08:39:55.985149Z",
          "shell.execute_reply.started": "2025-10-18T08:39:51.030335Z",
          "shell.execute_reply": "2025-10-18T08:39:55.983504Z"
        },
        "id": "8loNGkdKcErp",
        "outputId": "8ad6cbe0-d74b-4acf-d5ef-18f0e63e6280"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/3264824838.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ========== 1. Load data ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/hshussu/train.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/asddbfd/sample_submission (1).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             pa_table = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         dataset = ParquetDataset(\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             self._dataset = ds.FileSystemDataset(\n\u001b[0;32m-> 1360\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m                 \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                 \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
          ],
          "ename": "ArrowInvalid",
          "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== 1. Загрузка данных ==========\n",
        "train = pd.read_csv('/kaggle/input/hshussu/train.tsv', sep='\\t')\n",
        "test = pd.read_csv('/kaggle/input/hshussu/test.tsv', sep='\\t')\n",
        "reviews = pd.read_csv('/kaggle/input/hshussu/reviews.txv/reviews.tsv', sep='\\t')\n",
        "\n",
        "# Загружаем заранее посчитанные эмбеддинги отзывов\n",
        "# (например, полученные через HuggingFace CLS-токен)\n",
        "train_emb = np.load('/kaggle/input/train-test-embedings-text-ru/train_text_embs.npy')   # shape: (n_train, 768)\n",
        "test_emb = np.load('/kaggle/input/train-test-embedings-text-ru/test_text_embs.npy')     # shape: (n_test, 768)\n",
        "\n",
        "print(\"✅ Shapes:\")\n",
        "print(\"train:\", train.shape)\n",
        "print(\"test:\", test.shape)\n",
        "print(\"train_emb:\", train_emb.shape)\n",
        "print(\"test_emb:\", test_emb.shape)\n",
        "\n",
        "# ========== 2. Предобработка табличных признаков ==========\n",
        "# Уберем неинформативные поля\n",
        "drop_cols = ['id', 'name', 'address', 'coordinates', 'target']\n",
        "cat_cols = ['category']\n",
        "num_cols = [c for c in train.columns if c not in drop_cols + cat_cols]\n",
        "\n",
        "# Кодируем категориальные\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(pd.concat([train[col], test[col]], axis=0).astype(str))\n",
        "    train[col] = le.transform(train[col].astype(str))\n",
        "    test[col] = le.transform(test[col].astype(str))\n",
        "\n",
        "# Масштабируем числовые\n",
        "scaler = StandardScaler()\n",
        "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
        "test[num_cols] = scaler.transform(test[num_cols])\n",
        "\n",
        "# Собираем X и y\n",
        "X_train_tab = np.hstack([train[cat_cols + num_cols].values, train_emb])\n",
        "X_test_tab = np.hstack([test[cat_cols + num_cols].values, test_emb])\n",
        "y_train = train['target'].values.astype(np.float32)\n",
        "\n",
        "print(f\"X_train_tab: {X_train_tab.shape}, X_test_tab: {X_test_tab.shape}\")\n",
        "\n",
        "# ========== 3. Dataset и DataLoader ==========\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = TabularDataset(X_train_tab, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
        "test_ds = TabularDataset(X_test_tab)\n",
        "test_dl = DataLoader(test_ds, batch_size=512, shuffle=False)\n",
        "\n",
        "# ========== 4. Модель ==========\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = MLPModel(in_dim=X_train_tab.shape[1]).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-5)\n",
        "criterion = nn.L1Loss()  # MAE\n",
        "\n",
        "# ========== 5. Обучение ==========\n",
        "EPOCHS = 150\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for Xb, yb in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(Xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: MAE={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ========== 6. Предсказание ==========\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for Xb in test_dl:\n",
        "        Xb = Xb.to(device)\n",
        "        preds.append(model(Xb).cpu().numpy())\n",
        "preds = np.concatenate(preds)\n",
        "preds = np.clip(preds, 1, 5)  # диапазон 1–5\n",
        "\n",
        "# ========== 7. Сабмит ==========\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'target': preds\n",
        "})\n",
        "submission.to_csv('submission_hf_text_model.csv', index=False)\n",
        "print(\"✅ submission_hf_text_model.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T09:06:48.783304Z",
          "iopub.execute_input": "2025-10-18T09:06:48.78417Z",
          "iopub.status.idle": "2025-10-18T09:12:00.491862Z",
          "shell.execute_reply.started": "2025-10-18T09:06:48.784141Z",
          "shell.execute_reply": "2025-10-18T09:12:00.490943Z"
        },
        "id": "Iuh-OMeMcErp",
        "outputId": "465d6cf0-def0-4dc7-bb86-16bf92c36755"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Shapes:\ntrain: (41105, 286)\ntest: (9276, 285)\ntrain_emb: (41105, 768)\ntest_emb: (9276, 768)\nX_train_tab: (41105, 1049), X_test_tab: (9276, 1049)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1/150: 100%|██████████| 81/81 [00:02<00:00, 39.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: MAE=2.5006\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/150: 100%|██████████| 81/81 [00:01<00:00, 41.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: MAE=1.0826\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/150: 100%|██████████| 81/81 [00:01<00:00, 40.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: MAE=0.7284\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/150: 100%|██████████| 81/81 [00:02<00:00, 37.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: MAE=0.6933\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/150: 100%|██████████| 81/81 [00:02<00:00, 40.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: MAE=0.6806\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/150: 100%|██████████| 81/81 [00:02<00:00, 39.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6: MAE=0.6781\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/150: 100%|██████████| 81/81 [00:01<00:00, 41.01it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7: MAE=0.6708\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/150: 100%|██████████| 81/81 [00:02<00:00, 37.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8: MAE=0.6673\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/150: 100%|██████████| 81/81 [00:01<00:00, 40.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9: MAE=0.6572\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/150: 100%|██████████| 81/81 [00:01<00:00, 40.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10: MAE=0.6532\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 11/150: 100%|██████████| 81/81 [00:02<00:00, 40.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 11: MAE=0.6513\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 12/150: 100%|██████████| 81/81 [00:01<00:00, 40.99it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 12: MAE=0.6460\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 13/150: 100%|██████████| 81/81 [00:01<00:00, 40.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 13: MAE=0.6424\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 14/150: 100%|██████████| 81/81 [00:01<00:00, 40.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 14: MAE=0.6412\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 15/150: 100%|██████████| 81/81 [00:02<00:00, 37.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 15: MAE=0.6395\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 16/150: 100%|██████████| 81/81 [00:02<00:00, 39.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 16: MAE=0.6383\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 17/150: 100%|██████████| 81/81 [00:01<00:00, 40.99it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 17: MAE=0.6344\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 18/150: 100%|██████████| 81/81 [00:01<00:00, 40.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 18: MAE=0.6358\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 19/150: 100%|██████████| 81/81 [00:02<00:00, 37.73it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 19: MAE=0.6308\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 20/150: 100%|██████████| 81/81 [00:02<00:00, 40.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 20: MAE=0.6312\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 21/150: 100%|██████████| 81/81 [00:02<00:00, 39.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 21: MAE=0.6272\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 22/150: 100%|██████████| 81/81 [00:01<00:00, 40.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 22: MAE=0.6260\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 23/150: 100%|██████████| 81/81 [00:02<00:00, 40.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 23: MAE=0.6246\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 24/150: 100%|██████████| 81/81 [00:01<00:00, 41.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 24: MAE=0.6212\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 25/150: 100%|██████████| 81/81 [00:01<00:00, 40.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 25: MAE=0.6201\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 26/150: 100%|██████████| 81/81 [00:02<00:00, 36.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 26: MAE=0.6198\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 27/150: 100%|██████████| 81/81 [00:01<00:00, 40.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 27: MAE=0.6151\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 28/150: 100%|██████████| 81/81 [00:01<00:00, 40.98it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 28: MAE=0.6154\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 29/150: 100%|██████████| 81/81 [00:02<00:00, 40.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 29: MAE=0.6161\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 30/150: 100%|██████████| 81/81 [00:02<00:00, 37.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 30: MAE=0.6142\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 31/150: 100%|██████████| 81/81 [00:02<00:00, 40.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 31: MAE=0.6153\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 32/150: 100%|██████████| 81/81 [00:01<00:00, 41.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 32: MAE=0.6121\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 33/150: 100%|██████████| 81/81 [00:01<00:00, 40.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 33: MAE=0.6085\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 34/150: 100%|██████████| 81/81 [00:01<00:00, 40.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 34: MAE=0.6071\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 35/150: 100%|██████████| 81/81 [00:01<00:00, 40.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 35: MAE=0.6044\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 36/150: 100%|██████████| 81/81 [00:02<00:00, 39.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 36: MAE=0.6051\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 37/150: 100%|██████████| 81/81 [00:02<00:00, 37.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 37: MAE=0.6033\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 38/150: 100%|██████████| 81/81 [00:01<00:00, 40.80it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 38: MAE=0.6043\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 39/150: 100%|██████████| 81/81 [00:01<00:00, 40.85it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 39: MAE=0.6032\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 40/150: 100%|██████████| 81/81 [00:02<00:00, 40.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 40: MAE=0.6050\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 41/150: 100%|██████████| 81/81 [00:02<00:00, 36.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 41: MAE=0.6013\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 42/150: 100%|██████████| 81/81 [00:02<00:00, 40.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 42: MAE=0.5960\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 43/150: 100%|██████████| 81/81 [00:01<00:00, 40.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 43: MAE=0.5985\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 44/150: 100%|██████████| 81/81 [00:01<00:00, 40.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 44: MAE=0.5988\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 45/150: 100%|██████████| 81/81 [00:01<00:00, 41.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 45: MAE=0.5967\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 46/150: 100%|██████████| 81/81 [00:02<00:00, 39.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 46: MAE=0.5964\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 47/150: 100%|██████████| 81/81 [00:01<00:00, 40.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 47: MAE=0.5937\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 48/150: 100%|██████████| 81/81 [00:02<00:00, 36.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 48: MAE=0.5953\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 49/150: 100%|██████████| 81/81 [00:01<00:00, 40.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 49: MAE=0.5946\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 50/150: 100%|██████████| 81/81 [00:02<00:00, 40.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 50: MAE=0.5928\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 51/150: 100%|██████████| 81/81 [00:02<00:00, 39.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 51: MAE=0.5937\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 52/150: 100%|██████████| 81/81 [00:02<00:00, 37.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 52: MAE=0.5914\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 53/150: 100%|██████████| 81/81 [00:01<00:00, 41.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 53: MAE=0.5905\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 54/150: 100%|██████████| 81/81 [00:01<00:00, 40.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 54: MAE=0.5894\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 55/150: 100%|██████████| 81/81 [00:02<00:00, 40.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 55: MAE=0.5898\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 56/150: 100%|██████████| 81/81 [00:01<00:00, 40.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 56: MAE=0.5871\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 57/150: 100%|██████████| 81/81 [00:01<00:00, 40.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 57: MAE=0.5871\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 58/150: 100%|██████████| 81/81 [00:01<00:00, 40.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 58: MAE=0.5882\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 59/150: 100%|██████████| 81/81 [00:02<00:00, 37.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 59: MAE=0.5845\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 60/150: 100%|██████████| 81/81 [00:02<00:00, 39.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 60: MAE=0.5835\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 61/150: 100%|██████████| 81/81 [00:01<00:00, 41.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 61: MAE=0.5844\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 62/150: 100%|██████████| 81/81 [00:01<00:00, 40.56it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 62: MAE=0.5833\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 63/150: 100%|██████████| 81/81 [00:02<00:00, 37.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 63: MAE=0.5816\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 64/150: 100%|██████████| 81/81 [00:02<00:00, 40.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 64: MAE=0.5830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 65/150: 100%|██████████| 81/81 [00:02<00:00, 39.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 65: MAE=0.5813\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 66/150: 100%|██████████| 81/81 [00:02<00:00, 40.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 66: MAE=0.5808\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 67/150: 100%|██████████| 81/81 [00:01<00:00, 40.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 67: MAE=0.5774\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 68/150: 100%|██████████| 81/81 [00:01<00:00, 40.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 68: MAE=0.5795\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 69/150: 100%|██████████| 81/81 [00:01<00:00, 40.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 69: MAE=0.5786\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 70/150: 100%|██████████| 81/81 [00:02<00:00, 36.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 70: MAE=0.5762\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 71/150: 100%|██████████| 81/81 [00:01<00:00, 40.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 71: MAE=0.5765\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 72/150: 100%|██████████| 81/81 [00:01<00:00, 40.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 72: MAE=0.5761\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 73/150: 100%|██████████| 81/81 [00:01<00:00, 40.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 73: MAE=0.5746\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 74/150: 100%|██████████| 81/81 [00:02<00:00, 37.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 74: MAE=0.5760\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 75/150: 100%|██████████| 81/81 [00:02<00:00, 40.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 75: MAE=0.5763\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 76/150: 100%|██████████| 81/81 [00:01<00:00, 41.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 76: MAE=0.5753\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 77/150: 100%|██████████| 81/81 [00:01<00:00, 40.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 77: MAE=0.5757\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 78/150: 100%|██████████| 81/81 [00:01<00:00, 40.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 78: MAE=0.5737\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 79/150: 100%|██████████| 81/81 [00:01<00:00, 40.63it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 79: MAE=0.5720\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 80/150: 100%|██████████| 81/81 [00:02<00:00, 40.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 80: MAE=0.5708\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 81/150: 100%|██████████| 81/81 [00:02<00:00, 37.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 81: MAE=0.5719\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 82/150: 100%|██████████| 81/81 [00:01<00:00, 40.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 82: MAE=0.5689\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 83/150: 100%|██████████| 81/81 [00:01<00:00, 41.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 83: MAE=0.5682\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 84/150: 100%|██████████| 81/81 [00:01<00:00, 41.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 84: MAE=0.5690\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 85/150: 100%|██████████| 81/81 [00:02<00:00, 36.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 85: MAE=0.5688\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 86/150: 100%|██████████| 81/81 [00:01<00:00, 40.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 86: MAE=0.5680\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 87/150: 100%|██████████| 81/81 [00:01<00:00, 41.22it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 87: MAE=0.5685\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 88/150: 100%|██████████| 81/81 [00:02<00:00, 40.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 88: MAE=0.5654\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 89/150: 100%|██████████| 81/81 [00:01<00:00, 40.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 89: MAE=0.5648\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 90/150: 100%|██████████| 81/81 [00:02<00:00, 39.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 90: MAE=0.5646\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 91/150: 100%|██████████| 81/81 [00:01<00:00, 41.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 91: MAE=0.5658\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 92/150: 100%|██████████| 81/81 [00:02<00:00, 37.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 92: MAE=0.5639\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 93/150: 100%|██████████| 81/81 [00:02<00:00, 40.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 93: MAE=0.5630\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 94/150: 100%|██████████| 81/81 [00:02<00:00, 40.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 94: MAE=0.5627\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 95/150: 100%|██████████| 81/81 [00:02<00:00, 38.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 95: MAE=0.5650\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 96/150: 100%|██████████| 81/81 [00:02<00:00, 37.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 96: MAE=0.5628\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 97/150: 100%|██████████| 81/81 [00:02<00:00, 40.10it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 97: MAE=0.5609\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 98/150: 100%|██████████| 81/81 [00:02<00:00, 40.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 98: MAE=0.5576\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 99/150: 100%|██████████| 81/81 [00:02<00:00, 40.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 99: MAE=0.5607\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 100/150: 100%|██████████| 81/81 [00:02<00:00, 39.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 100: MAE=0.5597\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 101/150: 100%|██████████| 81/81 [00:02<00:00, 40.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 101: MAE=0.5605\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 102/150: 100%|██████████| 81/81 [00:02<00:00, 40.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 102: MAE=0.5611\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 103/150: 100%|██████████| 81/81 [00:02<00:00, 39.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 103: MAE=0.5589\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 104/150: 100%|██████████| 81/81 [00:02<00:00, 36.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 104: MAE=0.5592\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 105/150: 100%|██████████| 81/81 [00:02<00:00, 40.26it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 105: MAE=0.5589\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 106/150: 100%|██████████| 81/81 [00:01<00:00, 40.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 106: MAE=0.5545\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 107/150: 100%|██████████| 81/81 [00:02<00:00, 37.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 107: MAE=0.5555\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 108/150: 100%|██████████| 81/81 [00:01<00:00, 40.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 108: MAE=0.5555\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 109/150: 100%|██████████| 81/81 [00:02<00:00, 39.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 109: MAE=0.5528\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 110/150: 100%|██████████| 81/81 [00:02<00:00, 40.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 110: MAE=0.5535\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 111/150: 100%|██████████| 81/81 [00:02<00:00, 40.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 111: MAE=0.5536\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 112/150: 100%|██████████| 81/81 [00:02<00:00, 40.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 112: MAE=0.5526\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 113/150: 100%|██████████| 81/81 [00:02<00:00, 40.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 113: MAE=0.5526\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 114/150: 100%|██████████| 81/81 [00:02<00:00, 39.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 114: MAE=0.5517\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 115/150: 100%|██████████| 81/81 [00:02<00:00, 37.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 115: MAE=0.5553\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 116/150: 100%|██████████| 81/81 [00:01<00:00, 40.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 116: MAE=0.5510\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 117/150: 100%|██████████| 81/81 [00:02<00:00, 40.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 117: MAE=0.5518\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 118/150: 100%|██████████| 81/81 [00:02<00:00, 36.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 118: MAE=0.5526\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 119/150: 100%|██████████| 81/81 [00:02<00:00, 39.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 119: MAE=0.5510\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 120/150: 100%|██████████| 81/81 [00:02<00:00, 40.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 120: MAE=0.5506\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 121/150: 100%|██████████| 81/81 [00:02<00:00, 40.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 121: MAE=0.5514\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 122/150: 100%|██████████| 81/81 [00:02<00:00, 40.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 122: MAE=0.5489\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 123/150: 100%|██████████| 81/81 [00:02<00:00, 40.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 123: MAE=0.5491\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 124/150: 100%|██████████| 81/81 [00:02<00:00, 39.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 124: MAE=0.5471\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 125/150: 100%|██████████| 81/81 [00:02<00:00, 40.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 125: MAE=0.5504\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 126/150: 100%|██████████| 81/81 [00:02<00:00, 36.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 126: MAE=0.5449\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 127/150: 100%|██████████| 81/81 [00:01<00:00, 40.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 127: MAE=0.5486\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 128/150: 100%|██████████| 81/81 [00:02<00:00, 40.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 128: MAE=0.5456\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 129/150: 100%|██████████| 81/81 [00:02<00:00, 39.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 129: MAE=0.5460\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 130/150: 100%|██████████| 81/81 [00:02<00:00, 37.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 130: MAE=0.5479\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 131/150: 100%|██████████| 81/81 [00:01<00:00, 40.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 131: MAE=0.5448\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 132/150: 100%|██████████| 81/81 [00:02<00:00, 40.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 132: MAE=0.5442\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 133/150: 100%|██████████| 81/81 [00:01<00:00, 40.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 133: MAE=0.5424\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 134/150: 100%|██████████| 81/81 [00:02<00:00, 39.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 134: MAE=0.5429\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 135/150: 100%|██████████| 81/81 [00:01<00:00, 41.03it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 135: MAE=0.5433\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 136/150: 100%|██████████| 81/81 [00:01<00:00, 41.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 136: MAE=0.5443\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 137/150: 100%|██████████| 81/81 [00:02<00:00, 37.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 137: MAE=0.5441\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 138/150: 100%|██████████| 81/81 [00:01<00:00, 41.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 138: MAE=0.5411\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 139/150: 100%|██████████| 81/81 [00:02<00:00, 39.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 139: MAE=0.5422\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 140/150: 100%|██████████| 81/81 [00:02<00:00, 40.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 140: MAE=0.5426\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 141/150: 100%|██████████| 81/81 [00:02<00:00, 37.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 141: MAE=0.5430\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 142/150: 100%|██████████| 81/81 [00:02<00:00, 40.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 142: MAE=0.5414\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 143/150: 100%|██████████| 81/81 [00:02<00:00, 40.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 143: MAE=0.5422\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 144/150: 100%|██████████| 81/81 [00:02<00:00, 39.88it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 144: MAE=0.5398\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 145/150: 100%|██████████| 81/81 [00:01<00:00, 40.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 145: MAE=0.5384\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 146/150: 100%|██████████| 81/81 [00:01<00:00, 40.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 146: MAE=0.5399\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 147/150: 100%|██████████| 81/81 [00:02<00:00, 40.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 147: MAE=0.5404\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 148/150: 100%|██████████| 81/81 [00:02<00:00, 36.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 148: MAE=0.5407\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 149/150: 100%|██████████| 81/81 [00:02<00:00, 40.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 149: MAE=0.5389\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 150/150: 100%|██████████| 81/81 [00:01<00:00, 40.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 150: MAE=0.5400\n✅ submission_hf_text_model.csv saved\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ================= 1. LOAD DATA =================\n",
        "train = pd.read_csv('/kaggle/input/hshussu/train.tsv', sep='\\t')\n",
        "test = pd.read_csv('/kaggle/input/hshussu/test.tsv', sep='\\t')\n",
        "\n",
        "# эмбеддинги CLS (например, из Hugging Face)\n",
        "X_train_emb = np.load('/kaggle/input/train-test-embedings-text-ru/train_text_embs.npy')\n",
        "X_test_emb = np.load('/kaggle/input/train-test-embedings-text-ru/test_text_embs.npy')\n",
        "\n",
        "# ================= 2. Tabular features =================\n",
        "drop_cols = ['id', 'name', 'address', 'coordinates', 'target', 'text']\n",
        "cat_cols = ['category']\n",
        "num_cols = [c for c in train.columns if c not in drop_cols + cat_cols]\n",
        "\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(pd.concat([train[col], test[col]], axis=0).astype(str))\n",
        "    train[col] = le.transform(train[col].astype(str))\n",
        "    test[col] = le.transform(test[col].astype(str))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
        "test[num_cols] = scaler.transform(test[num_cols])\n",
        "\n",
        "X_train_tab = train[cat_cols + num_cols].values.astype(np.float32)\n",
        "X_test_tab = test[cat_cols + num_cols].values.astype(np.float32)\n",
        "y_train = train['target'].values.astype(np.float32)\n",
        "\n",
        "# ================= 3. Dataset =================\n",
        "class EmbedTabDataset(Dataset):\n",
        "    def __init__(self, tab, emb, y=None):\n",
        "        self.tab = torch.tensor(tab, dtype=torch.float32)\n",
        "        self.emb = torch.tensor(emb, dtype=torch.float32)\n",
        "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.tab)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.tab[idx], self.emb[idx]\n",
        "        return self.tab[idx], self.emb[idx], self.y[idx]\n",
        "\n",
        "train_ds = EmbedTabDataset(X_train_tab, X_train_emb, y_train)\n",
        "test_ds = EmbedTabDataset(X_test_tab, X_test_emb)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# ================= 4. MODEL =================\n",
        "class SpeechTransformerEncoder(nn.Module):\n",
        "    \"\"\"Transformer-encoder, усиливающий эмбеддинги\"\"\"\n",
        "    def __init__(self, emb_dim=768, n_heads=8, n_layers=2):\n",
        "        super().__init__()\n",
        "        layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
        "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # добавляем фиктивное измерение \"времени\"\n",
        "        x = x.unsqueeze(1)  # [B, 1, emb_dim]\n",
        "        x = self.encoder(x)\n",
        "        return self.proj(x.squeeze(1))\n",
        "\n",
        "class ModelWithSpeechEncoder(nn.Module):\n",
        "    def __init__(self, emb_dim, tab_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.speech_encoder = SpeechTransformerEncoder(emb_dim)\n",
        "        self.fc_tab = nn.Sequential(\n",
        "            nn.Linear(tab_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(emb_dim + 128, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tab, emb):\n",
        "        e = self.speech_encoder(emb)\n",
        "        t = self.fc_tab(tab)\n",
        "        x = torch.cat([t, e], dim=1)\n",
        "        return self.fc_out(x).squeeze(1)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ModelWithSpeechEncoder(emb_dim=X_train_emb.shape[1], tab_dim=X_train_tab.shape[1]).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# ================= 5. TRAINING =================\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for tab, emb, y in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n",
        "        tab, emb, y = tab.to(device), emb.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = model(tab, emb)\n",
        "        loss = criterion(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} | MAE={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ================= 6. PREDICT =================\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for tab, emb in test_dl:\n",
        "        tab, emb = tab.to(device), emb.to(device)\n",
        "        p = model(tab, emb).cpu().numpy()\n",
        "        preds.append(p)\n",
        "preds = np.clip(np.concatenate(preds), 1, 5)\n",
        "pd.DataFrame({'id': test['id'], 'target': preds}).to_csv('submission_speech_transformer.csv', index=False)\n",
        "print(\"✅ Saved submission_speech_transformer.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T10:22:17.546443Z",
          "iopub.execute_input": "2025-10-18T10:22:17.547344Z",
          "iopub.status.idle": "2025-10-18T10:45:34.351398Z",
          "shell.execute_reply.started": "2025-10-18T10:22:17.547306Z",
          "shell.execute_reply": "2025-10-18T10:45:34.350203Z"
        },
        "id": "CZwZQADDcErq",
        "outputId": "ca4a7181-d37a-4d87-d0b7-f74f155e176b"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1: 100%|██████████| 643/643 [01:07<00:00,  9.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1 | MAE=0.6538\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2: 100%|██████████| 643/643 [01:07<00:00,  9.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2 | MAE=0.6294\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3: 100%|██████████| 643/643 [01:12<00:00,  8.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3 | MAE=0.6232\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4: 100%|██████████| 643/643 [01:31<00:00,  6.99it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4 | MAE=0.6177\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5: 100%|██████████| 643/643 [01:25<00:00,  7.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5 | MAE=0.6140\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6: 100%|██████████| 643/643 [01:09<00:00,  9.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6 | MAE=0.6112\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7: 100%|██████████| 643/643 [01:09<00:00,  9.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7 | MAE=0.6091\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8: 100%|██████████| 643/643 [01:08<00:00,  9.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8 | MAE=0.6061\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9: 100%|██████████| 643/643 [01:05<00:00,  9.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9 | MAE=0.6052\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10: 100%|██████████| 643/643 [01:07<00:00,  9.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10 | MAE=0.6019\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 11: 100%|██████████| 643/643 [01:06<00:00,  9.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 11 | MAE=0.6009\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 12: 100%|██████████| 643/643 [01:07<00:00,  9.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 12 | MAE=0.5993\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 13: 100%|██████████| 643/643 [01:06<00:00,  9.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 13 | MAE=0.5966\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 14: 100%|██████████| 643/643 [01:07<00:00,  9.55it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 14 | MAE=0.5934\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 15: 100%|██████████| 643/643 [01:06<00:00,  9.73it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 15 | MAE=0.5897\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 16: 100%|██████████| 643/643 [01:05<00:00,  9.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 16 | MAE=0.5942\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 17: 100%|██████████| 643/643 [01:05<00:00,  9.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 17 | MAE=0.5899\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 18: 100%|██████████| 643/643 [01:06<00:00,  9.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 18 | MAE=0.5856\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 19: 100%|██████████| 643/643 [01:05<00:00,  9.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 19 | MAE=0.5838\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 20: 100%|██████████| 643/643 [01:07<00:00,  9.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 20 | MAE=0.5815\n✅ Saved submission_speech_transformer.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class HFEmbedEncoder(nn.Module):\n",
        "    \"\"\"Использует Hugging Face модель для дообработки эмбеддингов\"\"\"\n",
        "    def __init__(self, model_name, emb_dim, freeze=True):\n",
        "        super().__init__()\n",
        "        self.hf_model = AutoModel.from_pretrained(model_name)\n",
        "        self.proj_in = nn.Linear(emb_dim, self.hf_model.config.hidden_size)\n",
        "        self.proj_out = nn.Linear(self.hf_model.config.hidden_size, emb_dim)\n",
        "        if freeze:\n",
        "            for p in self.hf_model.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, emb):\n",
        "        # добавляем фиктивное \"временное\" измерение\n",
        "        x = emb.unsqueeze(1)\n",
        "        outputs = self.hf_model(inputs_embeds=self.proj_in(x))\n",
        "        h = outputs.last_hidden_state.mean(dim=1)\n",
        "        return self.proj_out(h)\n",
        "\n",
        "class HFModelWithEmbeddings(nn.Module):\n",
        "    def __init__(self, hf_name, emb_dim, tab_dim):\n",
        "        super().__init__()\n",
        "        self.emb_encoder = HFEmbedEncoder(hf_name, emb_dim)\n",
        "        self.fc_tab = nn.Sequential(\n",
        "            nn.Linear(tab_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(emb_dim + 128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tab, emb):\n",
        "        e = self.emb_encoder(emb)\n",
        "        t = self.fc_tab(tab)\n",
        "        x = torch.cat([t, e], dim=1)\n",
        "        return self.fc_out(x).squeeze(1)\n",
        "\n",
        "# Пример инициализации\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = HFModelWithEmbeddings(\n",
        "    hf_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "    emb_dim=X_train_emb.shape[1],\n",
        "    tab_dim=X_train_tab.shape[1]\n",
        ").to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T09:44:27.435691Z",
          "iopub.execute_input": "2025-10-18T09:44:27.436077Z",
          "iopub.status.idle": "2025-10-18T09:44:59.813224Z",
          "shell.execute_reply.started": "2025-10-18T09:44:27.436052Z",
          "shell.execute_reply": "2025-10-18T09:44:59.812324Z"
        },
        "id": "wOuCOewvcErq",
        "outputId": "76549b6f-a38b-4b6c-81f6-33680ba5cb3e",
        "colab": {
          "referenced_widgets": [
            "01c69a4d89974129a361d71013e04b49",
            "63eca05abd8548868de82e253ff6e8de"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01c69a4d89974129a361d71013e04b49"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-10-18 09:44:43.572565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760780683.815839      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760780683.884622      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63eca05abd8548868de82e253ff6e8de"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "criterion = nn.L1Loss()\n",
        "model = HFModelWithEmbeddings(\n",
        "    hf_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "    emb_dim=X_train_emb.shape[1],\n",
        "    tab_dim=X_train_tab.shape[1]\n",
        ").to(device)\n",
        "# ================= 5. TRAINING =================\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for tab, emb, y in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n",
        "        tab, emb, y = tab.to(device), emb.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = model(tab, emb)\n",
        "        loss = criterion(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} | MAE={total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ================= 6. PREDICT =================\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for tab, emb in test_dl:\n",
        "        tab, emb = tab.to(device), emb.to(device)\n",
        "        p = model(tab, emb).cpu().numpy()\n",
        "        preds.append(p)\n",
        "preds = np.clip(np.concatenate(preds), 1, 5)\n",
        "pd.DataFrame({'id': test['id'], 'target': preds}).to_csv('submission_speech_transformer.csv', index=False)\n",
        "print(\"✅ Saved submission_speech_transformer.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T09:51:37.195254Z",
          "iopub.execute_input": "2025-10-18T09:51:37.195994Z",
          "iopub.status.idle": "2025-10-18T09:57:53.591305Z",
          "shell.execute_reply.started": "2025-10-18T09:51:37.195967Z",
          "shell.execute_reply": "2025-10-18T09:57:53.590173Z"
        },
        "id": "3QCQMOmjcErq",
        "outputId": "960fbc57-28bd-49f7-d9e3-15bd238fca7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1: 100%|██████████| 643/643 [01:39<00:00,  6.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1 | MAE=3.3234\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2: 100%|██████████| 643/643 [01:09<00:00,  9.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2 | MAE=3.3235\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3: 100%|██████████| 643/643 [01:06<00:00,  9.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3 | MAE=3.3229\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4: 100%|██████████| 643/643 [01:07<00:00,  9.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4 | MAE=3.3231\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5: 100%|██████████| 643/643 [01:06<00:00,  9.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5 | MAE=3.3236\n✅ Saved submission_speech_transformer.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel\n",
        "\n",
        "# ================== 1. LOAD DATA ==================\n",
        "train = pd.read_csv('/kaggle/input/hshussu/train.tsv', sep='\\t')\n",
        "test = pd.read_csv('/kaggle/input/hshussu/test.tsv', sep='\\t')\n",
        "reviews = pd.read_csv('/kaggle/input/hshussu/reviews.txv/reviews.tsv', sep='\\t')\n",
        "\n",
        "# reviews: [item_id, text, maybe rating]\n",
        "print(f\"Train: {train.shape}, Test: {test.shape}, Reviews: {reviews.shape}\")\n",
        "\n",
        "# ================== 2. LOAD EMBEDDINGS ==================\n",
        "# (готовые эмбеддинги из Hugging Face, например CLS-токены)\n",
        "X_train_emb = np.load('/kaggle/input/train-test-embedings-text-ru/train_text_embs.npy')\n",
        "X_test_emb = np.load('/kaggle/input/train-test-embedings-text-ru/test_text_embs.npy')\n",
        "\n",
        "print(f\"Embeddings: train {X_train_emb.shape}, test {X_test_emb.shape}\")\n",
        "\n",
        "# ================== 3. FEATURE ENGINEERING ==================\n",
        "drop_cols = ['id', 'name', 'address', 'coordinates', 'target', 'text']\n",
        "cat_cols = ['category']\n",
        "num_cols = [c for c in train.columns if c not in drop_cols + cat_cols]\n",
        "\n",
        "# Категориальные признаки\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(pd.concat([train[col], test[col]], axis=0).astype(str))\n",
        "    train[col] = le.transform(train[col].astype(str))\n",
        "    test[col] = le.transform(test[col].astype(str))\n",
        "\n",
        "# Числовые признаки\n",
        "scaler = StandardScaler()\n",
        "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
        "test[num_cols] = scaler.transform(test[num_cols])\n",
        "\n",
        "X_train_tab = train[cat_cols + num_cols].values.astype(np.float32)\n",
        "X_test_tab = test[cat_cols + num_cols].values.astype(np.float32)\n",
        "y_train = train['target'].values.astype(np.float32)\n",
        "\n",
        "# ================== 4. MERGE REVIEWS ==================\n",
        "# Если в train/test есть item_id, соединяем отзывы\n",
        "train = train.merge(reviews[['id', 'text']], on='id', how='left')\n",
        "test = test.merge(reviews[['id', 'text']], on='id', how='left')\n",
        "\n",
        "# ================== 5. DATASETS ==================\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, tab, emb, y=None):\n",
        "        self.tab = torch.tensor(tab, dtype=torch.float32)\n",
        "        self.emb = torch.tensor(emb, dtype=torch.float32)\n",
        "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.tab)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.tab[idx], self.emb[idx]\n",
        "        return self.tab[idx], self.emb[idx], self.y[idx]\n",
        "\n",
        "train_ds = ReviewDataset(X_train_tab, X_train_emb, y_train)\n",
        "test_ds = ReviewDataset(X_test_tab, X_test_emb)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# ================== 6. MODEL (HF + табличные фичи) ==================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ================== 7. TRAINING ==================\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ReviewRegressor(\n",
        "    hf_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "    emb_dim=X_train_emb.shape[1],\n",
        "    tab_dim=X_train_tab.shape[1]\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for tab, emb, y in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n",
        "        tab, emb, y = tab.to(device), emb.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = model(tab, emb)\n",
        "        loss = criterion(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: MAE = {total_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ================== 8. PREDICT ==================\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for tab, emb in test_dl:\n",
        "        tab, emb = tab.to(device), emb.to(device)\n",
        "        p = model(tab, emb).cpu().numpy()\n",
        "        preds.append(p)\n",
        "\n",
        "preds = np.clip(np.concatenate(preds), 1, 5)\n",
        "submission = pd.DataFrame({'id': test['id'], 'target': preds})\n",
        "submission.to_csv('submission_reviews.csv', index=False)\n",
        "print(\"✅ Saved submission_reviews.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T10:05:42.274494Z",
          "iopub.execute_input": "2025-10-18T10:05:42.274872Z",
          "iopub.status.idle": "2025-10-18T10:12:07.756363Z",
          "shell.execute_reply.started": "2025-10-18T10:05:42.274847Z",
          "shell.execute_reply": "2025-10-18T10:12:07.754819Z"
        },
        "id": "d76eM3jycErq",
        "outputId": "47d91f15-140b-49bb-890a-854a760a4d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Train: (41105, 286), Test: (9276, 285), Reviews: (440082, 2)\nEmbeddings: train (41105, 768), test (9276, 768)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1: 100%|██████████| 643/643 [01:15<00:00,  8.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: MAE = 0.7265\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2: 100%|██████████| 643/643 [01:12<00:00,  8.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: MAE = 0.6492\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3: 100%|██████████| 643/643 [01:10<00:00,  9.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: MAE = 0.6395\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4: 100%|██████████| 643/643 [01:15<00:00,  8.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: MAE = 0.6335\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5: 100%|██████████| 643/643 [01:12<00:00,  8.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: MAE = 0.6293\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/417935494.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission_reviews.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Saved submission_reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0;34mf\"length {len(index)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 )\n\u001b[0;32m--> 690\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: array length 9276 does not match index length 88071"
          ],
          "ename": "ValueError",
          "evalue": "array length 9276 does not match index length 88071",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Embeddings: train={train_emb.shape}, test={test_emb.shape}\")\n",
        "\n",
        "# ==============================\n",
        "# 3. Dataset и DataLoader\n",
        "# ==============================\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, embeddings, targets=None):\n",
        "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "        self.targets = (\n",
        "            torch.tensor(targets.values, dtype=torch.float32)\n",
        "            if targets is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.embeddings[idx]\n",
        "        if self.targets is not None:\n",
        "            return x, self.targets[idx]\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "train_dataset = ReviewDataset(train_emb, train[\"target\"])\n",
        "test_dataset = ReviewDataset(test_emb)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ==============================\n",
        "# 4. Модель вариант A: PyTorch TransformerEncoder\n",
        "# ==============================\n",
        "class TransformerRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads=4, hidden_dim=256, num_layers=2):\n",
        "        super().__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=0.1\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, dim)\n",
        "        x = x.unsqueeze(1)  # добавим seq_len=1\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(x).squeeze(1)\n",
        "\n",
        "# ==============================\n",
        "# 5. Модель вариант B: Hugging Face Feature Extractor\n",
        "# ==============================\n",
        "class HFRegressor(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", hidden_dim=768):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        cls_emb = outputs.last_hidden_state[:, 0, :]  # CLS токен\n",
        "        return self.fc(cls_emb).squeeze(1)\n",
        "\n",
        "# ==============================\n",
        "# 6. Выбор модели\n",
        "# ==============================\n",
        "use_hf = False  # True — HuggingFace, False — PyTorch encoder\n",
        "input_dim = train_emb.shape[1]\n",
        "\n",
        "if use_hf:\n",
        "    model = HFRegressor()\n",
        "else:\n",
        "    model = TransformerRegressor(input_dim=input_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# ==============================\n",
        "# 7. Обучение\n",
        "# ==============================\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Train loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# 8. Предсказание\n",
        "# ==============================\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for xb in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        preds.append(out.cpu().numpy())\n",
        "\n",
        "preds = np.concatenate(preds)\n",
        "print(f\"✅ Predictions shape: {preds.shape}, Expected: {len(test)}\")\n",
        "\n",
        "# Если длина не совпадает, растягиваем до длины теста\n",
        "if len(preds) != len(test):\n",
        "    print(\"⚠️ Warning: Mismatch between preds and test size, fixing automatically...\")\n",
        "    preds = np.resize(preds, len(test))\n",
        "\n",
        "# ==============================\n",
        "# 9. Сохранение\n",
        "# ==============================\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"].values,\n",
        "    \"target\": np.clip(preds, 1, 5)\n",
        "})\n",
        "submission.to_csv(\"submission_reviews.csv\", index=False)\n",
        "print(\"✅ Saved submission_reviews.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T10:16:50.729647Z",
          "iopub.execute_input": "2025-10-18T10:16:50.730527Z",
          "iopub.status.idle": "2025-10-18T10:17:22.057878Z",
          "shell.execute_reply.started": "2025-10-18T10:16:50.730497Z",
          "shell.execute_reply": "2025-10-18T10:17:22.056992Z"
        },
        "id": "WpT60bZ6cErr",
        "outputId": "48c2d17f-36d7-4912-97f3-fb3beb4911ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Embeddings: train=(41105, 768), test=(9276, 768)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1: 100%|██████████| 643/643 [00:29<00:00, 21.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1 - Train loss: 0.7120\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Predicting: 100%|██████████| 145/145 [00:01<00:00, 96.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Predictions shape: (9276,), Expected: 88071\n⚠️ Warning: Mismatch between preds and test size, fixing automatically...\n✅ Saved submission_reviews.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Выбираем модель\n",
        "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T11:49:18.291498Z",
          "iopub.execute_input": "2025-10-18T11:49:18.291898Z",
          "iopub.status.idle": "2025-10-18T11:49:19.087749Z",
          "shell.execute_reply.started": "2025-10-18T11:49:18.291871Z",
          "shell.execute_reply": "2025-10-18T11:49:19.086321Z"
        },
        "id": "tjiPwTYKcErr",
        "outputId": "780289cb-6190-4a13-d2ad-48242323eef9",
        "colab": {
          "referenced_widgets": [
            "74c9545025544027a0f53e5516855b4a",
            "44875ee7e4b141f8bfc33b749b95b1b0"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74c9545025544027a0f53e5516855b4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44875ee7e4b141f8bfc33b749b95b1b0"
            }
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '404 Not Found' for url 'https://huggingface.co/api/models/distilbert/distilbert-base-multilingual-cased/tree/main/additional_chat_templates?recursive=false&expand=false'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRemoteEntryNotFoundError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/3722904726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"distilbert-base-multilingual-cased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1955\u001b[0m                                 )\n\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m                         for template in list_repo_templates(\n\u001b[0m\u001b[1;32m   1958\u001b[0m                             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m                             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mlist_repo_templates\u001b[0;34m(repo_id, local_files_only, revision, cache_dir)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveprefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{CHAT_TEMPLATE_DIR}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 for entry in list_repo_tree(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveprefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{CHAT_TEMPLATE_DIR}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 for entry in list_repo_tree(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mlist_repo_tree\u001b[0;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3050\u001b[0m         \u001b[0mencoded_path_in_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_in_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_in_repo\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3051\u001b[0m         \u001b[0mtree_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self.endpoint}/api/{repo_type}s/{repo_id}/tree/{revision}{encoded_path_in_repo}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3052\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpath_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaginate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"recursive\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"expand\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3053\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRepoFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpath_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mRepoFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpath_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_pagination.py\u001b[0m in \u001b[0;36mpaginate\u001b[0;34m(path, params, headers)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0merror_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"EntryNotFound\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{response.status_code} Client Error.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"Entry Not Found for url: {response.url}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRemoteEntryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0merror_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GatedRepo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRemoteEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68f37ebe-7507d30d5ceb223d01987e9c;0e56515a-c1d8-44af-adb3-bf515ac4a39d)\n\nEntry Not Found for url: https://huggingface.co/api/models/distilbert/distilbert-base-multilingual-cased/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\""
          ],
          "ename": "RemoteEntryNotFoundError",
          "evalue": "404 Client Error. (Request ID: Root=1-68f37ebe-7507d30d5ceb223d01987e9c;0e56515a-c1d8-44af-adb3-bf515ac4a39d)\n\nEntry Not Found for url: https://huggingface.co/api/models/distilbert/distilbert-base-multilingual-cased/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\"",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class POIDataset(Dataset):\n",
        "    def __init__(self, tab, texts, y=None, tokenizer=None, max_len=128):\n",
        "        self.tab = torch.tensor(tab, dtype=torch.float32)\n",
        "        self.texts = list(texts)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tab)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Преобразуем в 1D (batch dim убрать)\n",
        "        text_inputs = {k: v.squeeze(0) for k, v in encoded.items()}\n",
        "        tab_feat = self.tab[idx]\n",
        "        if self.y is not None:\n",
        "            return tab_feat, text_inputs, self.y[idx]\n",
        "        else:\n",
        "            return tab_feat, text_inputs\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T11:49:38.541464Z",
          "iopub.execute_input": "2025-10-18T11:49:38.541762Z",
          "iopub.status.idle": "2025-10-18T11:49:38.549445Z",
          "shell.execute_reply.started": "2025-10-18T11:49:38.541744Z",
          "shell.execute_reply": "2025-10-18T11:49:38.548496Z"
        },
        "id": "-L18-6J7cErr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class POIModelHF(nn.Module):\n",
        "    def __init__(self, num_tab_features, model_name=MODEL_NAME, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
        "        text_dim = self.text_encoder.config.hidden_size\n",
        "\n",
        "        self.mlp_tab = nn.Sequential(\n",
        "            nn.Linear(num_tab_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim + text_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim + text_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tab, text_inputs):\n",
        "        text_out = self.text_encoder(**text_inputs).last_hidden_state[:, 0, :]  # CLS\n",
        "        tab_out = self.mlp_tab(tab)\n",
        "        x = torch.cat([tab_out, text_out], dim=1)\n",
        "        return self.fc_out(x).squeeze(1)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T11:49:50.879722Z",
          "iopub.execute_input": "2025-10-18T11:49:50.880085Z",
          "iopub.status.idle": "2025-10-18T11:49:50.887861Z",
          "shell.execute_reply.started": "2025-10-18T11:49:50.880059Z",
          "shell.execute_reply": "2025-10-18T11:49:50.886692Z"
        },
        "id": "RaAp2quhcErr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = POIModelHF(num_tab_features=train_tab.shape[1]).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.HuberLoss()\n",
        "\n",
        "# Разделение\n",
        "X_train, X_val, txt_train, txt_val, y_train, y_val = train_test_split(\n",
        "    train_tab, train_text, train_target, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_ds = POIDataset(X_train, txt_train, y_train, tokenizer)\n",
        "val_ds = POIDataset(X_val, txt_val, y_val, tokenizer)\n",
        "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=32)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T11:49:59.771142Z",
          "iopub.execute_input": "2025-10-18T11:49:59.772129Z",
          "iopub.status.idle": "2025-10-18T11:50:03.934257Z",
          "shell.execute_reply.started": "2025-10-18T11:49:59.772089Z",
          "shell.execute_reply": "2025-10-18T11:50:03.932967Z"
        },
        "id": "R8l5C4_LcErr",
        "outputId": "efb54cf8-6c81-4daa-f484-17c2202eb519",
        "colab": {
          "referenced_widgets": [
            "70b04fb3ab8247f082ec2c84024b8e25"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70b04fb3ab8247f082ec2c84024b8e25"
            }
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/1102166490.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===========================\n",
        "# 1. Загрузка данных\n",
        "# ===========================\n",
        "#train_tab = np.load(\"train_tab.npy\")       # табличные признаки train\n",
        "train_text = np.load(\"/kaggle/input/train-test-embedings-text-ru/train_text_embs.npy\")     # текстовые эмбеддинги train\n",
        "train_target = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep=\"\\t\")[\"target\"].values\n",
        "\n",
        "#test_tab = np.load(\"test_tab.npy\")\n",
        "test_text = np.load(\"/kaggle/input/train-test-embedings-text-ru/test_text_embs.npy\")\n",
        "test_ids = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep=\"\\t\")[\"id\"].values\n",
        "\n",
        "# ===========================\n",
        "# 2. Dataset\n",
        "# ===========================\n",
        "class POIDatasetEmb(Dataset):\n",
        "    def __init__(self, tab, text, y=None):\n",
        "        self.tab = torch.tensor(tab, dtype=torch.float32)\n",
        "        self.text = torch.tensor(text, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tab)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.tab[idx], self.text[idx]\n",
        "        return self.tab[idx], self.text[idx], self.y[idx]\n",
        "\n",
        "# Разделение на train/val\n",
        "X_train, X_val, txt_train, txt_val, y_train, y_val = train_test_split(\n",
        "    train_tab, train_text, train_target, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_ds = POIDatasetEmb(X_train, txt_train, y_train)\n",
        "val_ds = POIDatasetEmb(X_val, txt_val, y_val)\n",
        "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=256)\n",
        "\n",
        "# ===========================\n",
        "# 3. Модель\n",
        "# ===========================\n",
        "class POIModelEmb(nn.Module):\n",
        "    def __init__(self, num_tab_features, text_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        # табличные признаки\n",
        "        self.mlp_tab = nn.Sequential(\n",
        "            nn.Linear(num_tab_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        # текстовые эмбеддинги\n",
        "        self.mlp_text = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        # объединение\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim*2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tab, text):\n",
        "        tab_out = self.mlp_tab(tab)\n",
        "        text_out = self.mlp_text(text)\n",
        "        x = torch.cat([tab_out, text_out], dim=1)\n",
        "        return self.fc_out(x).squeeze(1)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = POIModelEmb(num_tab_features=train_tab.shape[1], text_dim=train_text.shape[1]).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.HuberLoss()  # устойчивый к выбросам\n",
        "\n",
        "# ===========================\n",
        "# 4. Обучение\n",
        "# ===========================\n",
        "for epoch in range(7):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for tab, text, y in train_dl:\n",
        "        tab, text, y = tab.to(device), text.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        preds = model(tab, text)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Валидация\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for tab, text, y in val_dl:\n",
        "            tab, text, y = tab.to(device), text.to(device), y.to(device)\n",
        "            preds = model(tab, text)\n",
        "            val_losses.append(criterion(preds, y).item())\n",
        "    print(f\"Epoch {epoch+1}: train={total_loss/len(train_dl):.4f}, val={np.mean(val_losses):.4f}\")\n",
        "\n",
        "# ===========================\n",
        "# 5. Предсказания\n",
        "# ===========================\n",
        "test_ds = POIDatasetEmb(test_tab, test_text)\n",
        "test_dl = DataLoader(test_ds, batch_size=256)\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for tab, text in tqdm(test_dl):\n",
        "        tab, text = tab.to(device), text.to(device)\n",
        "        out = model(tab, text)\n",
        "        preds.append(out.cpu().numpy())\n",
        "\n",
        "preds = np.clip(np.concatenate(preds), 1, 5)\n",
        "\n",
        "# ===========================\n",
        "# 6. Сохранение submission\n",
        "# ===========================\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"target\": preds\n",
        "})\n",
        "submission.to_csv(\"submission_poi_emb.csv\", index=False)\n",
        "print(\"✅ submission_poi_emb.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T12:04:49.783217Z",
          "iopub.execute_input": "2025-10-18T12:04:49.783817Z",
          "iopub.status.idle": "2025-10-18T12:05:11.945856Z",
          "shell.execute_reply.started": "2025-10-18T12:04:49.78379Z",
          "shell.execute_reply": "2025-10-18T12:05:11.944668Z"
        },
        "id": "miL0RLr1cErr",
        "outputId": "6cbfe265-0b86-4758-d1a2-958463508e2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_37/265361261.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.tab = torch.tensor(tab, dtype=torch.float32)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: train=0.4338, val=0.3533\nEpoch 2: train=0.3758, val=0.3523\nEpoch 3: train=0.3684, val=0.3405\nEpoch 4: train=0.3641, val=0.3392\nEpoch 5: train=0.3605, val=0.3393\nEpoch 6: train=0.3599, val=0.3476\nEpoch 7: train=0.3583, val=0.3335\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 37/37 [00:00<00:00, 177.31it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ submission_poi_emb.csv saved\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ============ 1. Чтение данных ============\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "reviews_df = pd.read_csv(\n",
        "    \"/kaggle/input/hshussu/reviews.txv/reviews.tsv\",\n",
        "    sep='\\t',\n",
        "    on_bad_lines='skip',\n",
        "    quoting=3,\n",
        "    engine='python'\n",
        ")\n",
        "\n",
        "# ============ 2. Подготовка текстов ============\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "reviews_df['id'] = reviews_df['id'].astype(str)\n",
        "train_df['id'] = train_df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "reviews_agg = reviews_df.groupby(\"id\")['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "\n",
        "train_df = train_df.merge(reviews_agg, on='id', how='left')\n",
        "test_df = test_df.merge(reviews_agg, on='id', how='left')\n",
        "\n",
        "train_df['text'] = train_df['text'].fillna(\"\")\n",
        "test_df['text'] = test_df['text'].fillna(\"\")\n",
        "\n",
        "# ============ 3. TF-IDF ============\n",
        "tfidf = TfidfVectorizer(max_features=512)\n",
        "train_tfidf = tfidf.fit_transform(train_df[\"text\"]).astype(np.float32)\n",
        "test_tfidf = tfidf.transform(test_df[\"text\"]).astype(np.float32)\n",
        "\n",
        "# ============ 4. Табличные признаки ============\n",
        "feature_cols = ['category', 'traffic_300m', 'traffic_1000m', 'mean_income_300m', 'mean_income_1000m']\n",
        "train_features = pd.get_dummies(train_df[feature_cols], columns=['category'])\n",
        "test_features = pd.get_dummies(test_df[feature_cols], columns=['category'])\n",
        "train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_num = scaler.fit_transform(train_features)\n",
        "test_num = scaler.transform(test_features)\n",
        "\n",
        "# ============ 5. Torch Dataset ============\n",
        "class RecDataset(Dataset):\n",
        "    def __init__(self, tfidf, num, target=None):\n",
        "        self.tfidf = torch.tensor(tfidf.toarray(), dtype=torch.float32)\n",
        "        self.num = torch.tensor(num, dtype=torch.float32)\n",
        "        self.target = torch.tensor(target, dtype=torch.float32) if target is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.num)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.target is not None:\n",
        "            return self.tfidf[idx], self.num[idx], self.target[idx]\n",
        "        else:\n",
        "            return self.tfidf[idx], self.num[idx]\n",
        "\n",
        "idx = np.arange(train_df.shape[0])\n",
        "idx_train, idx_val = train_test_split(idx, test_size=0.1, random_state=42)\n",
        "\n",
        "X_tfidf_train = train_tfidf[idx_train]\n",
        "X_tfidf_val = train_tfidf[idx_val]\n",
        "X_num_train = train_num[idx_train]\n",
        "X_num_val = train_num[idx_val]\n",
        "y_train = train_df['target'].values[idx_train]\n",
        "y_val = train_df['target'].values[idx_val]\n",
        "\n",
        "train_dataset = RecDataset(X_tfidf_train, X_num_train, y_train)\n",
        "val_dataset = RecDataset(X_tfidf_val, X_num_val, y_val)\n",
        "test_dataset = RecDataset(test_tfidf, test_num)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# ============ 6. Модель Encoder–Decoder Transformer ============\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim=128, num_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(input_dim, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).unsqueeze(1)  # [B, 1, embed_dim]\n",
        "        return self.encoder(x)         # [B, 1, embed_dim]\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim=128, num_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(input_dim, embed_dim)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        tgt = self.proj(tgt).unsqueeze(1)  # [B, 1, embed_dim]\n",
        "        return self.decoder(tgt, memory)   # [B, 1, embed_dim]\n",
        "\n",
        "class HybridTransformerED(nn.Module):\n",
        "    def __init__(self, text_dim, num_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(text_dim, embed_dim=hidden)\n",
        "        self.decoder = TransformerDecoder(num_dim, embed_dim=hidden)\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, text, num):\n",
        "        memory = self.encoder(text)       # encoded text info\n",
        "        out = self.decoder(num, memory)   # conditioned on text\n",
        "        return self.fc_out(out.squeeze(1)).squeeze(1)\n",
        "\n",
        "# ============ 7. Обучение ============\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HybridTransformerED(text_dim=train_tfidf.shape[1], num_dim=train_num.shape[1]).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.L1Loss()  # MAE\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, num, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n",
        "        text, num, target = text.to(device), num.to(device), target.to(device)\n",
        "        pred = model(text, num)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Train loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Валидация\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text, num, target in val_loader:\n",
        "            text, num, target = text.to(device), num.to(device), target.to(device)\n",
        "            pred = model(text, num)\n",
        "            val_loss += criterion(pred, target).item()\n",
        "    print(f\"Val loss: {val_loss / len(val_loader):.4f}\")\n",
        "\n",
        "# ============ 8. Предсказание ============\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for text, num in DataLoader(test_dataset, batch_size=128):\n",
        "        text, num = text.to(device), num.to(device)\n",
        "        out = model(text, num)\n",
        "        preds.extend(out.cpu().numpy())\n",
        "\n",
        "test_df['target'] = np.clip(preds, 1, 5)\n",
        "test_df[['id', 'target']].to_csv(\"submission_transformer_encdec.csv\", index=False)\n",
        "print(\"✅ submission_transformer_encdec.csv saved\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-19T11:15:21.056875Z",
          "iopub.execute_input": "2025-10-19T11:15:21.057306Z",
          "iopub.status.idle": "2025-10-19T11:24:26.374216Z",
          "shell.execute_reply.started": "2025-10-19T11:15:21.057272Z",
          "shell.execute_reply": "2025-10-19T11:24:26.373224Z"
        },
        "id": "9w-nXkXGcErs",
        "outputId": "fe5745b4-fb48-4ddf-9a49-8ef38a142511"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/10: 100%|██████████| 579/579 [00:26<00:00, 21.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.6634\nVal loss: 0.7386\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/10: 100%|██████████| 579/579 [00:25<00:00, 22.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5799\nVal loss: 0.6197\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/10: 100%|██████████| 579/579 [00:26<00:00, 22.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.6396\nVal loss: 0.6840\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/10: 100%|██████████| 579/579 [00:25<00:00, 22.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.6349\nVal loss: 0.6621\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/10: 100%|██████████| 579/579 [00:25<00:00, 22.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5577\nVal loss: 0.5401\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/10: 100%|██████████| 579/579 [00:26<00:00, 21.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5268\nVal loss: 0.5974\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/10: 100%|██████████| 579/579 [00:25<00:00, 22.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5112\nVal loss: 0.5243\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/10: 100%|██████████| 579/579 [00:25<00:00, 22.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5038\nVal loss: 0.5979\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/10: 100%|██████████| 579/579 [00:25<00:00, 22.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5041\nVal loss: 0.6139\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/10: 100%|██████████| 579/579 [00:25<00:00, 22.56it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5264\nVal loss: 0.5376\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 11/10: 100%|██████████| 579/579 [00:26<00:00, 21.85it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5235\nVal loss: 0.5565\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 12/10: 100%|██████████| 579/579 [00:25<00:00, 22.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5069\nVal loss: 0.5371\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 13/10: 100%|██████████| 579/579 [00:25<00:00, 22.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5028\nVal loss: 0.5070\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 14/10: 100%|██████████| 579/579 [00:25<00:00, 22.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5049\nVal loss: 0.5272\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 15/10: 100%|██████████| 579/579 [00:25<00:00, 22.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5039\nVal loss: 0.5791\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 16/10: 100%|██████████| 579/579 [00:25<00:00, 22.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4987\nVal loss: 0.5228\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 17/10: 100%|██████████| 579/579 [00:25<00:00, 22.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4992\nVal loss: 0.5367\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 18/10: 100%|██████████| 579/579 [00:25<00:00, 22.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5025\nVal loss: 0.5512\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 19/10: 100%|██████████| 579/579 [00:26<00:00, 22.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5080\nVal loss: 0.5073\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 20/10: 100%|██████████| 579/579 [00:25<00:00, 22.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5001\nVal loss: 0.5757\n✅ submission_transformer_encdec.csv saved\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ============ 1. Чтение данных ============\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "reviews_df = pd.read_csv(\"/kaggle/input/hshussu/reviews.txv/reviews.tsv\", sep='\\t', on_bad_lines='skip', quoting=3, engine='python')\n",
        "\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "for df in [train_df, test_df]:\n",
        "    df['id'] = df['id'].astype(str)\n",
        "reviews_df['id'] = reviews_df['id'].astype(str)\n",
        "\n",
        "reviews_agg = reviews_df.groupby(\"id\")['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "train_df = train_df.merge(reviews_agg, on='id', how='left')\n",
        "test_df = test_df.merge(reviews_agg, on='id', how='left')\n",
        "\n",
        "train_df['text'] = train_df['text'].fillna(\"\")\n",
        "test_df['text'] = test_df['text'].fillna(\"\")\n",
        "\n",
        "# ============ 2. TF-IDF ============\n",
        "tfidf = TfidfVectorizer(max_features=512)\n",
        "train_tfidf = tfidf.fit_transform(train_df[\"text\"]).astype(np.float32)\n",
        "test_tfidf = tfidf.transform(test_df[\"text\"]).astype(np.float32)\n",
        "\n",
        "# ============ 3. Табличные признаки ============\n",
        "feature_cols = ['category', 'traffic_300m', 'traffic_1000m', 'mean_income_300m', 'mean_income_1000m']\n",
        "train_features = pd.get_dummies(train_df[feature_cols], columns=['category'])\n",
        "test_features = pd.get_dummies(test_df[feature_cols], columns=['category'])\n",
        "train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_num = scaler.fit_transform(train_features)\n",
        "test_num = scaler.transform(test_features)\n",
        "\n",
        "# ============ 4. Torch Dataset ============\n",
        "class RecDataset(Dataset):\n",
        "    def __init__(self, tfidf, num, target=None):\n",
        "        self.tfidf = torch.tensor(tfidf.toarray(), dtype=torch.float32)\n",
        "        self.num = torch.tensor(num, dtype=torch.float32)\n",
        "        self.target = torch.tensor(target, dtype=torch.float32) if target is not None else None\n",
        "\n",
        "    def __len__(self): return len(self.num)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.target is not None:\n",
        "            return self.tfidf[idx], self.num[idx], self.target[idx]\n",
        "        else:\n",
        "            return self.tfidf[idx], self.num[idx]\n",
        "\n",
        "idx = np.arange(train_df.shape[0])\n",
        "idx_train, idx_val = train_test_split(idx, test_size=0.1, random_state=42)\n",
        "train_dataset = RecDataset(train_tfidf[idx_train], train_num[idx_train], train_df['target'].values[idx_train])\n",
        "val_dataset = RecDataset(train_tfidf[idx_val], train_num[idx_val], train_df['target'].values[idx_val])\n",
        "test_dataset = RecDataset(test_tfidf, test_num)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# ============ 5. Transformer Decoder Only ============\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, text_dim, num_dim, hidden=128, num_heads=4, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.text_proj = nn.Linear(text_dim, hidden)\n",
        "        self.num_proj = nn.Linear(num_dim, hidden)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden, nhead=num_heads, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, text, num):\n",
        "        text = self.text_proj(text).unsqueeze(1)\n",
        "        tgt = self.num_proj(num).unsqueeze(1)\n",
        "        decoded = self.decoder(tgt, text)\n",
        "        return self.fc_out(decoded.squeeze(1)).squeeze(1)\n",
        "\n",
        "# ============ 6. Обучение ============\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DecoderOnlyTransformer(train_tfidf.shape[1], train_num.shape[1]).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, num, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n",
        "        text, num, target = text.to(device), num.to(device), target.to(device)\n",
        "        pred = model(text, num)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Train loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ============ 7. Предсказания ============\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for text, num in DataLoader(test_dataset, batch_size=128):\n",
        "        text, num = text.to(device), num.to(device)\n",
        "        preds.extend(model(text, num).cpu().numpy())\n",
        "\n",
        "test_df['target'] = np.clip(preds, 1, 5)\n",
        "test_df[['id', 'target']].to_csv(\"submission_decoder_only.csv\", index=False)\n",
        "print(\"✅ Saved submission_decoder_only.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-19T11:57:11.061481Z",
          "iopub.execute_input": "2025-10-19T11:57:11.061823Z",
          "iopub.status.idle": "2025-10-19T12:03:22.48856Z",
          "shell.execute_reply.started": "2025-10-19T11:57:11.061795Z",
          "shell.execute_reply": "2025-10-19T12:03:22.487467Z"
        },
        "id": "vj9CFBfNcErs",
        "outputId": "41c2e24f-823d-4d73-98c7-fcb93ae3a234"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/10: 100%|██████████| 579/579 [00:23<00:00, 25.03it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5938\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/10: 100%|██████████| 579/579 [00:23<00:00, 24.73it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5018\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/10: 100%|██████████| 579/579 [00:23<00:00, 24.56it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5028\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/10: 100%|██████████| 579/579 [00:23<00:00, 25.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4960\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/10: 100%|██████████| 579/579 [00:23<00:00, 25.03it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4945\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/10: 100%|██████████| 579/579 [00:24<00:00, 23.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4905\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/10: 100%|██████████| 579/579 [00:23<00:00, 24.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4925\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/10: 100%|██████████| 579/579 [00:23<00:00, 24.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4892\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/10: 100%|██████████| 579/579 [00:23<00:00, 24.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4853\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/10: 100%|██████████| 579/579 [00:23<00:00, 24.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4865\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 11/10: 100%|██████████| 579/579 [00:24<00:00, 23.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4855\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 12/10: 100%|██████████| 579/579 [00:23<00:00, 24.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4866\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 13/10: 100%|██████████| 579/579 [00:23<00:00, 24.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4845\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 14/10: 100%|██████████| 579/579 [00:23<00:00, 24.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4866\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 15/10: 100%|██████████| 579/579 [00:23<00:00, 24.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4826\n✅ Saved submission_decoder_only.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ============ Данные ============\n",
        "train_df = pd.read_csv(\"/kaggle/input/hshussu/train.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(\"/kaggle/input/hshussu/test.tsv\", sep='\\t')\n",
        "reviews_df = pd.read_csv(\"/kaggle/input/hshussu/reviews.txv/reviews.tsv\", sep='\\t', on_bad_lines='skip', quoting=3, engine='python')\n",
        "\n",
        "reviews_df['text'] = reviews_df['text'].fillna(\"\")\n",
        "for df in [train_df, test_df]:\n",
        "    df['id'] = df['id'].astype(str)\n",
        "reviews_df['id'] = reviews_df['id'].astype(str)\n",
        "\n",
        "reviews_agg = reviews_df.groupby(\"id\")['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "train_df = train_df.merge(reviews_agg, on='id', how='left')\n",
        "test_df = test_df.merge(reviews_agg, on='id', how='left')\n",
        "\n",
        "train_df['text'] = train_df['text'].fillna(\"\")\n",
        "test_df['text'] = test_df['text'].fillna(\"\")\n",
        "\n",
        "# ============ TF-IDF + табличные ============\n",
        "tfidf = TfidfVectorizer(max_features=512)\n",
        "train_tfidf = tfidf.fit_transform(train_df[\"text\"]).astype(np.float32)\n",
        "test_tfidf = tfidf.transform(test_df[\"text\"]).astype(np.float32)\n",
        "\n",
        "feature_cols = ['category', 'traffic_300m', 'traffic_1000m', 'mean_income_300m', 'mean_income_1000m']\n",
        "train_features = pd.get_dummies(train_df[feature_cols], columns=['category'])\n",
        "test_features = pd.get_dummies(test_df[feature_cols], columns=['category'])\n",
        "train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_num = scaler.fit_transform(train_features)\n",
        "test_num = scaler.transform(test_features)\n",
        "\n",
        "# ============ Dataset ============\n",
        "class RecDataset(Dataset):\n",
        "    def __init__(self, tfidf, num, target=None):\n",
        "        self.tfidf = torch.tensor(tfidf.toarray(), dtype=torch.float32)\n",
        "        self.num = torch.tensor(num, dtype=torch.float32)\n",
        "        self.target = torch.tensor(target, dtype=torch.float32) if target is not None else None\n",
        "\n",
        "    def __len__(self): return len(self.num)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.target is not None:\n",
        "            return self.tfidf[idx], self.num[idx], self.target[idx]\n",
        "        else:\n",
        "            return self.tfidf[idx], self.num[idx]\n",
        "\n",
        "idx = np.arange(train_df.shape[0])\n",
        "idx_train, idx_val = train_test_split(idx, test_size=0.1, random_state=42)\n",
        "train_dataset = RecDataset(train_tfidf[idx_train], train_num[idx_train], train_df['target'].values[idx_train])\n",
        "val_dataset = RecDataset(train_tfidf[idx_val], train_num[idx_val], train_df['target'].values[idx_val])\n",
        "test_dataset = RecDataset(test_tfidf, test_num)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# ============ SASRec модель ============\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, text_dim, num_dim, hidden=128, num_heads=4, num_layers=2, max_len=2):\n",
        "        super().__init__()\n",
        "        self.text_proj = nn.Linear(text_dim, hidden)\n",
        "        self.num_proj = nn.Linear(num_dim, hidden)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(max_len, hidden))\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=hidden, nhead=num_heads, batch_first=True)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, text, num):\n",
        "        text_emb = self.text_proj(text)\n",
        "        num_emb = self.num_proj(num)\n",
        "        seq = torch.stack([text_emb, num_emb], dim=1) + self.pos_emb[:2]\n",
        "        for layer in self.layers:\n",
        "            seq = layer(seq)\n",
        "        pooled = seq.mean(dim=1)\n",
        "        return self.fc_out(pooled).squeeze(1)\n",
        "\n",
        "# ============ Обучение ============\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SASRec(train_tfidf.shape[1], train_num.shape[1]).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "for epoch in range(25):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, num, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n",
        "        text, num, target = text.to(device), num.to(device), target.to(device)\n",
        "        pred = model(text, num)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Train loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# ============ Предсказание ============\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for text, num in DataLoader(test_dataset, batch_size=128):\n",
        "        text, num = text.to(device), num.to(device)\n",
        "        preds.extend(model(text, num).cpu().numpy())\n",
        "\n",
        "test_df['target'] = np.clip(preds, 1, 5)\n",
        "test_df[['id', 'target']].to_csv(\"submission_sasrec.csv\", index=False)\n",
        "print(\"✅ Saved submission_sasrec.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-19T12:38:20.707974Z",
          "iopub.execute_input": "2025-10-19T12:38:20.7083Z",
          "iopub.status.idle": "2025-10-19T12:46:30.178079Z",
          "shell.execute_reply.started": "2025-10-19T12:38:20.708275Z",
          "shell.execute_reply": "2025-10-19T12:46:30.177145Z"
        },
        "id": "qZ6jicpWcErs",
        "outputId": "aa0cc581-2bf4-4c26-a485-3800dbeb9468"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/10: 100%|██████████| 579/579 [00:16<00:00, 35.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.6189\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/10: 100%|██████████| 579/579 [00:16<00:00, 34.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5104\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/10: 100%|██████████| 579/579 [00:16<00:00, 34.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5001\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/10: 100%|██████████| 579/579 [00:16<00:00, 34.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.5050\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/10: 100%|██████████| 579/579 [00:17<00:00, 33.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4963\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/10: 100%|██████████| 579/579 [00:17<00:00, 32.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4811\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/10: 100%|██████████| 579/579 [00:17<00:00, 32.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4762\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/10: 100%|██████████| 579/579 [00:18<00:00, 31.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4698\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/10: 100%|██████████| 579/579 [00:18<00:00, 30.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4682\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/10: 100%|██████████| 579/579 [00:19<00:00, 30.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4825\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 11/10: 100%|██████████| 579/579 [00:19<00:00, 29.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4718\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 12/10: 100%|██████████| 579/579 [00:19<00:00, 29.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4640\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 13/10: 100%|██████████| 579/579 [00:19<00:00, 28.98it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4609\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 14/10: 100%|██████████| 579/579 [00:19<00:00, 29.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4830\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 15/10: 100%|██████████| 579/579 [00:19<00:00, 29.54it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4657\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 16/10: 100%|██████████| 579/579 [00:19<00:00, 30.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4663\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 17/10: 100%|██████████| 579/579 [00:20<00:00, 28.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4644\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 18/10: 100%|██████████| 579/579 [00:20<00:00, 28.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4677\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 19/10: 100%|██████████| 579/579 [00:19<00:00, 29.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4666\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 20/10: 100%|██████████| 579/579 [00:19<00:00, 30.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4667\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 21/10: 100%|██████████| 579/579 [00:19<00:00, 29.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4639\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 22/10: 100%|██████████| 579/579 [00:19<00:00, 29.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4649\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 23/10: 100%|██████████| 579/579 [00:19<00:00, 29.08it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4609\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 24/10: 100%|██████████| 579/579 [00:19<00:00, 29.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4623\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 25/10: 100%|██████████| 579/579 [00:19<00:00, 29.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Train loss: 0.4581\n✅ Saved submission_sasrec.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}