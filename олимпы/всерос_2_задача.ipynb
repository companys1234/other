{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSbeab3nIr54"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "End-to-end pipeline to train a model that predicts car prices from images + tabular metadata.\n",
        "Save predictions (id, true_price, pred_price) to a CSV.\n",
        "\n",
        "How to use:\n",
        "    python car_price_pipeline.py \\\n",
        "        --parquet data/cars.parquet \\\n",
        "        --images_dir data/images/ \\\n",
        "        --output_dir outputs/ \\\n",
        "        --image_ext .jpg \\\n",
        "        --epochs 10\n",
        "\n",
        "The script expects the parquet file to contain at least an `id` column and a `price` column\n",
        "(or you can provide a test set where `price` is missing / NaN).\n",
        "If images are named by id (e.g. <id>.jpg) the script will find them in images_dir.\n",
        "If your parquet already contains an `image_path` column, it will use that.\n",
        "\n",
        "The model: pretrained ResNet backbone (from torchvision) -> embedding -> concat tabular features -> MLP head -> single regression output.\n",
        "Tabular preprocessing uses sklearn ColumnTransformer (StandardScaler for numeric, OneHotEncoder for categorical).\n",
        "\n",
        "Outputs:\n",
        " - best model saved to output_dir/best_model.pth\n",
        " - last model to output_dir/last_model.pth\n",
        " - predictions to output_dir/predictions.csv\n",
        " - training log to output_dir/train_log.csv\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ----------------------------- Utilities ---------------------------------\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# ----------------------------- Dataset ----------------------------------\n",
        "class CarPriceDataset(Dataset):\n",
        "    def __init__(self, df, images_dir, image_ext='.jpg', tabular_transform=None, image_transform=None, id_col='id', price_col='price'):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.image_ext = image_ext\n",
        "        self.tabular_transform = tabular_transform\n",
        "        self.image_transform = image_transform\n",
        "        self.id_col = id_col\n",
        "        self.price_col = price_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _get_image_path(self, row):\n",
        "        # prefer explicit image path if present\n",
        "        if 'image_path' in row and pd.notna(row['image_path']):\n",
        "            return Path(row['image_path'])\n",
        "        # otherwise look up by id\n",
        "        img_name = f\"{row[self.id_col]}{self.image_ext}\"\n",
        "        return self.images_dir / img_name\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = self._get_image_path(row)\n",
        "        # load image\n",
        "        from PIL import Image\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            # If image missing, create a black image to avoid crash (but log may be needed)\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        if self.image_transform:\n",
        "            img = self.image_transform(img)\n",
        "        # tabular\n",
        "        tab = None\n",
        "        if self.tabular_transform is not None:\n",
        "            # tabular_transform expects dataframe-like; we will pass a single-row DataFrame\n",
        "            # but sklearn transformers expect 2D arrays => pass a row as DataFrame and then ravel\n",
        "            Xtab = self.tabular_transform.transform(self.df.drop(columns=[self.id_col, self.price_col], errors='ignore'))\n",
        "            # Note: computing transform for whole df per item is inefficient. We'll handle tabular data precomputed outside.\n",
        "            pass\n",
        "        sample = {\n",
        "            'id': row[self.id_col],\n",
        "            'image': img\n",
        "        }\n",
        "        # price if exists\n",
        "        if self.price_col in self.df.columns and pd.notna(row[self.price_col]):\n",
        "            sample['price'] = float(row[self.price_col])\n",
        "        return sample\n",
        "\n",
        "# We'll implement a more efficient dataset that takes precomputed tabular numpy array\n",
        "class CarPriceDatasetFast(Dataset):\n",
        "    def __init__(self, df, tabular_array, images_dir, image_ext='.jpg', image_transform=None, id_col='id', price_col='price'):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tabular = tabular_array\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.image_ext = image_ext\n",
        "        self.image_transform = image_transform\n",
        "        self.id_col = id_col\n",
        "        self.price_col = price_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _get_image_path(self, row):\n",
        "        if 'image_path' in row and pd.notna(row['image_path']):\n",
        "            return Path(row['image_path'])\n",
        "        img_name = f\"{row[self.id_col]}{self.image_ext}\"\n",
        "        return self.images_dir / img_name\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = self._get_image_path(row)\n",
        "        from PIL import Image\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        if self.image_transform:\n",
        "            img = self.image_transform(img)\n",
        "        tab = self.tabular[idx].astype(np.float32) if self.tabular is not None else np.array([], dtype=np.float32)\n",
        "        sample = {\n",
        "            'id': row[self.id_col],\n",
        "            'image': img,\n",
        "            'tabular': tab\n",
        "        }\n",
        "        if self.price_col in self.df.columns and pd.notna(row[self.price_col]):\n",
        "            sample['price'] = float(row[self.price_col])\n",
        "        return sample\n",
        "\n",
        "# ----------------------------- Model ------------------------------------\n",
        "class ImageTabularRegressor(nn.Module):\n",
        "    def __init__(self, backbone_name='resnet18', pretrained=True, tab_dim=0, embed_dim=512, head_hidden=[256, 64], dropout=0.2):\n",
        "        super().__init__()\n",
        "        # load backbone\n",
        "        if backbone_name.startswith('resnet'):\n",
        "            model = getattr(models, backbone_name)(pretrained=pretrained)\n",
        "            # remove last fc\n",
        "            in_features = model.fc.in_features\n",
        "            modules = list(model.children())[:-1]  # remove fc\n",
        "            self.backbone = nn.Sequential(*modules)\n",
        "            self.backbone_embed_dim = in_features\n",
        "        else:\n",
        "            raise NotImplementedError('Only resnet* backbones implemented in this example')\n",
        "\n",
        "        # projection from backbone output to embed_dim\n",
        "        self.img_proj = nn.Linear(self.backbone_embed_dim, embed_dim)\n",
        "        self.tab_dim = tab_dim\n",
        "        # head\n",
        "        head_in = embed_dim + tab_dim\n",
        "        layers = []\n",
        "        prev = head_in\n",
        "        for h in head_hidden:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev = h\n",
        "        layers.append(nn.Linear(prev, 1))\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, images, tabular=None):\n",
        "        # images: tensor Bx3xHxW\n",
        "        x = self.backbone(images)  # B x C x 1 x 1\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.img_proj(x)\n",
        "        if tabular is not None:\n",
        "            x = torch.cat([x, tabular], dim=1)\n",
        "        out = self.head(x).squeeze(1)\n",
        "        return out\n",
        "\n",
        "# ----------------------------- Training ---------------------------------\n",
        "\n",
        "def get_transforms(train=True, size=224):\n",
        "    if train:\n",
        "        return T.Compose([\n",
        "            T.Resize((size, size)),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomApply([T.ColorJitter(0.2,0.2,0.2,0.02)], p=0.5),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return T.Compose([\n",
        "            T.Resize((size, size)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for batch in loader:\n",
        "        images = batch['image'].to(device)\n",
        "        tabs = batch['tabular'].to(device) if 'tabular' in batch else None\n",
        "        prices = batch.get('price', None)\n",
        "        if prices is None:\n",
        "            continue\n",
        "        prices = prices.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                preds = model(images, tabs)\n",
        "                loss = criterion(preds, prices)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            preds = model(images, tabs)\n",
        "            loss = criterion(preds, prices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        count += images.size(0)\n",
        "    return running_loss / max(1, count)\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    preds_list = []\n",
        "    ids = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            images = batch['image'].to(device)\n",
        "            tabs = batch['tabular'].to(device) if 'tabular' in batch else None\n",
        "            prices = batch.get('price', None)\n",
        "            ids.extend(batch['id'])\n",
        "            if prices is not None:\n",
        "                prices = prices.to(device)\n",
        "                trues.extend(prices.cpu().numpy().tolist())\n",
        "            pred = model(images, tabs)\n",
        "            preds_list.extend(pred.detach().cpu().numpy().tolist())\n",
        "            if prices is not None:\n",
        "                running_loss += ((pred - prices)**2).sum().item()\n",
        "                count += images.size(0)\n",
        "    mse = running_loss / max(1, count)\n",
        "    rmse = math.sqrt(mse) if count>0 else None\n",
        "    return rmse, ids, trues, preds_list\n",
        "\n",
        "# ----------------------------- Main pipeline -----------------------------\n",
        "\n",
        "def main(args):\n",
        "    seed_everything(args.seed)\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    # 1) read parquet\n",
        "    df = pd.read_parquet(args.parquet)\n",
        "    if 'id' not in df.columns:\n",
        "        raise RuntimeError('Parquet must contain an `id` column')\n",
        "\n",
        "    # detect price column\n",
        "    price_col = args.price_col\n",
        "    if price_col not in df.columns:\n",
        "        print(f\"Price column `{price_col}` not found in parquet. Proceeding as test-only (no targets).\")\n",
        "\n",
        "    # split train/val if prices exist\n",
        "    has_price = price_col in df.columns and df[price_col].notna().any()\n",
        "    if has_price:\n",
        "        train_df, val_df = train_test_split(df[df[price_col].notna()], test_size=args.val_size, random_state=args.seed)\n",
        "        # If there are rows without price, treat them as test\n",
        "        test_df = df[df[price_col].isna()]\n",
        "    else:\n",
        "        train_df = pd.DataFrame(columns=df.columns)\n",
        "        val_df = pd.DataFrame(columns=df.columns)\n",
        "        test_df = df.copy()\n",
        "\n",
        "    # 2) prepare tabular features\n",
        "    # choose tabular columns = all except id, price, image_path\n",
        "    ignore_cols = set([args.id_col, price_col, 'image_path'])\n",
        "    tab_cols = [c for c in df.columns if c not in ignore_cols]\n",
        "    print('Tabular columns used:', tab_cols)\n",
        "\n",
        "    # Build transformer\n",
        "    numeric_cols = [c for c in tab_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    cat_cols = [c for c in tab_cols if c not in numeric_cols]\n",
        "\n",
        "    transformers = []\n",
        "    if numeric_cols:\n",
        "        transformers.append(('num', StandardScaler(), numeric_cols))\n",
        "    if cat_cols:\n",
        "        transformers.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols))\n",
        "\n",
        "    if transformers:\n",
        "        col_transformer = ColumnTransformer(transformers)\n",
        "        # fit on train + val + test to avoid unseen categories issues? fit only on train recommended\n",
        "        if len(train_df)>0:\n",
        "            col_transformer.fit(train_df[tab_cols].fillna(''))\n",
        "        else:\n",
        "            col_transformer.fit(df[tab_cols].fillna(''))\n",
        "        # transform datasets\n",
        "        def make_tab_array(df_part):\n",
        "            if len(tab_cols)==0:\n",
        "                return np.zeros((len(df_part),0), dtype=np.float32)\n",
        "            X = df_part[tab_cols].fillna('')\n",
        "            arr = col_transformer.transform(X)\n",
        "            return arr.astype(np.float32)\n",
        "        train_tab = make_tab_array(train_df)\n",
        "        val_tab = make_tab_array(val_df)\n",
        "        test_tab = make_tab_array(test_df)\n",
        "        tab_dim = train_tab.shape[1]\n",
        "    else:\n",
        "        # no tabular features\n",
        "        train_tab = np.zeros((len(train_df),0), dtype=np.float32)\n",
        "        val_tab = np.zeros((len(val_df),0), dtype=np.float32)\n",
        "        test_tab = np.zeros((len(test_df),0), dtype=np.float32)\n",
        "        tab_dim = 0\n",
        "\n",
        "    # 3) Transforms and Datasets\n",
        "    train_transform = get_transforms(train=True, size=args.img_size)\n",
        "    val_transform = get_transforms(train=False, size=args.img_size)\n",
        "\n",
        "    train_ds = CarPriceDatasetFast(train_df, train_tab, args.images_dir, image_ext=args.image_ext, image_transform=train_transform, id_col=args.id_col, price_col=price_col)\n",
        "    val_ds = CarPriceDatasetFast(val_df, val_tab, args.images_dir, image_ext=args.image_ext, image_transform=val_transform, id_col=args.id_col, price_col=price_col)\n",
        "    test_ds = CarPriceDatasetFast(test_df, test_tab, args.images_dir, image_ext=args.image_ext, image_transform=val_transform, id_col=args.id_col, price_col=price_col)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "    # 4) Model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and not args.use_cpu else 'cpu')\n",
        "    model = ImageTabularRegressor(backbone_name=args.backbone, pretrained=args.pretrained, tab_dim=tab_dim, embed_dim=args.embed_dim, head_hidden=args.head_hidden, dropout=args.dropout)\n",
        "    model.to(device)\n",
        "\n",
        "    # 5) Loss, optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
        "    scaler = torch.cuda.amp.GradScaler() if (device.type=='cuda' and args.use_amp) else None\n",
        "\n",
        "    # 6) Training loop\n",
        "    best_rmse = float('inf')\n",
        "    history = []\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        start = time.time()\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler=scaler)\n",
        "        val_rmse, _, _, _ = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_rmse if val_rmse is not None else train_loss)\n",
        "        elapsed = time.time()-start\n",
        "        print(f\"Epoch {epoch}/{args.epochs} — train_loss: {train_loss:.4f} — val_rmse: {val_rmse} — time: {elapsed:.1f}s\")\n",
        "        history.append({'epoch': epoch, 'train_loss': train_loss, 'val_rmse': val_rmse})\n",
        "        # checkpoint\n",
        "        torch.save(model.state_dict(), os.path.join(args.output_dir, 'last_model.pth'))\n",
        "        if val_rmse is not None and val_rmse < best_rmse:\n",
        "            best_rmse = val_rmse\n",
        "            torch.save(model.state_dict(), os.path.join(args.output_dir, 'best_model.pth'))\n",
        "\n",
        "    # save training log\n",
        "    pd.DataFrame(history).to_csv(os.path.join(args.output_dir, 'train_log.csv'), index=False)\n",
        "\n",
        "    # 7) Inference on test + val (we'll run on all available rows to produce predictions)\n",
        "    model.eval()\n",
        "    def run_inference(loader):\n",
        "        ids = []\n",
        "        preds = []\n",
        "        trues = []\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                images = batch['image'].to(device)\n",
        "                tabs = batch['tabular'].to(device) if 'tabular' in batch else None\n",
        "                out = model(images, tabs)\n",
        "                preds.extend(out.detach().cpu().numpy().tolist())\n",
        "                ids.extend(batch['id'])\n",
        "                if 'price' in batch:\n",
        "                    trues.extend(batch['price'])\n",
        "                else:\n",
        "                    trues.extend([None]*len(batch['id']))\n",
        "        return ids, trues, preds\n",
        "\n",
        "    all_ids = []\n",
        "    all_trues = []\n",
        "    all_preds = []\n",
        "    # val\n",
        "    if len(val_ds)>0:\n",
        "        ids, trues, preds = run_inference(val_loader)\n",
        "        all_ids.extend(ids); all_trues.extend(trues); all_preds.extend(preds)\n",
        "    # test\n",
        "    if len(test_ds)>0:\n",
        "        ids, trues, preds = run_inference(test_loader)\n",
        "        all_ids.extend(ids); all_trues.extend(trues); all_preds.extend(preds)\n",
        "\n",
        "    out_df = pd.DataFrame({'id': all_ids, 'true_price': all_trues, 'pred_price': all_preds})\n",
        "    out_csv = os.path.join(args.output_dir, 'predictions.csv')\n",
        "    out_df.to_csv(out_csv, index=False)\n",
        "    print('Saved predictions to', out_csv)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--parquet', type=str, required=True)\n",
        "    parser.add_argument('--images_dir', type=str, required=True)\n",
        "    parser.add_argument('--output_dir', type=str, default='outputs')\n",
        "    parser.add_argument('--image_ext', type=str, default='.jpg')\n",
        "    parser.add_argument('--id_col', type=str, default='id')\n",
        "    parser.add_argument('--price_col', type=str, default='price')\n",
        "    parser.add_argument('--val_size', type=float, default=0.15)\n",
        "    parser.add_argument('--img_size', type=int, default=224)\n",
        "    parser.add_argument('--batch_size', type=int, default=32)\n",
        "    parser.add_argument('--epochs', type=int, default=10)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-5)\n",
        "    parser.add_argument('--num_workers', type=int, default=4)\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--backbone', type=str, default='resnet18')\n",
        "    parser.add_argument('--pretrained', action='store_true')\n",
        "    parser.add_argument('--embed_dim', type=int, default=512)\n",
        "    parser.add_argument('--head_hidden', nargs='+', type=int, default=[256,64])\n",
        "    parser.add_argument('--dropout', type=float, default=0.2)\n",
        "    parser.add_argument('--use_amp', action='store_true')\n",
        "    parser.add_argument('--use_cpu', action='store_true')\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ]
    }
  ]
}