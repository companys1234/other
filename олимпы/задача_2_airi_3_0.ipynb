{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRSfSRXtanJE"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy  as np\n",
        "from pathlib import Path\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "TARGET_COLUMN_NAMES = ['wp_r', 'wp_g', 'wp_b']\n",
        "\n",
        "HIST_BINS = [116, 100]\n",
        "HIST_RANGE_FLAT = [-sqrt(3), sqrt(3), -1, 2] #value ranges for beta and alpha\n",
        "HIST_TARGET_SIZE = [128, 128]\n",
        "HIST_VERT_PADD, HIST_HORR_PADD = 6, 14\n",
        "\n",
        "WHITE_LEVEL = 2**12 - 1 - 256\n",
        "\n",
        "def read_hist(path2hist: Path):\n",
        "    hist = cv2.imread(str(path2hist), cv2.IMREAD_UNCHANGED)\n",
        "    hist = hist[HIST_HORR_PADD:-HIST_HORR_PADD,\n",
        "                HIST_VERT_PADD:-HIST_VERT_PADD]\n",
        "    hist = hist.astype(np.float32) / 255\n",
        "    return hist.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "\n",
        "this_dir = Path('/content/sasd').parent\n",
        "\n",
        "\n",
        "\n",
        "def read_image(path2img: Path | str,\n",
        "           white_level_corr: bool = True):\n",
        "    img = cv2.imread(str(path2img), cv2.IMREAD_UNCHANGED)\n",
        "    if white_level_corr:\n",
        "        img = img / WHITE_LEVEL\n",
        "    if img.shape[-1] == 3:\n",
        "        img = img[..., ::-1]\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_hists_dir(part: Literal['train', 'test'] = 'train'):\n",
        "    return this_dir / f'{part}_histograms'\n",
        "\n",
        "\n",
        "def get_imgs_dir(part: Literal['train', 'test'] = 'train'):\n",
        "    return this_dir / f'{part}_imgs'\n",
        "\n",
        "\n",
        "def get_historgam_by_name(img_name: str,\n",
        "                          part: Literal['train', 'test'] = 'train'):\n",
        "    hists_dir = get_hists_dir(part)\n",
        "    hist = read_hist((hists_dir / img_name).with_suffix('.png'))\n",
        "    return hist\n",
        "\n",
        "\n",
        "class IllumDataset:\n",
        "    def __init__(self,\n",
        "                 part: Literal['train', 'test'] = 'train'):\n",
        "        self.part = part\n",
        "        self._init_paths()\n",
        "        self._init_white_points()\n",
        "\n",
        "    def _init_paths(self):\n",
        "        self.imgs_paths = sorted(get_imgs_dir(self.part).glob('*.png'))\n",
        "        self.hists_paths = sorted(get_hists_dir(self.part).glob('*.png'))\n",
        "\n",
        "    def _init_white_points(self):\n",
        "        df = pd.read_csv(this_dir / f'{self.part}.csv', converters={'white_points': literal_eval})\n",
        "        self.white_points = df[['wp_r', 'wp_g', 'wp_b']].values\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return read_image(self.imgs_paths[idx]), read_hist(self.hists_paths[idx]), self.white_points[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "\n",
        "class AWBDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset: IllumDataset,\n",
        "                 ids: list,\n",
        "                 require_transform: bool = False,\n",
        "                 preload: bool = False):\n",
        "        self.imgs_paths   = [elem for i, elem in enumerate(dataset.imgs_paths) if i in ids]\n",
        "        self.hists_paths  = [elem for i, elem in enumerate(dataset.hists_paths) if i in ids]\n",
        "        self.white_points = [elem for i, elem in enumerate(dataset.white_points) if i in ids]\n",
        "        self.preload = preload\n",
        "        if require_transform:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.RandomCrop(256)\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        if self.preload:\n",
        "            self.imgs = [read_image(elem) for elem in tqdm(self.imgs_paths, desc=\"Preloading images\")]\n",
        "            self.hists = [read_hist(elem) for elem in tqdm(self.imgs_paths, desc=\"Preloading histograms\")]\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.preload:\n",
        "            return (\n",
        "                self.transform(self.imgs[idx]),\n",
        "                self.hists[idx],\n",
        "                np.array(self.white_points[idx], dtype=np.float32)\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                self.transform((read_image(self.imgs_paths[idx])).astype(np.float32)),\n",
        "                read_hist(self.hists_paths[idx]),\n",
        "                np.array(self.white_points[idx], dtype=np.float32)\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "\n",
        "class DataModule:\n",
        "    def __init__(self,\n",
        "                 dataset: IllumDataset,\n",
        "                 val_size: float = 0.2,\n",
        "                 batch_size: int = 32,\n",
        "                 preload: bool = False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.preload = preload\n",
        "        self._make_train_val_split(val_size=val_size)\n",
        "        self._init_datasets()\n",
        "        self._init_dataloaders()\n",
        "\n",
        "    def _make_train_val_split(self, val_size: float = 0.2):\n",
        "        n = len(self.dataset)\n",
        "        permuted_ids = np.random.permutation(np.arange(n))\n",
        "        self.val_ids = permuted_ids[:int(n * val_size)]\n",
        "        self.train_ids = permuted_ids[int(n * val_size):]\n",
        "\n",
        "    def _init_datasets(self):\n",
        "        self.train_dataset = AWBDataset(self.dataset, self.train_ids, True, self.preload)\n",
        "        self.val_dataset   = AWBDataset(self.dataset, self.val_ids, False, self.preload)\n",
        "\n",
        "    def _init_dataloaders(self):\n",
        "        self.train_dataloader = DataLoader(self.train_dataset,\n",
        "                                           batch_size=self.batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=10)\n",
        "        self.val_dataloader = DataLoader(self.val_dataset,\n",
        "                                         batch_size=self.batch_size,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=2)\n",
        "\n",
        "    def get_train_dataloader(self) -> DataLoader:\n",
        "        return self.train_dataloader\n",
        "\n",
        "    def get_val_dataloader(self) -> DataLoader:\n",
        "        return self.val_dataloader\n",
        "\n",
        "    def get_train_dataset(self) -> AWBDataset:\n",
        "        return self.train_dataset\n",
        "\n",
        "    def get_val_dataset(self) -> AWBDataset:\n",
        "        return self.val_dataset\n",
        "\n",
        "\n",
        "def create_datamodule(mode: Literal['train', 'test'] = 'train',\n",
        "                      val_size=0.2, batch_size=32) -> DataModule:\n",
        "    illum_dataset = IllumDataset(mode)\n",
        "    return DataModule(illum_dataset, val_size=val_size, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "d1jYuXw6gAAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from math import sqrt\n",
        "from typing import Literal\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Константы из первого скрипта\n",
        "TARGET_COLUMN_NAMES = ['wp_r', 'wp_g', 'wp_b']\n",
        "HIST_BINS = [116, 100]\n",
        "HIST_RANGE_FLAT = [-sqrt(3), sqrt(3), -1, 2]  # value ranges for beta and alpha\n",
        "HIST_TARGET_SIZE = [128, 128]\n",
        "HIST_VERT_PADD, HIST_HORR_PADD = 6, 14\n",
        "WHITE_LEVEL = 2**12 - 1 - 256\n",
        "\n",
        "def read_hist(path2hist: Path):\n",
        "    \"\"\"Чтение гистограммы из файла\"\"\"\n",
        "    hist = cv2.imread(str(path2hist), cv2.IMREAD_UNCHANGED)\n",
        "    hist = hist[HIST_HORR_PADD:-HIST_HORR_PADD,\n",
        "                HIST_VERT_PADD:-HIST_VERT_PADD]\n",
        "    hist = hist.astype(np.float32) / 255\n",
        "    return hist.astype(np.float32)\n",
        "\n",
        "def read_image(path2img: Path | str, white_level_corr: bool = True):\n",
        "    \"\"\"Чтение изображения из файла\"\"\"\n",
        "    img = cv2.imread(str(path2img), cv2.IMREAD_UNCHANGED)\n",
        "    if white_level_corr:\n",
        "        img = img / WHITE_LEVEL\n",
        "    if img.shape[-1] == 3:\n",
        "        img = img[..., ::-1]  # Convert BGR to RGB\n",
        "    return img\n",
        "\n",
        "def get_hists_dir(part: Literal['train', 'test'] = 'train'):\n",
        "    \"\"\"Получение пути к директории с гистограммами\"\"\"\n",
        "    return Path('/content/sasd').parent / f'{part}_histograms'\n",
        "\n",
        "def get_imgs_dir(part: Literal['train', 'test'] = 'train'):\n",
        "    \"\"\"Получение пути к директории с изображениями\"\"\"\n",
        "    return Path('/content/sasd').parent / f'{part}_imgs'\n",
        "\n",
        "def get_histogram_by_name(img_name: str, part: Literal['train', 'test'] = 'train'):\n",
        "    \"\"\"Получение гистограммы по имени изображения\"\"\"\n",
        "    hists_dir = get_hists_dir(part)\n",
        "    hist = read_hist((hists_dir / img_name).with_suffix('.png'))\n",
        "    return hist\n",
        "\n",
        "class IllumDataset(Dataset):\n",
        "    \"\"\"Основной датасет для работы с изображениями и гистограммами\"\"\"\n",
        "\n",
        "    def __init__(self, part: Literal['train', 'test'] = 'train'):\n",
        "        self.part = part\n",
        "        self._init_paths()\n",
        "        self._init_white_points()\n",
        "\n",
        "    def _init_paths(self):\n",
        "        \"\"\"Инициализация путей к файлам\"\"\"\n",
        "        self.imgs_dir = get_imgs_dir(self.part)\n",
        "        self.hists_dir = get_hists_dir(self.part)\n",
        "\n",
        "        # Получаем список всех изображений\n",
        "        self.imgs_paths = sorted(self.imgs_dir.glob('*.png'))\n",
        "        self.hists_paths = sorted(self.hists_dir.glob('*.png'))\n",
        "\n",
        "        # Проверяем соответствие количества файлов\n",
        "        if len(self.imgs_paths) != len(self.hists_paths):\n",
        "            print(f\"Warning: Different number of images ({len(self.imgs_paths)}) and histograms ({len(self.hists_paths)})\")\n",
        "\n",
        "    def _init_white_points(self):\n",
        "        \"\"\"Инициализация белых точек из CSV файла\"\"\"\n",
        "        csv_path = Path(\"/content/sasd\").parent / f'{self.part}.csv'\n",
        "        if csv_path.exists():\n",
        "            df = pd.read_csv(csv_path, converters={'white_points': literal_eval})\n",
        "            self.white_points = df[['wp_r', 'wp_g', 'wp_b']].values\n",
        "        else:\n",
        "            print(f\"Warning: CSV file {csv_path} not found\")\n",
        "            self.white_points = np.zeros((len(self.imgs_paths), 3))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Получение одного элемента датасета\"\"\"\n",
        "        # Чтение изображения\n",
        "        image = read_image(self.imgs_paths[idx])\n",
        "\n",
        "        # Чтение гистограммы\n",
        "        histogram = read_hist(self.hists_paths[idx])\n",
        "\n",
        "        # Получение белых точек\n",
        "        white_point = self.white_points[idx] if hasattr(self, 'white_points') else np.zeros(3)\n",
        "\n",
        "        return {\n",
        "            'image': image.astype(np.float32),\n",
        "            'histogram': histogram.astype(np.float32),\n",
        "            'white_point': white_point.astype(np.float32),\n",
        "            'image_path': str(self.imgs_paths[idx]),\n",
        "            'histogram_path': str(self.hists_paths[idx])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Количество элементов в датасете\"\"\"\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "class AWBDataset(Dataset):\n",
        "    \"\"\"Расширенный датасет с дополнительными возможностями\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: IllumDataset, indices: list = None,\n",
        "                 transform=None, preload: bool = False):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.indices = indices if indices is not None else list(range(len(base_dataset)))\n",
        "        self.transform = transform\n",
        "        self.preload = preload\n",
        "\n",
        "        if self.preload:\n",
        "            self._preload_data()\n",
        "\n",
        "    def _preload_data(self):\n",
        "        \"\"\"Предзагрузка данных в память\"\"\"\n",
        "        self.images = []\n",
        "        self.histograms = []\n",
        "        self.white_points = []\n",
        "\n",
        "        for idx in tqdm(self.indices, desc=\"Preloading data\"):\n",
        "            item = self.base_dataset[idx]\n",
        "            self.images.append(item['image'])\n",
        "            self.histograms.append(item['histogram'])\n",
        "            self.white_points.append(item['white_point'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Получение одного элемента датасета\"\"\"\n",
        "        actual_idx = self.indices[idx]\n",
        "\n",
        "        if self.preload:\n",
        "            image = self.images[idx]\n",
        "            histogram = self.histograms[idx]\n",
        "            white_point = self.white_points[idx]\n",
        "        else:\n",
        "            item = self.base_dataset[actual_idx]\n",
        "            image = item['image']\n",
        "            histogram = item['histogram']\n",
        "            white_point = item['white_point']\n",
        "\n",
        "        # Применение трансформаций\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'histogram': histogram,\n",
        "            'white_point': white_point\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Количество элементов в датасете\"\"\"\n",
        "        return len(self.indices)\n",
        "\n",
        "class DataModule:\n",
        "    \"\"\"Модуль данных для обучения и валидации\"\"\"\n",
        "\n",
        "    def __init__(self, part: Literal['train', 'test'] = 'train',\n",
        "                 val_size: float = 0.2, batch_size: int = 32,\n",
        "                 preload: bool = False, transform=None):\n",
        "        self.part = part\n",
        "        self.val_size = val_size\n",
        "        self.batch_size = batch_size\n",
        "        self.preload = preload\n",
        "\n",
        "        # Создаем базовый датасет\n",
        "        self.base_dataset = IllumDataset(part)\n",
        "\n",
        "        # Разделяем на train/val\n",
        "        self._make_train_val_split()\n",
        "\n",
        "        # Трансформации\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        # Создаем датасеты\n",
        "        self._init_datasets()\n",
        "\n",
        "        # Создаем dataloaders\n",
        "        self._init_dataloaders()\n",
        "\n",
        "    def _make_train_val_split(self):\n",
        "        \"\"\"Разделение данных на тренировочную и валидационную части\"\"\"\n",
        "        n = len(self.base_dataset)\n",
        "        all_indices = np.arange(n)\n",
        "        np.random.shuffle(all_indices)\n",
        "\n",
        "        val_count = int(n * self.val_size)\n",
        "        self.train_indices = all_indices[val_count:]\n",
        "        self.val_indices = all_indices[:val_count]\n",
        "\n",
        "    def _init_datasets(self):\n",
        "        \"\"\"Инициализация датасетов\"\"\"\n",
        "        self.train_dataset = AWBDataset(\n",
        "            self.base_dataset, self.train_indices,\n",
        "            transform=self.transform, preload=self.preload\n",
        "        )\n",
        "\n",
        "        self.val_dataset = AWBDataset(\n",
        "            self.base_dataset, self.val_indices,\n",
        "            transform=self.transform, preload=self.preload\n",
        "        )\n",
        "\n",
        "    def _init_dataloaders(self):\n",
        "        \"\"\"Инициализация dataloaders\"\"\"\n",
        "        self.train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        self.val_dataloader = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        \"\"\"Получение train и val dataloaders\"\"\"\n",
        "        return self.train_dataloader, self.val_dataloader\n",
        "\n",
        "    def get_datasets(self):\n",
        "        \"\"\"Получение train и val datasets\"\"\"\n",
        "        return self.train_dataset, self.val_dataset\n",
        "\n",
        "# Пример использования\n",
        "if __name__ == \"__main__\":\n",
        "    # Создаем базовый датасет\n",
        "    dataset = IllumDataset('train')\n",
        "\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "    # Получаем первый элемент\n",
        "    sample = dataset[0]\n",
        "    print(f\"Image shape: {sample['image'].shape}\")\n",
        "    print(f\"Histogram shape: {sample['histogram'].shape}\")\n",
        "    print(f\"White point: {sample['white_point']}\")\n",
        "\n",
        "    # Создаем DataModule для обучения\n",
        "    datamodule = DataModule('train', val_size=0.2, batch_size=16, preload=False)\n",
        "\n",
        "    train_loader, val_loader = datamodule.get_dataloaders()\n",
        "\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Val batches: {len(val_loader)}\")\n",
        "\n",
        "    # Пример итерации по данным\n",
        "    for batch in train_loader:\n",
        "        images = batch['image']\n",
        "        histograms = batch['histogram']\n",
        "        white_points = batch['white_point']\n",
        "\n",
        "        print(f\"Batch images shape: {images.shape}\")\n",
        "        print(f\"Batch histograms shape: {histograms.shape}\")\n",
        "        print(f\"Batch white points shape: {white_points.shape}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8astjCQh-5K",
        "outputId": "3e2a1762-78b0-47c6-e7a7-cad4bd0b205d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 1\n",
            "Image shape: (768, 1024, 3)\n",
            "Histogram shape: (100, 116)\n",
            "White point: [0.17368333 0.5086421  0.21542864]\n",
            "Train batches: 1\n",
            "Val batches: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch images shape: torch.Size([1, 3, 768, 1024])\n",
            "Batch histograms shape: torch.Size([1, 100, 116])\n",
            "Batch white points shape: torch.Size([1, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from math import acos, pi\n",
        "\n",
        "class AWB_DualStream(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(AWB_DualStream, self).__init__()\n",
        "\n",
        "        # Image branch - ResNet18\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.image_branch = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.image_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Histogram branch\n",
        "        self.hist_branch = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "        # Fusion and regression\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(512 + 128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, hist):\n",
        "        # Image features\n",
        "        img_feat = self.image_branch(image)\n",
        "        img_feat = self.image_pool(img_feat)\n",
        "        img_feat = img_feat.view(img_feat.size(0), -1)\n",
        "\n",
        "        # Histogram features (add channel dimension if needed)\n",
        "        if hist.dim() == 3:\n",
        "            hist = hist.unsqueeze(1)\n",
        "        hist_feat = self.hist_branch(hist)\n",
        "        hist_feat = hist_feat.view(hist_feat.size(0), -1)\n",
        "\n",
        "        # Fusion\n",
        "        combined = torch.cat([img_feat, hist_feat], dim=1)\n",
        "        wp_pred = self.fusion(combined)\n",
        "\n",
        "        return wp_pred"
      ],
      "metadata": {
        "id": "ur7rcNIOkWKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def angular_loss(y_pred, y_true, eps=1e-8):\n",
        "    \"\"\"Angular loss between predicted and true white points\"\"\"\n",
        "    y_pred_norm = y_pred / (y_pred.norm(dim=1, keepdim=True) + eps)\n",
        "    y_true_norm = y_true / (y_true.norm(dim=1, keepdim=True) + eps)\n",
        "\n",
        "    cos_sim = torch.sum(y_pred_norm * y_true_norm, dim=1)\n",
        "    cos_sim = torch.clamp(cos_sim, -1 + eps, 1 - eps)\n",
        "\n",
        "    angular_error = torch.acos(cos_sim) * (180 / pi)  # Convert to degrees\n",
        "    return angular_error.mean()\n",
        "\n",
        "def dist2hist_loss(y_pred, y_true, eps=1e-8):\n",
        "    \"\"\"Distance to histogram-based loss\"\"\"\n",
        "    # Normalize both vectors\n",
        "    y_pred_norm = y_pred / (y_pred.norm(dim=1, keepdim=True) + eps)\n",
        "    y_true_norm = y_true / (y_true.norm(dim=1, keepdim=True) + eps)\n",
        "\n",
        "    # Calculate Euclidean distance\n",
        "    euclidean_dist = torch.norm(y_pred_norm - y_true_norm, dim=1)\n",
        "\n",
        "    # Combine with angular error for better optimization\n",
        "    angular_error = angular_loss(y_pred, y_true, eps)\n",
        "\n",
        "    # Weighted combination (adjust weights as needed)\n",
        "    return 0.7 * euclidean_dist.mean() + 0.3 * angular_error\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combined loss function for AWB training\"\"\"\n",
        "    def __init__(self, alpha=0.7, beta=0.3, eps=1e-8):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Normalize vectors\n",
        "        y_pred_norm = y_pred / (y_pred.norm(dim=1, keepdim=True) + self.eps)\n",
        "        y_true_norm = y_true / (y_true.norm(dim=1, keepdim=True) + self.eps)\n",
        "\n",
        "        # Euclidean distance component\n",
        "        euclidean_dist = torch.norm(y_pred_norm - y_true_norm, dim=1).mean()\n",
        "\n",
        "        # Angular error component\n",
        "        cos_sim = torch.sum(y_pred_norm * y_true_norm, dim=1)\n",
        "        cos_sim = torch.clamp(cos_sim, -1 + self.eps, 1 - self.eps)\n",
        "        angular_error = torch.acos(cos_sim).mean() * (180 / pi)\n",
        "\n",
        "        return self.alpha * euclidean_dist + self.beta * angular_error"
      ],
      "metadata": {
        "id": "niSnOnOemJWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from math import sqrt\n",
        "from typing import Literal\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from math import acos, pi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Константы\n",
        "TARGET_COLUMN_NAMES = ['wp_r', 'wp_g', 'wp_b']\n",
        "HIST_BINS = [116, 100]\n",
        "HIST_RANGE_FLAT = [-sqrt(3), sqrt(3), -1, 2]\n",
        "HIST_TARGET_SIZE = [128, 128]\n",
        "HIST_VERT_PADD, HIST_HORR_PADD = 6, 14\n",
        "WHITE_LEVEL = 2**12 - 1 - 256\n",
        "\n",
        "def read_hist(path2hist: Path):\n",
        "    \"\"\"Чтение гистограммы из файла\"\"\"\n",
        "    hist = cv2.imread(str(path2hist), cv2.IMREAD_UNCHANGED)\n",
        "    if hist is None:\n",
        "        raise ValueError(f\"Could not read histogram from {path2hist}\")\n",
        "    hist = hist[HIST_HORR_PADD:-HIST_HORR_PADD,\n",
        "                HIST_VERT_PADD:-HIST_VERT_PADD]\n",
        "    hist = hist.astype(np.float32) / 255\n",
        "    return hist\n",
        "\n",
        "def read_image(path2img: Path | str, white_level_corr: bool = True):\n",
        "    \"\"\"Чтение изображения из файла\"\"\"\n",
        "    img = cv2.imread(str(path2img), cv2.IMREAD_UNCHANGED)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not read image from {path2img}\")\n",
        "    if white_level_corr:\n",
        "        img = img / WHITE_LEVEL\n",
        "    if img.shape[-1] == 3:\n",
        "        img = img[..., ::-1]  # Convert BGR to RGB\n",
        "    return img\n",
        "\n",
        "class IllumDataset(Dataset):\n",
        "    \"\"\"Основной датасет для работы с изображениями и гистограммами\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: Path, part: Literal['train', 'test'] = 'train'):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.part = part\n",
        "        self._init_paths()\n",
        "        self._init_white_points()\n",
        "\n",
        "    def _init_paths(self):\n",
        "        \"\"\"Инициализация путей к файлам\"\"\"\n",
        "        self.imgs_dir = '/content/train_imgs'\n",
        "        self.hists_dir = '/content/train_histograms'\n",
        "    def _init_white_points(self):\n",
        "        \"\"\"Инициализация белых точек из CSV файла\"\"\"\n",
        "        csv_path = '/content/train_compatible.csv'\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Получение одного элемента датасета\"\"\"\n",
        "        try:\n",
        "            # Чтение изображения\n",
        "            image = read_image(self.imgs_paths[idx])\n",
        "\n",
        "            # Чтение гистограммы\n",
        "            histogram = read_hist(self.hists_paths[idx])\n",
        "\n",
        "            # Получение белых точек\n",
        "            white_point = self.white_points[idx] if hasattr(self, 'white_points') else np.zeros(3)\n",
        "\n",
        "            return {\n",
        "                'image': image.astype(np.float32),\n",
        "                'histogram': histogram.astype(np.float32),\n",
        "                'white_point': white_point.astype(np.float32),\n",
        "                'image_name': self.imgs_paths[idx].stem\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {idx}: {e}\")\n",
        "            # Return a dummy sample to avoid breaking the training\n",
        "            dummy_img = np.zeros((256, 256, 3), dtype=np.float32)\n",
        "            dummy_hist = np.zeros((100, 100), dtype=np.float32)\n",
        "            dummy_wp = np.zeros(3, dtype=np.float32)\n",
        "            return {\n",
        "                'image': dummy_img,\n",
        "                'histogram': dummy_hist,\n",
        "                'white_point': dummy_wp,\n",
        "                'image_name': 'dummy'\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Количество элементов в датасете\"\"\"\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "# Модель и функции потерь (остаются без изменений)\n",
        "class AWB_DualStream(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(AWB_DualStream, self).__init__()\n",
        "\n",
        "        # Image branch - ResNet18\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.image_branch = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.image_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Histogram branch\n",
        "        self.hist_branch = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "        # Fusion and regression\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(512 + 128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, hist):\n",
        "        # Image features\n",
        "        img_feat = self.image_branch(image)\n",
        "        img_feat = self.image_pool(img_feat)\n",
        "        img_feat = img_feat.view(img_feat.size(0), -1)\n",
        "\n",
        "        # Histogram features\n",
        "        if hist.dim() == 3:\n",
        "            hist = hist.unsqueeze(1)\n",
        "        hist_feat = self.hist_branch(hist)\n",
        "        hist_feat = hist_feat.view(hist_feat.size(0), -1)\n",
        "\n",
        "        # Fusion\n",
        "        combined = torch.cat([img_feat, hist_feat], dim=1)\n",
        "        wp_pred = self.fusion(combined)\n",
        "\n",
        "        return wp_pred\n",
        "\n",
        "def angular_loss(y_pred, y_true, eps=1e-8):\n",
        "    \"\"\"Angular loss between predicted and true white points\"\"\"\n",
        "    y_pred_norm = y_pred / (y_pred.norm(dim=1, keepdim=True) + eps)\n",
        "    y_true_norm = y_true / (y_true.norm(dim=1, keepdim=True) + eps)\n",
        "\n",
        "    cos_sim = torch.sum(y_pred_norm * y_true_norm, dim=1)\n",
        "    cos_sim = torch.clamp(cos_sim, -1 + eps, 1 - eps)\n",
        "\n",
        "    angular_error = torch.acos(cos_sim) * (180 / pi)\n",
        "    return angular_error.mean()\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combined loss function for AWB training\"\"\"\n",
        "    def __init__(self, alpha=0.7, beta=0.3, eps=1e-8):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Normalize vectors\n",
        "        y_pred_norm = y_pred / (y_pred.norm(dim=1, keepdim=True) + self.eps)\n",
        "        y_true_norm = y_true / (y_true.norm(dim=1, keepdim=True) + self.eps)\n",
        "\n",
        "        # Euclidean distance component\n",
        "        euclidean_dist = torch.norm(y_pred_norm - y_true_norm, dim=1).mean()\n",
        "\n",
        "        # Angular error component\n",
        "        cos_sim = torch.sum(y_pred_norm * y_true_norm, dim=1)\n",
        "        cos_sim = torch.clamp(cos_sim, -1 + self.eps, 1 - self.eps)\n",
        "        angular_error = torch.acos(cos_sim).mean() * (180 / pi)\n",
        "\n",
        "        return self.alpha * euclidean_dist + self.beta * angular_error\n",
        "\n",
        "class AWBTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "        self.criterion = CombinedLoss(alpha=0.7, beta=0.3)\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_errors = []\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc='Training')\n",
        "        for batch in pbar:\n",
        "            images = batch['image'].permute(0, 3, 1, 2).to(self.device)  # (B, H, W, C) -> (B, C, H, W)\n",
        "            hists = batch['histogram'].unsqueeze(1).to(self.device)  # Add channel dimension\n",
        "            wp_true = batch['white_point'].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            wp_pred = self.model(images, hists)\n",
        "            loss = self.criterion(wp_pred, wp_true)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_error = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                images = batch['image'].permute(0, 3, 1, 2).to(self.device)\n",
        "                hists = batch['histogram'].unsqueeze(1).to(self.device)\n",
        "                wp_true = batch['white_point'].to(self.device)\n",
        "\n",
        "                wp_pred = self.model(images, hists)\n",
        "                loss = self.criterion(wp_pred, wp_true)\n",
        "                angular_err = angular_loss(wp_pred, wp_true)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_error += angular_err.item()\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        avg_error = total_error / len(self.val_loader)\n",
        "\n",
        "        return avg_loss, avg_error\n",
        "\n",
        "    def train(self, num_epochs=50):\n",
        "        best_val_error = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "            # Train\n",
        "            train_loss = self.train_epoch()\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_error = self.validate()\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_errors.append(val_error)\n",
        "\n",
        "            print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Angular Error: {val_error:.2f}°')\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Save best model\n",
        "            if val_error < best_val_error:\n",
        "                best_val_error = val_error\n",
        "                torch.save(self.model.state_dict(), 'best_awb_model.pth')\n",
        "                print(f'New best model saved with error: {best_val_error:.2f}°')\n",
        "\n",
        "            print('-' * 50)\n",
        "\n",
        "    def plot_training(self):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "        plt.plot(self.val_losses, label='Val Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Training and Validation Loss')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.val_errors, label='Angular Error', color='red')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Error (°)')\n",
        "        plt.legend()\n",
        "        plt.title('Validation Angular Error')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_plot.png')\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    # Укажите правильный путь к вашим данным\n",
        "    base_dir = Path(\"/content\")  # Измените на ваш путь\n",
        "\n",
        "    # Создаем датасет\n",
        "    try:\n",
        "        dataset = IllumDataset(base_dir, 'train')\n",
        "        print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "        # Разделяем на train/val\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            dataset, [train_size, val_size]\n",
        "        )\n",
        "\n",
        "        # Создаем DataLoader'ы\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=32, shuffle=True, num_workers=2\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=32, shuffle=False, num_workers=2\n",
        "        )\n",
        "\n",
        "        # Инициализируем модель\n",
        "        model = AWB_DualStream(pretrained=True)\n",
        "\n",
        "        # Инициализируем тренер\n",
        "        trainer = AWBTrainer(model, train_loader, val_loader, device='cuda')\n",
        "\n",
        "        # Обучаем\n",
        "        trainer.train(num_epochs=30)\n",
        "\n",
        "        # Сохраняем финальную модель\n",
        "        torch.save(model.state_dict(), 'final_awb_model.pth')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please check your data paths and structure\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import torch.optim as optim\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBl9LMG6nV-C",
        "outputId": "173dc0ab-ddd5-43bd-afb4-584d855f1eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'IllumDataset' object has no attribute 'imgs_paths'\n",
            "Please check your data paths and structure\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_compatible_csv(existing_csv_path, output_csv_path):\n",
        "    \"\"\"Создает CSV файл с именами файлов без префикса пути\"\"\"\n",
        "    df = pd.read_csv(existing_csv_path)\n",
        "\n",
        "    # Извлекаем только имена файлов без путей\n",
        "    if 'names' in df.columns:\n",
        "        df['image_name'] = df['names'].apply(lambda x: Path(x).stem)\n",
        "    else:\n",
        "        # Предполагаем, что первая колонка содержит пути\n",
        "        first_col = df.columns[0]\n",
        "        df['image_name'] = df[first_col].apply(lambda x: Path(str(x)).stem)\n",
        "\n",
        "    # Сохраняем только нужные колонки\n",
        "    result_df = df[['image_name', 'wp_r', 'wp_g', 'wp_b']]\n",
        "    result_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Created compatible CSV: {output_csv_path}\")\n",
        "    print(f\"Rows: {len(result_df)}\")\n",
        "    return result_df\n",
        "\n",
        "# Использование\n",
        "create_compatible_csv(\"/content/train.csv\", \"/content/train2.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "0FSj3FwF4sni",
        "outputId": "9dbe425b-107a-4853-c011-376a2d9cb427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created compatible CSV: /content/train2.csv\n",
            "Rows: 570\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    image_name      wp_r      wp_g      wp_b\n",
              "0         0000  0.173683  0.508642  0.215429\n",
              "1         0002  0.266894  0.956725  0.577948\n",
              "2         0004  0.146930  0.495538  0.265573\n",
              "3         0005  0.218046  0.712538  0.402101\n",
              "4         0008  0.070384  0.183209  0.125570\n",
              "..         ...       ...       ...       ...\n",
              "565       0937  0.215597  0.226323  0.196691\n",
              "566       0938  0.169317  0.299355  0.544165\n",
              "567       0939  0.105052  0.339278  0.126710\n",
              "568       0940  0.222516  0.693838  0.352222\n",
              "569       0941  0.104517  0.303611  0.170755\n",
              "\n",
              "[570 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba85c6d0-c62b-4c85-99e3-38532216ca2a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>wp_r</th>\n",
              "      <th>wp_g</th>\n",
              "      <th>wp_b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000</td>\n",
              "      <td>0.173683</td>\n",
              "      <td>0.508642</td>\n",
              "      <td>0.215429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002</td>\n",
              "      <td>0.266894</td>\n",
              "      <td>0.956725</td>\n",
              "      <td>0.577948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0004</td>\n",
              "      <td>0.146930</td>\n",
              "      <td>0.495538</td>\n",
              "      <td>0.265573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0005</td>\n",
              "      <td>0.218046</td>\n",
              "      <td>0.712538</td>\n",
              "      <td>0.402101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0008</td>\n",
              "      <td>0.070384</td>\n",
              "      <td>0.183209</td>\n",
              "      <td>0.125570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>0937</td>\n",
              "      <td>0.215597</td>\n",
              "      <td>0.226323</td>\n",
              "      <td>0.196691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>0938</td>\n",
              "      <td>0.169317</td>\n",
              "      <td>0.299355</td>\n",
              "      <td>0.544165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>0939</td>\n",
              "      <td>0.105052</td>\n",
              "      <td>0.339278</td>\n",
              "      <td>0.126710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>0940</td>\n",
              "      <td>0.222516</td>\n",
              "      <td>0.693838</td>\n",
              "      <td>0.352222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>0941</td>\n",
              "      <td>0.104517</td>\n",
              "      <td>0.303611</td>\n",
              "      <td>0.170755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>570 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba85c6d0-c62b-4c85-99e3-38532216ca2a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba85c6d0-c62b-4c85-99e3-38532216ca2a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba85c6d0-c62b-4c85-99e3-38532216ca2a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4396fd9e-7044-428b-9519-113423fb4372\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4396fd9e-7044-428b-9519-113423fb4372')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4396fd9e-7044-428b-9519-113423fb4372 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"create_compatible_csv(\\\"/content/train\",\n  \"rows\": 570,\n  \"fields\": [\n    {\n      \"column\": \"image_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 570,\n        \"samples\": [\n          \"0847\",\n          \"0118\",\n          \"0216\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wp_r\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10585229639482309,\n        \"min\": 0.0016413042151472,\n        \"max\": 1.0,\n        \"num_unique_values\": 570,\n        \"samples\": [\n          0.120530638054337,\n          1.0,\n          0.1070682574957868\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wp_g\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23836343548737748,\n        \"min\": 0.0305009180571445,\n        \"max\": 1.0,\n        \"num_unique_values\": 562,\n        \"samples\": [\n          0.2445917444308211,\n          0.9943370227039724,\n          0.7094176168161563\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wp_b\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16360662976675058,\n        \"min\": 0.0141325800753881,\n        \"max\": 1.0,\n        \"num_unique_values\": 569,\n        \"samples\": [\n          0.1582210856805528,\n          0.9303451500171228,\n          0.0578082042599708\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from math import sqrt\n",
        "from typing import Literal\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from math import acos, pi\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "# Константы\n",
        "TARGET_COLUMN_NAMES = ['wp_r', 'wp_g', 'wp_b']\n",
        "HIST_BINS = [116, 100]\n",
        "HIST_RANGE_FLAT = [-sqrt(3), sqrt(3), -1, 2]\n",
        "HIST_TARGET_SIZE = [128, 128]\n",
        "HIST_VERT_PADD, HIST_HORR_PADD = 6, 14\n",
        "WHITE_LEVEL = 2**12 - 1 - 256\n",
        "\n",
        "def read_hist(path2hist: Path):\n",
        "    \"\"\"Чтение гистограммы из файла\"\"\"\n",
        "    hist = cv2.imread(str(path2hist), cv2.IMREAD_UNCHANGED)\n",
        "    if hist is None:\n",
        "        raise ValueError(f\"Could not read histogram from {path2hist}\")\n",
        "    hist = hist[HIST_HORR_PADD:-HIST_HORR_PADD,\n",
        "                HIST_VERT_PADD:-HIST_VERT_PADD]\n",
        "    hist = hist.astype(np.float32) / 255\n",
        "    return hist\n",
        "\n",
        "def read_image(path2img: Path | str, white_level_corr: bool = True):\n",
        "    \"\"\"Чтение изображения из файла\"\"\"\n",
        "    img = cv2.imread(str(path2img), cv2.IMREAD_UNCHANGED)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not read image from {path2img}\")\n",
        "    if white_level_corr:\n",
        "        img = img / WHITE_LEVEL\n",
        "    if img.shape[-1] == 3:\n",
        "        img = img[..., ::-1]  # Convert BGR to RGB\n",
        "    return img\n",
        "\n",
        "class IllumDataset(Dataset):\n",
        "    \"\"\"Основной датасет для работы с изображениями и гистограммами\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: Path, part: Literal['train', 'test'] = 'train'):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.part = part\n",
        "        self._init_paths()\n",
        "        self._init_white_points()\n",
        "\n",
        "    def _init_paths(self):\n",
        "        \"\"\"Инициализация путей к файлам\"\"\"\n",
        "        self.imgs_dir = self.base_dir / f'{self.part}_imgs'\n",
        "        self.hists_dir = self.base_dir / f'{self.part}_histograms'\n",
        "\n",
        "        # Проверяем существование директорий\n",
        "        if not self.imgs_dir.exists():\n",
        "            raise ValueError(f\"Images directory {self.imgs_dir} does not exist\")\n",
        "        if not self.hists_dir.exists():\n",
        "            raise ValueError(f\"Histograms directory {self.hists_dir} does not exist\")\n",
        "\n",
        "        # Получаем список всех изображений\n",
        "        self.imgs_paths = sorted(self.imgs_dir.glob('*.png'))\n",
        "        self.hists_paths = sorted(self.hists_dir.glob('*.png'))\n",
        "\n",
        "        print(f\"Found {len(self.imgs_paths)} images and {len(self.hists_paths)} histograms\")\n",
        "\n",
        "        # Проверяем соответствие файлов\n",
        "        img_names = {p.stem for p in self.imgs_paths}\n",
        "        hist_names = {p.stem for p in self.hists_paths}\n",
        "\n",
        "        if img_names != hist_names:\n",
        "            print(f\"Warning: Image and histogram names don't match completely\")\n",
        "            common_names = img_names & hist_names\n",
        "            self.imgs_paths = [p for p in self.imgs_paths if p.stem in common_names]\n",
        "            self.hists_paths = [p for p in self.hists_paths if p.stem in common_names]\n",
        "            print(f\"Using {len(self.imgs_paths)} common files\")\n",
        "\n",
        "    def _init_white_points(self):\n",
        "        \"\"\"Инициализация белых точек из CSV файла\"\"\"\n",
        "        csv_path = self.base_dir / f'train2.csv'\n",
        "        self.white_points = np.zeros((len(self.imgs_paths), 3))\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Получение одного элемента датасета\"\"\"\n",
        "        try:\n",
        "          image = read_image(self.imgs_paths[idx])\n",
        "          histogram = read_hist(self.hists_paths[idx])\n",
        "          white_point = self.white_points[idx]\n",
        "\n",
        "          # Ресайз изображения до 256x256\n",
        "          image = cv2.resize(image, (256, 256))\n",
        "\n",
        "          # Ресайз гистограммы если нужно\n",
        "          histogram = cv2.resize(histogram, (128, 128))\n",
        "\n",
        "          image_tensor = torch.from_numpy(image).permute(2, 0, 1).float()\n",
        "          hist_tensor = torch.from_numpy(histogram).unsqueeze(0).float()\n",
        "          wp_tensor = torch.from_numpy(white_point).float()\n",
        "\n",
        "          return {\n",
        "              'image': image_tensor,\n",
        "              'histogram': hist_tensor,\n",
        "              'white_point': wp_tensor,\n",
        "              'image_name': self.imgs_paths[idx].stem\n",
        "          }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {idx}: {e}\")\n",
        "            # Return a dummy sample\n",
        "            dummy_img = torch.zeros((3, 256, 256), dtype=torch.float32)\n",
        "            dummy_hist = torch.zeros((1, 100, 100), dtype=torch.float32)\n",
        "            dummy_wp = torch.zeros(3, dtype=torch.float32)\n",
        "            return {\n",
        "                'image': dummy_img,\n",
        "                'histogram': dummy_hist,\n",
        "                'white_point': dummy_wp,\n",
        "                'image_name': 'dummy'\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Количество элементов в датасете\"\"\"\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "# Модель AWB_DualStream (остается без изменений)\n",
        "class AWB_DualStream(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(AWB_DualStream, self).__init__()\n",
        "\n",
        "        # Image branch - ResNet18\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.image_branch = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.image_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Histogram branch\n",
        "        self.hist_branch = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "        # Fusion and regression\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(512 + 128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 3),\n",
        "            nn.Sigmoid()  # ← Добавьте Sigmoid для диапазона [0, 1]\n",
        "        )\n",
        "    def forward(self, image, hist):\n",
        "        # Image features\n",
        "        img_feat = self.image_branch(image)\n",
        "        img_feat = self.image_pool(img_feat)\n",
        "        img_feat = img_feat.view(img_feat.size(0), -1)\n",
        "        img_feat = img_feat.unsqueeze(0)\n",
        "        # Histogram features\n",
        "        hist_feat = self.hist_branch(hist)\n",
        "        hist_feat = hist_feat.view(hist_feat.size(0), -1)\n",
        "        hist_feat = hist_feat.unsqueeze(0)\n",
        "\n",
        "        # Fusion\n",
        "        combined = torch.cat([img_feat, hist_feat], dim=1)\n",
        "        wp_pred = self.fusion(combined)\n",
        "\n",
        "        return wp_pred\n",
        "\n",
        "# Функции потерь (остаются без изменений)\n",
        "def angular_loss(y_pred, y_true, eps=1e-8):\n",
        "    \"\"\"Angular loss between predicted and true white points\"\"\"\n",
        "    y_pred_norm = y_pred / (y_pred.norm(dim=1, keepdim=True) + eps)\n",
        "    y_true_norm = y_true / (y_true.norm(dim=1, keepdim=True) + eps)\n",
        "\n",
        "    cos_sim = torch.sum(y_pred_norm * y_true_norm, dim=1)\n",
        "    cos_sim = torch.clamp(cos_sim, -1 + eps, 1 - eps)\n",
        "\n",
        "    angular_error = torch.acos(cos_sim) * (180 / pi)\n",
        "    return angular_error.mean()\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combined loss function for AWB training\"\"\"\n",
        "    def __init__(self, alpha=0.7, beta=0.3, eps=1e-8):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Normalize vectors\n",
        "        y_pred_norm = y_pred / (y_pred.norm(dim=1, keepdim=True) + self.eps)\n",
        "        y_true_norm = y_true / (y_true.norm(dim=1, keepdim=True) + self.eps)\n",
        "\n",
        "        # Euclidean distance component\n",
        "        euclidean_dist = torch.norm(y_pred_norm - y_true_norm, dim=1).mean()\n",
        "\n",
        "        # Angular error component\n",
        "        cos_sim = torch.sum(y_pred_norm * y_true_norm, dim=1)\n",
        "        cos_sim = torch.clamp(cos_sim, -1 + self.eps, 1 - self.eps)\n",
        "        angular_error = torch.acos(cos_sim).mean() * (180 / pi)\n",
        "\n",
        "        return self.alpha * euclidean_dist + self.beta * angular_error\n",
        "\n",
        "class AWBTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device='cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        # Убедитесь, что передаются правильные параметры\n",
        "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "        print(f\"Training {len(trainable_params)} parameters\")\n",
        "\n",
        "        #optimizer = optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "        self.criterion = CombinedLoss(alpha=0.7, beta=0.3)\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_errors = []\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(self.train_loader)\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc='Training')\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            images = batch['image'].to(self.device)\n",
        "            hists = batch['histogram'].to(self.device)\n",
        "            wp_true = batch['white_point'].to(self.device)\n",
        "\n",
        "            # Убедимся в правильности размерностей\n",
        "            if hists.dim() == 3:\n",
        "                hists = hists.unsqueeze(1)  # (B, H, W) -> (B, 1, H, W)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            wp_pred = self.model(images, hists)\n",
        "            loss = self.criterion(wp_pred, wp_true)\n",
        "\n",
        "            # Отладочная информация (только для первых few батчей)\n",
        "            if batch_idx < 2:  # Только первые 2 батча для отладки\n",
        "                print(f'Batch {batch_idx}:')\n",
        "                print(f'  wp_pred range: [{wp_pred.min():.3f}, {wp_pred.max():.3f}]')\n",
        "                print(f'  wp_true range: [{wp_true.min():.3f}, {wp_true.max():.3f}]')\n",
        "                print(f'  Loss: {loss.item():.4f}')\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients для стабильности\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        return total_loss   # ← ВОТ ИСПРАВЛЕНИЕ!\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_error = 0\n",
        "        num_batches = len(self.val_loader)\n",
        "\n",
        "        val_pbar = tqdm(self.val_loader, desc='Validation')\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_pbar):\n",
        "                images = batch['image'].to(self.device)\n",
        "                hists = batch['histogram'].to(self.device)\n",
        "                wp_true = batch['white_point'].to(self.device)\n",
        "\n",
        "                if hists.dim() == 3:\n",
        "                    hists = hists.unsqueeze(1)\n",
        "\n",
        "                wp_pred = self.model(images, hists)\n",
        "                loss = self.criterion(wp_pred, wp_true)\n",
        "                angular_err = angular_loss(wp_pred, wp_true)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_error += angular_err.item()\n",
        "\n",
        "                val_pbar.set_postfix({'val_loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_loss = total_loss\n",
        "        avg_error = total_error\n",
        "\n",
        "        return avg_loss, avg_error\n",
        "\n",
        "    def train(self, num_epochs=30):\n",
        "        best_val_error = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "            # Train\n",
        "            train_loss = self.train_epoch()\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_error = self.validate()\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_errors.append(val_error)\n",
        "\n",
        "            print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Angular Error: {val_error:.2f}°')\n",
        "\n",
        "            # Проверка обучения\n",
        "            if epoch == 0:\n",
        "                print(\"First epoch check:\")\n",
        "                print(f\"  Train loss: {train_loss:.4f}\")\n",
        "                print(f\"  Val loss: {val_loss:.4f}\")\n",
        "                print(f\"  If both are 0, check data and model!\")\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Save best model\n",
        "            if val_error < best_val_error:\n",
        "                best_val_error = val_error\n",
        "                torch.save(self.model.state_dict(), 'best_awb_model.pth')\n",
        "                print(f'New best model saved with error: {best_val_error:.2f}°')\n",
        "\n",
        "            print('-' * 50)\n",
        "\n",
        "    def plot_training(self):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "        plt.plot(self.val_losses, label='Val Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Training and Validation Loss')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.val_errors, label='Angular Error', color='red')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Error (°)')\n",
        "        plt.legend()\n",
        "        plt.title('Validation Angular Error')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_plot.png')\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    # Укажите правильный путь к вашим данным\n",
        "    base_dir = Path(\"/content\")  # Измените на ваш путь\n",
        "\n",
        "    try:\n",
        "        # Создаем датасет\n",
        "        dataset = IllumDataset(base_dir, 'train')\n",
        "        print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "        if len(dataset) == 0:\n",
        "            print(\"No data found! Please check your paths.\")\n",
        "            return\n",
        "\n",
        "        # Создаем DataLoader напрямую без random_split\n",
        "        train_loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=16,  # Уменьшим batch size для стабильности\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            drop_last=True  # Избегаем проблем с последним батчем\n",
        "        )\n",
        "\n",
        "        # Создаем валидационный loader (можно использовать тот же датасет для демо)\n",
        "        val_loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        # Инициализируем модель\n",
        "        model = AWB_DualStream(pretrained=True)\n",
        "        print(\"Model created\")\n",
        "\n",
        "        # Инициализируем тренер\n",
        "        trainer = AWBTrainer(model, train_loader, val_loader, device='cpu')\n",
        "        print(\"Trainer initialized\")\n",
        "\n",
        "        # Обучаем\n",
        "        print(\"Starting training...\")\n",
        "        trainer.train(num_epochs=10)  # Начнем с 10 эпох\n",
        "\n",
        "        # Сохраняем финальную модель\n",
        "        torch.save(model.state_dict(), 'final_awb_model.pth')\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Please check your data paths and structure\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ulZkmhJp51B",
        "outputId": "66917652-d3f9-4874-9499-260bb26f808b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 images and 1 histograms\n",
            "Dataset size: 1\n",
            "Model created\n",
            "Training 78 parameters\n",
            "Trainer initialized\n",
            "Starting training...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "First epoch check:\n",
            "  Train loss: 0.0000\n",
            "  Val loss: 0.0000\n",
            "  If both are 0, check data and model!\n",
            "New best model saved with error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 0it [00:00, ?it/s]\n",
            "Validation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0000, Val Loss: 0.0000, Angular Error: 0.00°\n",
            "--------------------------------------------------\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Инициализируйте модель\n",
        "model = AWB_DualStream(pretrained=True)\n",
        "\n",
        "# 4. Запустите обучение\n",
        "trainer = AWBTrainer(model, train_loader, train_loader, device='cpu')  # Используем train для val для демо\n",
        "trainer.train(num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "jTKy3FWzvpgK",
        "outputId": "d03a21f6-28bd-4c47-e016-ea699fc3bf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 78 parameters\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Training:   0%|          | 0/1 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 512 but got size 128 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1641450307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 4. Запустите обучение\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAWBTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Используем train для val для демо\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3616895521.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3616895521.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mwp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwp_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwp_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3616895521.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, hist)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Fusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_feat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mwp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 512 but got size 128 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Протестируйте функцию потерь отдельно\n",
        "def test_loss_function():\n",
        "    # Создаем случайные данные\n",
        "    pred = torch.randn(32, 3, requires_grad=True)  # batch_size=32\n",
        "    true = torch.randn(32, 3)\n",
        "\n",
        "    criterion = CombinedLoss()\n",
        "    loss = criterion(pred, true)\n",
        "\n",
        "    print(f\"Random data loss: {loss.item()}\")\n",
        "\n",
        "    # Проверяем backward\n",
        "    loss.backward()\n",
        "    print(f\"Gradients computed: {pred.grad is not None}\")\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "test_loss_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM67QUugk-Yn",
        "outputId": "2bc0d898-a836-4f65-95fb-b018abeb9e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random data loss: 30.192222595214844\n",
            "Gradients computed: True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.192222595214844"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_model_trainability(model):\n",
        "    \"\"\"Проверяет, может ли модель вообще обучаться\"\"\"\n",
        "    # Тестовый forward pass\n",
        "    test_image = torch.randn(1, 3, 256, 256)\n",
        "    test_hist = torch.randn(1, 1, 100, 100)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(test_image, test_hist)\n",
        "        print(f\"Model output: {output}\")\n",
        "        print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
        "\n",
        "    # Проверка параметров\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "            print(f\"✓ Trainable: {name} - {param.numel()} params\")\n",
        "        else:\n",
        "            print(f\"✗ Frozen: {name} - {param.numel()} params\")\n",
        "\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
        "\n",
        "    return trainable_params > 0\n",
        "\n",
        "check_model_trainability(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "2mi_6EMqk7Vh",
        "outputId": "a82d8312-11f2-4ff2-9386-42f50f48835b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1724428789.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainable_params\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcheck_model_trainability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Упрощенный отладочный тренировочный цикл\n",
        "def debug_training():\n",
        "    model = AWB_DualStream(pretrained=True)\n",
        "    model.train()\n",
        "\n",
        "    # Проверка модели\n",
        "    check_model_trainability(model)\n",
        "\n",
        "    # Тестовый батч\n",
        "    test_batch = next(iter(train_loader))\n",
        "    images = test_batch['image'].to('cpu')\n",
        "    hists = test_batch['histogram'].to('cpu')\n",
        "    wp_true = test_batch['white_point'].to('cpu')\n",
        "\n",
        "    print(\"Data check:\")\n",
        "    print(f\"Images: {images.shape}, range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "    print(f\"Hists: {hists.shape}, range: [{hists.min():.3f}, {hists.max():.3f}]\")\n",
        "    print(f\"WP true: {wp_true.shape}, range: [{wp_true.min():.3f}, {wp_true.max():.3f}]\")\n",
        "\n",
        "    # Forward\n",
        "    wp_pred = model(images, hists)\n",
        "    print(f\"WP pred: {wp_pred.shape}, range: [{wp_pred.min():.3f}, {wp_pred.max():.3f}]\")\n",
        "\n",
        "    # Loss\n",
        "    criterion = CombinedLoss()\n",
        "    loss = criterion(wp_pred, wp_true)\n",
        "    print(f\"Initial loss: {loss.item()}\")\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradients\n",
        "    has_gradients = False\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None and param.grad.norm() > 0:\n",
        "            has_gradients = True\n",
        "            print(f\"✓ Gradient in {name}: {param.grad.norm().item()}\")\n",
        "            break\n",
        "\n",
        "    if not has_gradients:\n",
        "        print(\"❌ NO GRADIENTS DETECTED!\")\n",
        "        # Попробуйте уменьшить learning rate или изменить архитектуру\n",
        "    else:\n",
        "        print(\"✅ Gradients detected - model can learn!\")\n",
        "\n",
        "debug_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DnO6KZGRkyYe",
        "outputId": "e4320e68-eb81-4207-d77f-c9aa61a6f9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[ 0.2221, -0.0370, -0.2335]])\n",
            "Output range: [-0.234, 0.222]\n",
            "✓ Trainable: image_branch.0.weight - 9408 params\n",
            "✓ Trainable: image_branch.1.weight - 64 params\n",
            "✓ Trainable: image_branch.1.bias - 64 params\n",
            "✓ Trainable: image_branch.4.0.conv1.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.0.bn1.weight - 64 params\n",
            "✓ Trainable: image_branch.4.0.bn1.bias - 64 params\n",
            "✓ Trainable: image_branch.4.0.conv2.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.0.bn2.weight - 64 params\n",
            "✓ Trainable: image_branch.4.0.bn2.bias - 64 params\n",
            "✓ Trainable: image_branch.4.1.conv1.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.1.bn1.weight - 64 params\n",
            "✓ Trainable: image_branch.4.1.bn1.bias - 64 params\n",
            "✓ Trainable: image_branch.4.1.conv2.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.1.bn2.weight - 64 params\n",
            "✓ Trainable: image_branch.4.1.bn2.bias - 64 params\n",
            "✓ Trainable: image_branch.5.0.conv1.weight - 73728 params\n",
            "✓ Trainable: image_branch.5.0.bn1.weight - 128 params\n",
            "✓ Trainable: image_branch.5.0.bn1.bias - 128 params\n",
            "✓ Trainable: image_branch.5.0.conv2.weight - 147456 params\n",
            "✓ Trainable: image_branch.5.0.bn2.weight - 128 params\n",
            "✓ Trainable: image_branch.5.0.bn2.bias - 128 params\n",
            "✓ Trainable: image_branch.5.0.downsample.0.weight - 8192 params\n",
            "✓ Trainable: image_branch.5.0.downsample.1.weight - 128 params\n",
            "✓ Trainable: image_branch.5.0.downsample.1.bias - 128 params\n",
            "✓ Trainable: image_branch.5.1.conv1.weight - 147456 params\n",
            "✓ Trainable: image_branch.5.1.bn1.weight - 128 params\n",
            "✓ Trainable: image_branch.5.1.bn1.bias - 128 params\n",
            "✓ Trainable: image_branch.5.1.conv2.weight - 147456 params\n",
            "✓ Trainable: image_branch.5.1.bn2.weight - 128 params\n",
            "✓ Trainable: image_branch.5.1.bn2.bias - 128 params\n",
            "✓ Trainable: image_branch.6.0.conv1.weight - 294912 params\n",
            "✓ Trainable: image_branch.6.0.bn1.weight - 256 params\n",
            "✓ Trainable: image_branch.6.0.bn1.bias - 256 params\n",
            "✓ Trainable: image_branch.6.0.conv2.weight - 589824 params\n",
            "✓ Trainable: image_branch.6.0.bn2.weight - 256 params\n",
            "✓ Trainable: image_branch.6.0.bn2.bias - 256 params\n",
            "✓ Trainable: image_branch.6.0.downsample.0.weight - 32768 params\n",
            "✓ Trainable: image_branch.6.0.downsample.1.weight - 256 params\n",
            "✓ Trainable: image_branch.6.0.downsample.1.bias - 256 params\n",
            "✓ Trainable: image_branch.6.1.conv1.weight - 589824 params\n",
            "✓ Trainable: image_branch.6.1.bn1.weight - 256 params\n",
            "✓ Trainable: image_branch.6.1.bn1.bias - 256 params\n",
            "✓ Trainable: image_branch.6.1.conv2.weight - 589824 params\n",
            "✓ Trainable: image_branch.6.1.bn2.weight - 256 params\n",
            "✓ Trainable: image_branch.6.1.bn2.bias - 256 params\n",
            "✓ Trainable: image_branch.7.0.conv1.weight - 1179648 params\n",
            "✓ Trainable: image_branch.7.0.bn1.weight - 512 params\n",
            "✓ Trainable: image_branch.7.0.bn1.bias - 512 params\n",
            "✓ Trainable: image_branch.7.0.conv2.weight - 2359296 params\n",
            "✓ Trainable: image_branch.7.0.bn2.weight - 512 params\n",
            "✓ Trainable: image_branch.7.0.bn2.bias - 512 params\n",
            "✓ Trainable: image_branch.7.0.downsample.0.weight - 131072 params\n",
            "✓ Trainable: image_branch.7.0.downsample.1.weight - 512 params\n",
            "✓ Trainable: image_branch.7.0.downsample.1.bias - 512 params\n",
            "✓ Trainable: image_branch.7.1.conv1.weight - 2359296 params\n",
            "✓ Trainable: image_branch.7.1.bn1.weight - 512 params\n",
            "✓ Trainable: image_branch.7.1.bn1.bias - 512 params\n",
            "✓ Trainable: image_branch.7.1.conv2.weight - 2359296 params\n",
            "✓ Trainable: image_branch.7.1.bn2.weight - 512 params\n",
            "✓ Trainable: image_branch.7.1.bn2.bias - 512 params\n",
            "✓ Trainable: hist_branch.0.weight - 288 params\n",
            "✓ Trainable: hist_branch.0.bias - 32 params\n",
            "✓ Trainable: hist_branch.1.weight - 32 params\n",
            "✓ Trainable: hist_branch.1.bias - 32 params\n",
            "✓ Trainable: hist_branch.4.weight - 18432 params\n",
            "✓ Trainable: hist_branch.4.bias - 64 params\n",
            "✓ Trainable: hist_branch.5.weight - 64 params\n",
            "✓ Trainable: hist_branch.5.bias - 64 params\n",
            "✓ Trainable: hist_branch.8.weight - 73728 params\n",
            "✓ Trainable: hist_branch.8.bias - 128 params\n",
            "✓ Trainable: hist_branch.9.weight - 128 params\n",
            "✓ Trainable: hist_branch.9.bias - 128 params\n",
            "✓ Trainable: fusion.0.weight - 163840 params\n",
            "✓ Trainable: fusion.0.bias - 256 params\n",
            "✓ Trainable: fusion.3.weight - 32768 params\n",
            "✓ Trainable: fusion.3.bias - 128 params\n",
            "✓ Trainable: fusion.6.weight - 384 params\n",
            "✓ Trainable: fusion.6.bias - 3 params\n",
            "Total params: 11,467,011\n",
            "Trainable params: 11,467,011 (100.0%)\n",
            "Data check:\n",
            "Images: torch.Size([1, 3, 768, 1024]), range: [0.003, 1.000]\n",
            "Hists: torch.Size([1, 100, 116]), range: [0.000, 1.000]\n",
            "WP true: torch.Size([1, 3]), range: [0.174, 0.509]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected 4D input (got 3D input)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1645922436.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Gradients detected - model can learn!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mdebug_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1645922436.py\u001b[0m in \u001b[0;36mdebug_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mwp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"WP pred: {wp_pred.shape}, range: [{wp_pred.min():.3f}, {wp_pred.max():.3f}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2173169673.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, hist)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Histogram features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mhist_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mhist_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# exponential_average_factor is set to self.momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"expected 4D input (got {input.dim()}D input)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_training():\n",
        "    model = AWB_DualStream(pretrained=True)\n",
        "    model.train()\n",
        "\n",
        "    # Проверка модели\n",
        "    check_model_trainability(model)\n",
        "\n",
        "    # Тестовый батч\n",
        "    test_batch = next(iter(train_loader))\n",
        "    images = test_batch['image'].to('cpu')\n",
        "    hists = test_batch['histogram'].to('cpu')\n",
        "    wp_true = test_batch['white_point'].to('cpu')\n",
        "\n",
        "    print(\"Data check:\")\n",
        "    print(f\"Images shape: {images.shape}\")  # Должно быть: (batch, 3, H, W)\n",
        "    print(f\"Hists shape: {hists.shape}\")    # Должно быть: (batch, 1, H, W)\n",
        "    print(f\"WP true shape: {wp_true.shape}\") # Должно быть: (batch, 3)\n",
        "\n",
        "    # Проверяем и исправляем размерности\n",
        "    if images.dim() == 3:\n",
        "        print(\"⚠️  Images are 3D, adding batch dimension\")\n",
        "        images = images.unsqueeze(0)  # (C, H, W) -> (1, C, H, W)\n",
        "\n",
        "    if hists.dim() == 3:\n",
        "        print(\"⚠️  Hists are 3D, adding batch and channel dimensions\")\n",
        "        hists = hists.unsqueeze(0).unsqueeze(0)  # (H, W) -> (1, 1, H, W)\n",
        "    elif hists.dim() == 2:\n",
        "        print(\"⚠️  Hists are 2D, adding batch and channel dimensions\")\n",
        "        hists = hists.unsqueeze(0).unsqueeze(0)  # (H, W) -> (1, 1, H, W)\n",
        "    if hists.dim() == 5:\n",
        "      hists = hists.squeeze(0)\n",
        "    print(f\"Fixed images shape: {images.shape}\")\n",
        "    print(f\"Fixed hists shape: {hists.shape}\")\n",
        "\n",
        "    # Forward\n",
        "    wp_pred = model(images, hists)\n",
        "    print(f\"WP pred: {wp_pred.shape}, range: [{wp_pred.min():.3f}, {wp_pred.max():.3f}]\")\n",
        "\n",
        "    # Loss\n",
        "    criterion = CombinedLoss()\n",
        "    loss = criterion(wp_pred, wp_true)\n",
        "    print(f\"Initial loss: {loss.item()}\")\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradients\n",
        "    has_gradients = False\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None and param.grad.norm() > 0:\n",
        "            has_gradients = True\n",
        "            print(f\"✓ Gradient in {name}: {param.grad.norm().item()}\")\n",
        "            break\n",
        "\n",
        "    if not has_gradients:\n",
        "        print(\"❌ NO GRADIENTS DETECTED!\")\n",
        "    else:\n",
        "        print(\"✅ Gradients detected - model can learn!\")\n",
        "debug_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfnApdOwmBd3",
        "outputId": "6af1618a-3c15-490b-b4fe-e7ea9650a38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[-0.1793, -0.2046, -0.1567]])\n",
            "Output range: [-0.205, -0.157]\n",
            "✓ Trainable: image_branch.0.weight - 9408 params\n",
            "✓ Trainable: image_branch.1.weight - 64 params\n",
            "✓ Trainable: image_branch.1.bias - 64 params\n",
            "✓ Trainable: image_branch.4.0.conv1.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.0.bn1.weight - 64 params\n",
            "✓ Trainable: image_branch.4.0.bn1.bias - 64 params\n",
            "✓ Trainable: image_branch.4.0.conv2.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.0.bn2.weight - 64 params\n",
            "✓ Trainable: image_branch.4.0.bn2.bias - 64 params\n",
            "✓ Trainable: image_branch.4.1.conv1.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.1.bn1.weight - 64 params\n",
            "✓ Trainable: image_branch.4.1.bn1.bias - 64 params\n",
            "✓ Trainable: image_branch.4.1.conv2.weight - 36864 params\n",
            "✓ Trainable: image_branch.4.1.bn2.weight - 64 params\n",
            "✓ Trainable: image_branch.4.1.bn2.bias - 64 params\n",
            "✓ Trainable: image_branch.5.0.conv1.weight - 73728 params\n",
            "✓ Trainable: image_branch.5.0.bn1.weight - 128 params\n",
            "✓ Trainable: image_branch.5.0.bn1.bias - 128 params\n",
            "✓ Trainable: image_branch.5.0.conv2.weight - 147456 params\n",
            "✓ Trainable: image_branch.5.0.bn2.weight - 128 params\n",
            "✓ Trainable: image_branch.5.0.bn2.bias - 128 params\n",
            "✓ Trainable: image_branch.5.0.downsample.0.weight - 8192 params\n",
            "✓ Trainable: image_branch.5.0.downsample.1.weight - 128 params\n",
            "✓ Trainable: image_branch.5.0.downsample.1.bias - 128 params\n",
            "✓ Trainable: image_branch.5.1.conv1.weight - 147456 params\n",
            "✓ Trainable: image_branch.5.1.bn1.weight - 128 params\n",
            "✓ Trainable: image_branch.5.1.bn1.bias - 128 params\n",
            "✓ Trainable: image_branch.5.1.conv2.weight - 147456 params\n",
            "✓ Trainable: image_branch.5.1.bn2.weight - 128 params\n",
            "✓ Trainable: image_branch.5.1.bn2.bias - 128 params\n",
            "✓ Trainable: image_branch.6.0.conv1.weight - 294912 params\n",
            "✓ Trainable: image_branch.6.0.bn1.weight - 256 params\n",
            "✓ Trainable: image_branch.6.0.bn1.bias - 256 params\n",
            "✓ Trainable: image_branch.6.0.conv2.weight - 589824 params\n",
            "✓ Trainable: image_branch.6.0.bn2.weight - 256 params\n",
            "✓ Trainable: image_branch.6.0.bn2.bias - 256 params\n",
            "✓ Trainable: image_branch.6.0.downsample.0.weight - 32768 params\n",
            "✓ Trainable: image_branch.6.0.downsample.1.weight - 256 params\n",
            "✓ Trainable: image_branch.6.0.downsample.1.bias - 256 params\n",
            "✓ Trainable: image_branch.6.1.conv1.weight - 589824 params\n",
            "✓ Trainable: image_branch.6.1.bn1.weight - 256 params\n",
            "✓ Trainable: image_branch.6.1.bn1.bias - 256 params\n",
            "✓ Trainable: image_branch.6.1.conv2.weight - 589824 params\n",
            "✓ Trainable: image_branch.6.1.bn2.weight - 256 params\n",
            "✓ Trainable: image_branch.6.1.bn2.bias - 256 params\n",
            "✓ Trainable: image_branch.7.0.conv1.weight - 1179648 params\n",
            "✓ Trainable: image_branch.7.0.bn1.weight - 512 params\n",
            "✓ Trainable: image_branch.7.0.bn1.bias - 512 params\n",
            "✓ Trainable: image_branch.7.0.conv2.weight - 2359296 params\n",
            "✓ Trainable: image_branch.7.0.bn2.weight - 512 params\n",
            "✓ Trainable: image_branch.7.0.bn2.bias - 512 params\n",
            "✓ Trainable: image_branch.7.0.downsample.0.weight - 131072 params\n",
            "✓ Trainable: image_branch.7.0.downsample.1.weight - 512 params\n",
            "✓ Trainable: image_branch.7.0.downsample.1.bias - 512 params\n",
            "✓ Trainable: image_branch.7.1.conv1.weight - 2359296 params\n",
            "✓ Trainable: image_branch.7.1.bn1.weight - 512 params\n",
            "✓ Trainable: image_branch.7.1.bn1.bias - 512 params\n",
            "✓ Trainable: image_branch.7.1.conv2.weight - 2359296 params\n",
            "✓ Trainable: image_branch.7.1.bn2.weight - 512 params\n",
            "✓ Trainable: image_branch.7.1.bn2.bias - 512 params\n",
            "✓ Trainable: hist_branch.0.weight - 288 params\n",
            "✓ Trainable: hist_branch.0.bias - 32 params\n",
            "✓ Trainable: hist_branch.1.weight - 32 params\n",
            "✓ Trainable: hist_branch.1.bias - 32 params\n",
            "✓ Trainable: hist_branch.4.weight - 18432 params\n",
            "✓ Trainable: hist_branch.4.bias - 64 params\n",
            "✓ Trainable: hist_branch.5.weight - 64 params\n",
            "✓ Trainable: hist_branch.5.bias - 64 params\n",
            "✓ Trainable: hist_branch.8.weight - 73728 params\n",
            "✓ Trainable: hist_branch.8.bias - 128 params\n",
            "✓ Trainable: hist_branch.9.weight - 128 params\n",
            "✓ Trainable: hist_branch.9.bias - 128 params\n",
            "✓ Trainable: fusion.0.weight - 163840 params\n",
            "✓ Trainable: fusion.0.bias - 256 params\n",
            "✓ Trainable: fusion.3.weight - 32768 params\n",
            "✓ Trainable: fusion.3.bias - 128 params\n",
            "✓ Trainable: fusion.6.weight - 384 params\n",
            "✓ Trainable: fusion.6.bias - 3 params\n",
            "Total params: 11,467,011\n",
            "Trainable params: 11,467,011 (100.0%)\n",
            "Data check:\n",
            "Images shape: torch.Size([1, 3, 768, 1024])\n",
            "Hists shape: torch.Size([1, 100, 116])\n",
            "WP true shape: torch.Size([1, 3])\n",
            "⚠️  Hists are 3D, adding batch and channel dimensions\n",
            "Fixed images shape: torch.Size([1, 3, 768, 1024])\n",
            "Fixed hists shape: torch.Size([1, 1, 100, 116])\n",
            "WP pred: torch.Size([1, 3]), range: [-0.185, 0.090]\n",
            "Initial loss: 41.514259338378906\n",
            "✓ Gradient in image_branch.0.weight: 16.85614776611328\n",
            "✅ Gradients detected - model can learn!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    \"\"\"Датасет для тестовых изображений\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, images_dir, img_size=224):\n",
        "        self.csv_path = Path(csv_path)\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Загрузка CSV файла\n",
        "        self.df = pd.read_csv(self.csv_path)\n",
        "        print(f\"Loaded test CSV: {len(self.df)} samples\")\n",
        "        print(f\"Columns: {list(self.df.columns)}\")\n",
        "\n",
        "        # Параметры нормализации (должны совпадать с тренировочными)\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _read_image(self, image_path):\n",
        "        \"\"\"Чтение и предобработка изображения\"\"\"\n",
        "        try:\n",
        "            img = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)\n",
        "            if img is None:\n",
        "                img = cv2.imread(str(image_path), cv2.IMREAD_COLOR)\n",
        "\n",
        "            if img is None:\n",
        "                raise ValueError(f\"Cannot read image: {image_path}\")\n",
        "\n",
        "            # Конвертируем в float и нормализуем\n",
        "            if img.dtype == np.uint16:\n",
        "                img = img.astype(np.float32) / 65535.0\n",
        "            elif img.dtype == np.uint8:\n",
        "                img = img.astype(np.float32) / 255.0\n",
        "\n",
        "            # BGR to RGB\n",
        "            if img.shape[-1] == 3:\n",
        "                img = img[..., ::-1]\n",
        "\n",
        "            # Resize\n",
        "            if img.shape[0] != self.img_size or img.shape[1] != self.img_size:\n",
        "                img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "\n",
        "            return img\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading image {image_path}: {e}\")\n",
        "            return np.ones((self.img_size, self.img_size, 3), dtype=np.float32) * 0.5\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            row = self.df.iloc[idx]\n",
        "\n",
        "            # Получаем путь к изображению (предполагаем, что первый столбец)\n",
        "            image_path_str = row.iloc[0]  # Первая колонка содержит пути\n",
        "            if pd.isna(image_path_str):\n",
        "                image_path_str = f\"test_image_{idx:04d}.png\"\n",
        "\n",
        "            # Создаем полный путь\n",
        "            image_path = self.images_dir / image_path_str\n",
        "\n",
        "            # Чтение изображения\n",
        "            image = self._read_image(image_path)\n",
        "\n",
        "            # Преобразование в tensor и нормализация\n",
        "            image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
        "            image_tensor = (image_tensor - self.mean) / self.std\n",
        "\n",
        "            return image_tensor, str(image_path_str)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {idx}: {e}\")\n",
        "            # Возвращаем dummy данные\n",
        "            dummy_image = torch.randn(3, self.img_size, self.img_size)\n",
        "            return dummy_image, f\"error_{idx}.png\"\n",
        "\n",
        "class WhiteBalanceModel(nn.Module):\n",
        "    \"\"\"Модель для предсказания точки белого\"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=False):\n",
        "        super(WhiteBalanceModel, self).__init__()\n",
        "\n",
        "        # Базовая архитектура (должна совпадать с обученной моделью)\n",
        "        from torchvision.models import efficientnet_b0\n",
        "\n",
        "        if pretrained:\n",
        "            from torchvision.models import EfficientNet_B0_Weights\n",
        "            self.backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        else:\n",
        "            self.backbone = efficientnet_b0(weights=None)\n",
        "\n",
        "        # Заменяем классификатор\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "def load_model(model_path, device, pretrained=False):\n",
        "    \"\"\"Загрузка обученной модели\"\"\"\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "\n",
        "    model = WhiteBalanceModel(pretrained=pretrained).to(device)\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            # Пробуем загрузить полный checkpoint или только веса\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(\"Loaded from checkpoint\")\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint)\n",
        "                print(\"Loaded model weights\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            print(\"Using randomly initialized model\")\n",
        "    else:\n",
        "        print(f\"Model file not found: {model_path}\")\n",
        "        print(\"Using randomly initialized model\")\n",
        "\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def create_predictions(model, test_loader, device):\n",
        "    \"\"\"Создание предсказаний для тестового набора\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_image_paths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, image_paths in tqdm(test_loader, desc=\"Making predictions\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Денормализуем предсказания из [0, 1] в [0, 65535]\n",
        "            predictions = outputs.cpu().numpy() * 65535.0\n",
        "\n",
        "            all_predictions.extend(predictions)\n",
        "            all_image_paths.extend(image_paths)\n",
        "\n",
        "    return all_image_paths, all_predictions\n",
        "\n",
        "def create_submission_file(image_paths, predictions, output_csv=\"submission.csv\"):\n",
        "    \"\"\"Создание submission файла в правильном формате\"\"\"\n",
        "    print(f\"Creating submission file: {output_csv}\")\n",
        "\n",
        "    data = []\n",
        "    for img_path, pred in zip(image_paths, predictions):\n",
        "        try:\n",
        "            # Извлекаем только имя файла (без пути)\n",
        "            if isinstance(img_path, str):\n",
        "                filename = Path(img_path).name\n",
        "            else:\n",
        "                filename = f\"image_{len(data):04d}.png\"\n",
        "\n",
        "            # Обеспечиваем корректный диапазон значений [0, 65535]\n",
        "            wp_r = float(np.clip(pred[0], 0, 65535))\n",
        "            wp_g = float(np.clip(pred[1], 0, 65535))\n",
        "            wp_b = float(np.clip(pred[2], 0, 65535))\n",
        "\n",
        "            data.append({\n",
        "                'image_path': filename,\n",
        "                'wp_r': wp_r,\n",
        "                'wp_g': wp_g,\n",
        "                'wp_b': wp_b\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "            # Добавляем значения по умолчанию\n",
        "            data.append({\n",
        "                'image_path': f\"error_{len(data):04d}.png\",\n",
        "                'wp_r': 32768.0,\n",
        "                'wp_g': 32768.0,\n",
        "                'wp_b': 32768.0\n",
        "            })\n",
        "\n",
        "    # Создаем DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Сохраняем CSV\n",
        "    df.to_csv(output_csv, index=False, float_format='%.6f')\n",
        "\n",
        "    print(f\"Submission file created with {len(df)} predictions\")\n",
        "    print(\"First 5 predictions:\")\n",
        "    print(df.head())\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Основная функция для создания предсказаний\"\"\"\n",
        "\n",
        "    # Пути к данным\n",
        "    TEST_CSV_PATH = \"/content/test.csv\"\n",
        "    TEST_IMAGES_DIR = \"/content/test_imgs\"\n",
        "    MODEL_PATH = \"/content/best_awb_model.pth\"\n",
        "    OUTPUT_CSV = \"/content/final_submission.csv\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"🔄 CREATING PREDICTIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Проверка файлов\n",
        "    print(\"🔍 Checking files...\")\n",
        "    print(f\"Test CSV: {TEST_CSV_PATH} → {'✅' if os.path.exists(TEST_CSV_PATH) else '❌'}\")\n",
        "    print(f\"Test images: {TEST_IMAGES_DIR} → {'✅' if os.path.exists(TEST_IMAGES_DIR) else '❌'}\")\n",
        "    print(f\"Model: {MODEL_PATH} → {'✅' if os.path.exists(MODEL_PATH) else '❌'}\")\n",
        "\n",
        "    if not os.path.exists(TEST_CSV_PATH):\n",
        "        print(\"❌ Test CSV not found!\")\n",
        "        return\n",
        "\n",
        "    # Устройство\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"🖥️  Device: {device}\")\n",
        "\n",
        "    # Создаем тестовый датасет\n",
        "    print(\"\\n📁 Creating test dataset...\")\n",
        "    try:\n",
        "        test_dataset = TestDataset(TEST_CSV_PATH, TEST_IMAGES_DIR)\n",
        "        print(f\"✅ Test dataset created: {len(test_dataset)} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    # DataLoader\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # Загрузка модели\n",
        "    print(\"\\n🤖 Loading model...\")\n",
        "    model = load_model(MODEL_PATH, device, pretrained=False)\n",
        "\n",
        "    # Создание предсказаний\n",
        "    print(\"\\n🎯 Making predictions...\")\n",
        "    image_paths, predictions = create_predictions(model, test_loader, device)\n",
        "\n",
        "    # Создание submission файла\n",
        "    print(\"\\n💾 Creating submission file...\")\n",
        "    submission_df = create_submission_file(image_paths, predictions, OUTPUT_CSV)\n",
        "\n",
        "    # Статистика\n",
        "    print(\"\\n📊 Prediction statistics:\")\n",
        "    pred_array = np.array(predictions)\n",
        "    print(f\"Total predictions: {len(predictions)}\")\n",
        "    print(f\"Value ranges:\")\n",
        "    print(f\"  R: {pred_array[:, 0].min():.1f} - {pred_array[:, 0].max():.1f}\")\n",
        "    print(f\"  G: {pred_array[:, 1].min():.1f} - {pred_array[:, 1].max():.1f}\")\n",
        "    print(f\"  B: {pred_array[:, 2].min():.1f} - {pred_array[:, 2].max():.1f}\")\n",
        "\n",
        "    # Проверка корректности значений\n",
        "    valid_mask = np.all((pred_array >= 0) & (pred_array <= 65535), axis=1)\n",
        "    invalid_count = np.sum(~valid_mask)\n",
        "\n",
        "    if invalid_count > 0:\n",
        "        print(f\"⚠️  Warning: {invalid_count} predictions outside valid range [0, 65535]\")\n",
        "    else:\n",
        "        print(\"✅ All predictions are within valid range\")\n",
        "\n",
        "    print(f\"\\n🎉 Done! Submission file saved to: {OUTPUT_CSV}\")\n",
        "\n",
        "# Альтернативная простая версия\n",
        "def quick_predict():\n",
        "    \"\"\"Быстрое создание предсказаний\"\"\"\n",
        "\n",
        "    # Пути по умолчанию\n",
        "    paths = {\n",
        "        'test_csv': '/content/test (3).csv',\n",
        "        'test_images': '/content/test_imgs',\n",
        "        'model': '/content/final_model.pth',\n",
        "        'output': '/content/submission (1).csv'\n",
        "    }\n",
        "\n",
        "    # Проверяем существование файлов\n",
        "    for name, path in paths.items():\n",
        "        if not os.path.exists(path) and name != 'output':\n",
        "            print(f\"❌ File not found: {path}\")\n",
        "            return\n",
        "\n",
        "    # Запускаем основной процесс\n",
        "    main()\n",
        "\n",
        "# Утилиты для проверки\n",
        "def check_submission_format():\n",
        "    \"\"\"Проверяет формат submission файла\"\"\"\n",
        "    try:\n",
        "        # Пример правильного формата\n",
        "        example_data = {\n",
        "            'image_path': ['test1.png', 'test2.png'],\n",
        "            'wp_r': [30000.0, 32000.0],\n",
        "            'wp_g': [31000.0, 33000.0],\n",
        "            'wp_b': [29000.0, 28000.0]\n",
        "        }\n",
        "\n",
        "        example_df = pd.DataFrame(example_data)\n",
        "        print(\"📋 Example submission format:\")\n",
        "        print(example_df)\n",
        "        print(\"\\n✅ Columns should be: image_path, wp_r, wp_g, wp_b\")\n",
        "        print(\"✅ Values should be in range [0, 65535]\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Показываем пример формата\n",
        "    check_submission_format()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    # Запускаем создание предсказаний\n",
        "    main()\n",
        "\n",
        "    # Или быстрый запуск"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYMYcwwDsfTS",
        "outputId": "ee0bb195-2ab6-476c-bbcb-55972b63cfec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Example submission format:\n",
            "  image_path     wp_r     wp_g     wp_b\n",
            "0  test1.png  30000.0  31000.0  29000.0\n",
            "1  test2.png  32000.0  33000.0  28000.0\n",
            "\n",
            "✅ Columns should be: image_path, wp_r, wp_g, wp_b\n",
            "✅ Values should be in range [0, 65535]\n",
            "\n",
            "============================================================\n",
            "============================================================\n",
            "🔄 CREATING PREDICTIONS\n",
            "============================================================\n",
            "🔍 Checking files...\n",
            "Test CSV: /content/test.csv → ✅\n",
            "Test images: /content/test_imgs → ✅\n",
            "Model: /content/best_awb_model.pth → ✅\n",
            "🖥️  Device: cpu\n",
            "\n",
            "📁 Creating test dataset...\n",
            "Loaded test CSV: 145 samples\n",
            "Columns: ['names']\n",
            "✅ Test dataset created: 145 samples\n",
            "\n",
            "🤖 Loading model...\n",
            "Loading model from: /content/best_awb_model.pth\n",
            "Error loading model: Error(s) in loading state_dict for WhiteBalanceModel:\n",
            "\tMissing key(s) in state_dict: \"backbone.features.0.0.weight\", \"backbone.features.0.1.weight\", \"backbone.features.0.1.bias\", \"backbone.features.0.1.running_mean\", \"backbone.features.0.1.running_var\", \"backbone.features.1.0.block.0.0.weight\", \"backbone.features.1.0.block.0.1.weight\", \"backbone.features.1.0.block.0.1.bias\", \"backbone.features.1.0.block.0.1.running_mean\", \"backbone.features.1.0.block.0.1.running_var\", \"backbone.features.1.0.block.1.fc1.weight\", \"backbone.features.1.0.block.1.fc1.bias\", \"backbone.features.1.0.block.1.fc2.weight\", \"backbone.features.1.0.block.1.fc2.bias\", \"backbone.features.1.0.block.2.0.weight\", \"backbone.features.1.0.block.2.1.weight\", \"backbone.features.1.0.block.2.1.bias\", \"backbone.features.1.0.block.2.1.running_mean\", \"backbone.features.1.0.block.2.1.running_var\", \"backbone.features.2.0.block.0.0.weight\", \"backbone.features.2.0.block.0.1.weight\", \"backbone.features.2.0.block.0.1.bias\", \"backbone.features.2.0.block.0.1.running_mean\", \"backbone.features.2.0.block.0.1.running_var\", \"backbone.features.2.0.block.1.0.weight\", \"backbone.features.2.0.block.1.1.weight\", \"backbone.features.2.0.block.1.1.bias\", \"backbone.features.2.0.block.1.1.running_mean\", \"backbone.features.2.0.block.1.1.running_var\", \"backbone.features.2.0.block.2.fc1.weight\", \"backbone.features.2.0.block.2.fc1.bias\", \"backbone.features.2.0.block.2.fc2.weight\", \"backbone.features.2.0.block.2.fc2.bias\", \"backbone.features.2.0.block.3.0.weight\", \"backbone.features.2.0.block.3.1.weight\", \"backbone.features.2.0.block.3.1.bias\", \"backbone.features.2.0.block.3.1.running_mean\", \"backbone.features.2.0.block.3.1.running_var\", \"backbone.features.2.1.block.0.0.weight\", \"backbone.features.2.1.block.0.1.weight\", \"backbone.features.2.1.block.0.1.bias\", \"backbone.features.2.1.block.0.1.running_mean\", \"backbone.features.2.1.block.0.1.running_var\", \"backbone.features.2.1.block.1.0.weight\", \"backbone.features.2.1.block.1.1.weight\", \"backbone.features.2.1.block.1.1.bias\", \"backbone.features.2.1.block.1.1.running_mean\", \"backbone.features.2.1.block.1.1.running_var\", \"backbone.features.2.1.block.2.fc1.weight\", \"backbone.features.2.1.block.2.fc1.bias\", \"backbone.features.2.1.block.2.fc2.weight\", \"backbone.features.2.1.block.2.fc2.bias\", \"backbone.features.2.1.block.3.0.weight\", \"backbone.features.2.1.block.3.1.weight\", \"backbone.features.2.1.block.3.1.bias\", \"backbone.features.2.1.block.3.1.running_mean\", \"backbone.features.2.1.block.3.1.running_var\", \"backbone.features.3.0.block.0.0.weight\", \"backbone.features.3.0.block.0.1.weight\", \"backbone.features.3.0.block.0.1.bias\", \"backbone.features.3.0.block.0.1.running_mean\", \"backbone.features.3.0.block.0.1.running_var\", \"backbone.features.3.0.block.1.0.weight\", \"backbone.features.3.0.block.1.1.weight\", \"backbone.features.3.0.block.1.1.bias\", \"backbone.features.3.0.block.1.1.running_mean\", \"backbone.features.3.0.block.1.1.running_var\", \"backbone.features.3.0.block.2.fc1.weight\", \"backbone.features.3.0.block.2.fc1.bias\", \"backbone.features.3.0.block.2.fc2.weight\", \"backbone.features.3.0.block.2.fc2.bias\", \"backbone.features.3.0.block.3.0.weight\", \"backbone.features.3.0.block.3.1.weight\", \"backbone.features.3.0.block.3.1.bias\", \"backbone.features.3.0.block.3.1.running_mean\", \"backbone.features.3.0.block.3.1.running_var\", \"backbone.features.3.1.block.0.0.weight\", \"backbone.features.3.1.block.0.1.weight\", \"backbone.features.3.1.block.0.1.bias\", \"backbone.features.3.1.block.0.1.running_mean\", \"backbone.features.3.1.block.0.1.running_var\", \"backbone.features.3.1.block.1.0.weight\", \"backbone.features.3.1.block.1.1.weight\", \"backbone.features.3.1.block.1.1.bias\", \"backbone.features.3.1.block.1.1.running_mean\", \"backbone.features.3.1.block.1.1.running_var\", \"backbone.features.3.1.block.2.fc1.weight\", \"backbone.features.3.1.block.2.fc1.bias\", \"backbone.features.3.1.block.2.fc2.weight\", \"backbone.features.3.1.block.2.fc2.bias\", \"backbone.features.3.1.block.3.0.weight\", \"backbone.features.3.1.block.3.1.weight\", \"backbone.features.3.1.block.3.1.bias\", \"backbone.features.3.1.block.3.1.running_mean\", \"backbone.features.3.1.block.3.1.running_var\", \"backbone.features.4.0.block.0.0.weight\", \"backbone.features.4.0.block.0.1.weight\", \"backbone.features.4.0.block.0.1.bias\", \"backbone.features.4.0.block.0.1.running_mean\", \"backbone.features.4.0.block.0.1.running_var\", \"backbone.features.4.0.block.1.0.weight\", \"backbone.features.4.0.block.1.1.weight\", \"backbone.features.4.0.block.1.1.bias\", \"backbone.features.4.0.block.1.1.running_mean\", \"backbone.features.4.0.block.1.1.running_var\", \"backbone.features.4.0.block.2.fc1.weight\", \"backbone.features.4.0.block.2.fc1.bias\", \"backbone.features.4.0.block.2.fc2.weight\", \"backbone.features.4.0.block.2.fc2.bias\", \"backbone.features.4.0.block.3.0.weight\", \"backbone.features.4.0.block.3.1.weight\", \"backbone.features.4.0.block.3.1.bias\", \"backbone.features.4.0.block.3.1.running_mean\", \"backbone.features.4.0.block.3.1.running_var\", \"backbone.features.4.1.block.0.0.weight\", \"backbone.features.4.1.block.0.1.weight\", \"backbone.features.4.1.block.0.1.bias\", \"backbone.features.4.1.block.0.1.running_mean\", \"backbone.features.4.1.block.0.1.running_var\", \"backbone.features.4.1.block.1.0.weight\", \"backbone.features.4.1.block.1.1.weight\", \"backbone.features.4.1.block.1.1.bias\", \"backbone.features.4.1.block.1.1.running_mean\", \"backbone.features.4.1.block.1.1.running_var\", \"backbone.features.4.1.block.2.fc1.weight\", \"backbone.features.4.1.block.2.fc1.bias\", \"backbone.features.4.1.block.2.fc2.weight\", \"backbone.features.4.1.block.2.fc2.bias\", \"backbone.features.4.1.block.3.0.weight\", \"backbone.features.4.1.block.3.1.weight\", \"backbone.features.4.1.block.3.1.bias\", \"backbone.features.4.1.block.3.1.running_mean\", \"backbone.features.4.1.block.3.1.running_var\", \"backbone.features.4.2.block.0.0.weight\", \"backbone.features.4.2.block.0.1.weight\", \"backbone.features.4.2.block.0.1.bias\", \"backbone.features.4.2.block.0.1.running_mean\", \"backbone.features.4.2.block.0.1.running_var\", \"backbone.features.4.2.block.1.0.weight\", \"backbone.features.4.2.block.1.1.weight\", \"backbone.features.4.2.block.1.1.bias\", \"backbone.features.4.2.block.1.1.running_mean\", \"backbone.features.4.2.block.1.1.running_var\", \"backbone.features.4.2.block.2.fc1.weight\", \"backbone.features.4.2.block.2.fc1.bias\", \"backbone.features.4.2.block.2.fc2.weight\", \"backbone.features.4.2.block.2.fc2.bias\", \"backbone.features.4.2.block.3.0.weight\", \"backbone.features.4.2.block.3.1.weight\", \"backbone.features.4.2.block.3.1.bias\", \"backbone.features.4.2.block.3.1.running_mean\", \"backbone.features.4.2.block.3.1.running_var\", \"backbone.features.5.0.block.0.0.weight\", \"backbone.features.5.0.block.0.1.weight\", \"backbone.features.5.0.block.0.1.bias\", \"backbone.features.5.0.block.0.1.running_mean\", \"backbone.features.5.0.block.0.1.running_var\", \"backbone.features.5.0.block.1.0.weight\", \"backbone.features.5.0.block.1.1.weight\", \"backbone.features.5.0.block.1.1.bias\", \"backbone.features.5.0.block.1.1.running_mean\", \"backbone.features.5.0.block.1.1.running_var\", \"backbone.features.5.0.block.2.fc1.weight\", \"backbone.features.5.0.block.2.fc1.bias\", \"backbone.features.5.0.block.2.fc2.weight\", \"backbone.features.5.0.block.2.fc2.bias\", \"backbone.features.5.0.block.3.0.weight\", \"backbone.features.5.0.block.3.1.weight\", \"backbone.features.5.0.block.3.1.bias\", \"backbone.features.5.0.block.3.1.running_mean\", \"backbone.features.5.0.block.3.1.running_var\", \"backbone.features.5.1.block.0.0.weight\", \"backbone.features.5.1.block.0.1.weight\", \"backbone.features.5.1.block.0.1.bias\", \"backbone.features.5.1.block.0.1.running_mean\", \"backbone.features.5.1.block.0.1.running_var\", \"backbone.features.5.1.block.1.0.weight\", \"backbone.features.5.1.block.1.1.weight\", \"backbone.features.5.1.block.1.1.bias\", \"backbone.features.5.1.block.1.1.running_mean\", \"backbone.features.5.1.block.1.1.running_var\", \"backbone.features.5.1.block.2.fc1.weight\", \"backbone.features.5.1.block.2.fc1.bias\", \"backbone.features.5.1.block.2.fc2.weight\", \"backbone.features.5.1.block.2.fc2.bias\", \"backbone.features.5.1.block.3.0.weight\", \"backbone.features.5.1.block.3.1.weight\", \"backbone.features.5.1.block.3.1.bias\", \"backbone.features.5.1.block.3.1.running_mean\", \"backbone.features.5.1.block.3.1.running_var\", \"backbone.features.5.2.block.0.0.weight\", \"backbone.features.5.2.block.0.1.weight\", \"backbone.features.5.2.block.0.1.bias\", \"backbone.features.5.2.block.0.1.running_mean\", \"backbone.features.5.2.block.0.1.running_var\", \"backbone.features.5.2.block.1.0.weight\", \"backbone.features.5.2.block.1.1.weight\", \"backbone.features.5.2.block.1.1.bias\", \"backbone.features.5.2.block.1.1.running_mean\", \"backbone.features.5.2.block.1.1.running_var\", \"backbone.features.5.2.block.2.fc1.weight\", \"backbone.features.5.2.block.2.fc1.bias\", \"backbone.features.5.2.block.2.fc2.weight\", \"backbone.features.5.2.block.2.fc2.bias\", \"backbone.features.5.2.block.3.0.weight\", \"backbone.features.5.2.block.3.1.weight\", \"backbone.features.5.2.block.3.1.bias\", \"backbone.features.5.2.block.3.1.running_mean\", \"backbone.features.5.2.block.3.1.running_var\", \"backbone.features.6.0.block.0.0.weight\", \"backbone.features.6.0.block.0.1.weight\", \"backbone.features.6.0.block.0.1.bias\", \"backbone.features.6.0.block.0.1.running_mean\", \"backbone.features.6.0.block.0.1.running_var\", \"backbone.features.6.0.block.1.0.weight\", \"backbone.features.6.0.block.1.1.weight\", \"backbone.features.6.0.block.1.1.bias\", \"backbone.features.6.0.block.1.1.running_mean\", \"backbone.features.6.0.block.1.1.running_var\", \"backbone.features.6.0.block.2.fc1.weight\", \"backbone.features.6.0.block.2.fc1.bias\", \"backbone.features.6.0.block.2.fc2.weight\", \"backbone.features.6.0.block.2.fc2.bias\", \"backbone.features.6.0.block.3.0.weight\", \"backbone.features.6.0.block.3.1.weight\", \"backbone.features.6.0.block.3.1.bias\", \"backbone.features.6.0.block.3.1.running_mean\", \"backbone.features.6.0.block.3.1.running_var\", \"backbone.features.6.1.block.0.0.weight\", \"backbone.features.6.1.block.0.1.weight\", \"backbone.features.6.1.block.0.1.bias\", \"backbone.features.6.1.block.0.1.running_mean\", \"backbone.features.6.1.block.0.1.running_var\", \"backbone.features.6.1.block.1.0.weight\", \"backbone.features.6.1.block.1.1.weight\", \"backbone.features.6.1.block.1.1.bias\", \"backbone.features.6.1.block.1.1.running_mean\", \"backbone.features.6.1.block.1.1.running_var\", \"backbone.features.6.1.block.2.fc1.weight\", \"backbone.features.6.1.block.2.fc1.bias\", \"backbone.features.6.1.block.2.fc2.weight\", \"backbone.features.6.1.block.2.fc2.bias\", \"backbone.features.6.1.block.3.0.weight\", \"backbone.features.6.1.block.3.1.weight\", \"backbone.features.6.1.block.3.1.bias\", \"backbone.features.6.1.block.3.1.running_mean\", \"backbone.features.6.1.block.3.1.running_var\", \"backbone.features.6.2.block.0.0.weight\", \"backbone.features.6.2.block.0.1.weight\", \"backbone.features.6.2.block.0.1.bias\", \"backbone.features.6.2.block.0.1.running_mean\", \"backbone.features.6.2.block.0.1.running_var\", \"backbone.features.6.2.block.1.0.weight\", \"backbone.features.6.2.block.1.1.weight\", \"backbone.features.6.2.block.1.1.bias\", \"backbone.features.6.2.block.1.1.running_mean\", \"backbone.features.6.2.block.1.1.running_var\", \"backbone.features.6.2.block.2.fc1.weight\", \"backbone.features.6.2.block.2.fc1.bias\", \"backbone.features.6.2.block.2.fc2.weight\", \"backbone.features.6.2.block.2.fc2.bias\", \"backbone.features.6.2.block.3.0.weight\", \"backbone.features.6.2.block.3.1.weight\", \"backbone.features.6.2.block.3.1.bias\", \"backbone.features.6.2.block.3.1.running_mean\", \"backbone.features.6.2.block.3.1.running_var\", \"backbone.features.6.3.block.0.0.weight\", \"backbone.features.6.3.block.0.1.weight\", \"backbone.features.6.3.block.0.1.bias\", \"backbone.features.6.3.block.0.1.running_mean\", \"backbone.features.6.3.block.0.1.running_var\", \"backbone.features.6.3.block.1.0.weight\", \"backbone.features.6.3.block.1.1.weight\", \"backbone.features.6.3.block.1.1.bias\", \"backbone.features.6.3.block.1.1.running_mean\", \"backbone.features.6.3.block.1.1.running_var\", \"backbone.features.6.3.block.2.fc1.weight\", \"backbone.features.6.3.block.2.fc1.bias\", \"backbone.features.6.3.block.2.fc2.weight\", \"backbone.features.6.3.block.2.fc2.bias\", \"backbone.features.6.3.block.3.0.weight\", \"backbone.features.6.3.block.3.1.weight\", \"backbone.features.6.3.block.3.1.bias\", \"backbone.features.6.3.block.3.1.running_mean\", \"backbone.features.6.3.block.3.1.running_var\", \"backbone.features.7.0.block.0.0.weight\", \"backbone.features.7.0.block.0.1.weight\", \"backbone.features.7.0.block.0.1.bias\", \"backbone.features.7.0.block.0.1.running_mean\", \"backbone.features.7.0.block.0.1.running_var\", \"backbone.features.7.0.block.1.0.weight\", \"backbone.features.7.0.block.1.1.weight\", \"backbone.features.7.0.block.1.1.bias\", \"backbone.features.7.0.block.1.1.running_mean\", \"backbone.features.7.0.block.1.1.running_var\", \"backbone.features.7.0.block.2.fc1.weight\", \"backbone.features.7.0.block.2.fc1.bias\", \"backbone.features.7.0.block.2.fc2.weight\", \"backbone.features.7.0.block.2.fc2.bias\", \"backbone.features.7.0.block.3.0.weight\", \"backbone.features.7.0.block.3.1.weight\", \"backbone.features.7.0.block.3.1.bias\", \"backbone.features.7.0.block.3.1.running_mean\", \"backbone.features.7.0.block.3.1.running_var\", \"backbone.features.8.0.weight\", \"backbone.features.8.1.weight\", \"backbone.features.8.1.bias\", \"backbone.features.8.1.running_mean\", \"backbone.features.8.1.running_var\", \"backbone.classifier.1.weight\", \"backbone.classifier.1.bias\", \"backbone.classifier.4.weight\", \"backbone.classifier.4.bias\", \"backbone.classifier.6.weight\", \"backbone.classifier.6.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"image_branch.0.weight\", \"image_branch.1.weight\", \"image_branch.1.bias\", \"image_branch.1.running_mean\", \"image_branch.1.running_var\", \"image_branch.1.num_batches_tracked\", \"image_branch.4.0.conv1.weight\", \"image_branch.4.0.bn1.weight\", \"image_branch.4.0.bn1.bias\", \"image_branch.4.0.bn1.running_mean\", \"image_branch.4.0.bn1.running_var\", \"image_branch.4.0.bn1.num_batches_tracked\", \"image_branch.4.0.conv2.weight\", \"image_branch.4.0.bn2.weight\", \"image_branch.4.0.bn2.bias\", \"image_branch.4.0.bn2.running_mean\", \"image_branch.4.0.bn2.running_var\", \"image_branch.4.0.bn2.num_batches_tracked\", \"image_branch.4.1.conv1.weight\", \"image_branch.4.1.bn1.weight\", \"image_branch.4.1.bn1.bias\", \"image_branch.4.1.bn1.running_mean\", \"image_branch.4.1.bn1.running_var\", \"image_branch.4.1.bn1.num_batches_tracked\", \"image_branch.4.1.conv2.weight\", \"image_branch.4.1.bn2.weight\", \"image_branch.4.1.bn2.bias\", \"image_branch.4.1.bn2.running_mean\", \"image_branch.4.1.bn2.running_var\", \"image_branch.4.1.bn2.num_batches_tracked\", \"image_branch.5.0.conv1.weight\", \"image_branch.5.0.bn1.weight\", \"image_branch.5.0.bn1.bias\", \"image_branch.5.0.bn1.running_mean\", \"image_branch.5.0.bn1.running_var\", \"image_branch.5.0.bn1.num_batches_tracked\", \"image_branch.5.0.conv2.weight\", \"image_branch.5.0.bn2.weight\", \"image_branch.5.0.bn2.bias\", \"image_branch.5.0.bn2.running_mean\", \"image_branch.5.0.bn2.running_var\", \"image_branch.5.0.bn2.num_batches_tracked\", \"image_branch.5.0.downsample.0.weight\", \"image_branch.5.0.downsample.1.weight\", \"image_branch.5.0.downsample.1.bias\", \"image_branch.5.0.downsample.1.running_mean\", \"image_branch.5.0.downsample.1.running_var\", \"image_branch.5.0.downsample.1.num_batches_tracked\", \"image_branch.5.1.conv1.weight\", \"image_branch.5.1.bn1.weight\", \"image_branch.5.1.bn1.bias\", \"image_branch.5.1.bn1.running_mean\", \"image_branch.5.1.bn1.running_var\", \"image_branch.5.1.bn1.num_batches_tracked\", \"image_branch.5.1.conv2.weight\", \"image_branch.5.1.bn2.weight\", \"image_branch.5.1.bn2.bias\", \"image_branch.5.1.bn2.running_mean\", \"image_branch.5.1.bn2.running_var\", \"image_branch.5.1.bn2.num_batches_tracked\", \"image_branch.6.0.conv1.weight\", \"image_branch.6.0.bn1.weight\", \"image_branch.6.0.bn1.bias\", \"image_branch.6.0.bn1.running_mean\", \"image_branch.6.0.bn1.running_var\", \"image_branch.6.0.bn1.num_batches_tracked\", \"image_branch.6.0.conv2.weight\", \"image_branch.6.0.bn2.weight\", \"image_branch.6.0.bn2.bias\", \"image_branch.6.0.bn2.running_mean\", \"image_branch.6.0.bn2.running_var\", \"image_branch.6.0.bn2.num_batches_tracked\", \"image_branch.6.0.downsample.0.weight\", \"image_branch.6.0.downsample.1.weight\", \"image_branch.6.0.downsample.1.bias\", \"image_branch.6.0.downsample.1.running_mean\", \"image_branch.6.0.downsample.1.running_var\", \"image_branch.6.0.downsample.1.num_batches_tracked\", \"image_branch.6.1.conv1.weight\", \"image_branch.6.1.bn1.weight\", \"image_branch.6.1.bn1.bias\", \"image_branch.6.1.bn1.running_mean\", \"image_branch.6.1.bn1.running_var\", \"image_branch.6.1.bn1.num_batches_tracked\", \"image_branch.6.1.conv2.weight\", \"image_branch.6.1.bn2.weight\", \"image_branch.6.1.bn2.bias\", \"image_branch.6.1.bn2.running_mean\", \"image_branch.6.1.bn2.running_var\", \"image_branch.6.1.bn2.num_batches_tracked\", \"image_branch.7.0.conv1.weight\", \"image_branch.7.0.bn1.weight\", \"image_branch.7.0.bn1.bias\", \"image_branch.7.0.bn1.running_mean\", \"image_branch.7.0.bn1.running_var\", \"image_branch.7.0.bn1.num_batches_tracked\", \"image_branch.7.0.conv2.weight\", \"image_branch.7.0.bn2.weight\", \"image_branch.7.0.bn2.bias\", \"image_branch.7.0.bn2.running_mean\", \"image_branch.7.0.bn2.running_var\", \"image_branch.7.0.bn2.num_batches_tracked\", \"image_branch.7.0.downsample.0.weight\", \"image_branch.7.0.downsample.1.weight\", \"image_branch.7.0.downsample.1.bias\", \"image_branch.7.0.downsample.1.running_mean\", \"image_branch.7.0.downsample.1.running_var\", \"image_branch.7.0.downsample.1.num_batches_tracked\", \"image_branch.7.1.conv1.weight\", \"image_branch.7.1.bn1.weight\", \"image_branch.7.1.bn1.bias\", \"image_branch.7.1.bn1.running_mean\", \"image_branch.7.1.bn1.running_var\", \"image_branch.7.1.bn1.num_batches_tracked\", \"image_branch.7.1.conv2.weight\", \"image_branch.7.1.bn2.weight\", \"image_branch.7.1.bn2.bias\", \"image_branch.7.1.bn2.running_mean\", \"image_branch.7.1.bn2.running_var\", \"image_branch.7.1.bn2.num_batches_tracked\", \"hist_branch.0.weight\", \"hist_branch.0.bias\", \"hist_branch.1.weight\", \"hist_branch.1.bias\", \"hist_branch.1.running_mean\", \"hist_branch.1.running_var\", \"hist_branch.1.num_batches_tracked\", \"hist_branch.4.weight\", \"hist_branch.4.bias\", \"hist_branch.5.weight\", \"hist_branch.5.bias\", \"hist_branch.5.running_mean\", \"hist_branch.5.running_var\", \"hist_branch.5.num_batches_tracked\", \"hist_branch.8.weight\", \"hist_branch.8.bias\", \"hist_branch.9.weight\", \"hist_branch.9.bias\", \"hist_branch.9.running_mean\", \"hist_branch.9.running_var\", \"hist_branch.9.num_batches_tracked\", \"fusion.0.weight\", \"fusion.0.bias\", \"fusion.3.weight\", \"fusion.3.bias\", \"fusion.6.weight\", \"fusion.6.bias\". \n",
            "Using randomly initialized model\n",
            "\n",
            "🎯 Making predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:   0%|          | 0/19 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0001.png: Cannot read image: /content/test_imgs/test_imgs/0001.png\n",
            "Error reading image /content/test_imgs/test_imgs/0034.png: Cannot read image: /content/test_imgs/test_imgs/0034.pngError reading image /content/test_imgs/test_imgs/0015.png: Cannot read image: /content/test_imgs/test_imgs/0015.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0035.png: Cannot read image: /content/test_imgs/test_imgs/0035.pngError reading image /content/test_imgs/test_imgs/0018.png: Cannot read image: /content/test_imgs/test_imgs/0018.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0042.png: Cannot read image: /content/test_imgs/test_imgs/0042.pngError reading image /content/test_imgs/test_imgs/0019.png: Cannot read image: /content/test_imgs/test_imgs/0019.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0049.png: Cannot read image: /content/test_imgs/test_imgs/0049.png\n",
            "Error reading image /content/test_imgs/test_imgs/0020.png: Cannot read image: /content/test_imgs/test_imgs/0020.png\n",
            "Error reading image /content/test_imgs/test_imgs/0055.png: Cannot read image: /content/test_imgs/test_imgs/0055.pngError reading image /content/test_imgs/test_imgs/0022.png: Cannot read image: /content/test_imgs/test_imgs/0022.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0027.png: Cannot read image: /content/test_imgs/test_imgs/0027.png\n",
            "Error reading image /content/test_imgs/test_imgs/0030.png: Cannot read image: /content/test_imgs/test_imgs/0030.pngError reading image /content/test_imgs/test_imgs/0061.png: Cannot read image: /content/test_imgs/test_imgs/0061.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0087.png: Cannot read image: /content/test_imgs/test_imgs/0087.png\n",
            "Error reading image /content/test_imgs/test_imgs/0062.png: Cannot read image: /content/test_imgs/test_imgs/0062.pngError reading image /content/test_imgs/test_imgs/0091.png: Cannot read image: /content/test_imgs/test_imgs/0091.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0083.png: Cannot read image: /content/test_imgs/test_imgs/0083.pngError reading image /content/test_imgs/test_imgs/0094.png: Cannot read image: /content/test_imgs/test_imgs/0094.png\n",
            "Error reading image /content/test_imgs/test_imgs/0095.png: Cannot read image: /content/test_imgs/test_imgs/0095.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0100.png: Cannot read image: /content/test_imgs/test_imgs/0100.png\n",
            "Error reading image /content/test_imgs/test_imgs/0133.png: Cannot read image: /content/test_imgs/test_imgs/0133.pngError reading image /content/test_imgs/test_imgs/0126.png: Cannot read image: /content/test_imgs/test_imgs/0126.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0134.png: Cannot read image: /content/test_imgs/test_imgs/0134.pngError reading image /content/test_imgs/test_imgs/0127.png: Cannot read image: /content/test_imgs/test_imgs/0127.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0150.png: Cannot read image: /content/test_imgs/test_imgs/0150.png\n",
            "Error reading image /content/test_imgs/test_imgs/0154.png: Cannot read image: /content/test_imgs/test_imgs/0154.png\n",
            "Error reading image /content/test_imgs/test_imgs/0132.png: Cannot read image: /content/test_imgs/test_imgs/0132.png\n",
            "Error reading image /content/test_imgs/test_imgs/0169.png: Cannot read image: /content/test_imgs/test_imgs/0169.png\n",
            "Error reading image /content/test_imgs/test_imgs/0175.png: Cannot read image: /content/test_imgs/test_imgs/0175.pngError reading image /content/test_imgs/test_imgs/0186.png: Cannot read image: /content/test_imgs/test_imgs/0186.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0179.png: Cannot read image: /content/test_imgs/test_imgs/0179.png\n",
            "Error reading image /content/test_imgs/test_imgs/0187.png: Cannot read image: /content/test_imgs/test_imgs/0187.pngError reading image /content/test_imgs/test_imgs/0182.png: Cannot read image: /content/test_imgs/test_imgs/0182.png\n",
            "\n",
            "Error reading image /content/test_imgs/test_imgs/0192.png: Cannot read image: /content/test_imgs/test_imgs/0192.png\n",
            "Error reading image /content/test_imgs/test_imgs/0199.png: Cannot read image: /content/test_imgs/test_imgs/0199.png\n",
            "Error reading image /content/test_imgs/test_imgs/0208.png: Cannot read image: /content/test_imgs/test_imgs/0208.png\n",
            "Error reading image /content/test_imgs/test_imgs/0222.png: Cannot read image: /content/test_imgs/test_imgs/0222.png\n",
            "Error reading image /content/test_imgs/test_imgs/0228.png: Cannot read image: /content/test_imgs/test_imgs/0228.png\n",
            "Error reading image /content/test_imgs/test_imgs/0234.png: Cannot read image: /content/test_imgs/test_imgs/0234.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:   5%|▌         | 1/19 [00:01<00:23,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0238.png: Cannot read image: /content/test_imgs/test_imgs/0238.png\n",
            "Error reading image /content/test_imgs/test_imgs/0239.png: Cannot read image: /content/test_imgs/test_imgs/0239.png\n",
            "Error reading image /content/test_imgs/test_imgs/0241.png: Cannot read image: /content/test_imgs/test_imgs/0241.png\n",
            "Error reading image /content/test_imgs/test_imgs/0242.png: Cannot read image: /content/test_imgs/test_imgs/0242.png\n",
            "Error reading image /content/test_imgs/test_imgs/0243.png: Cannot read image: /content/test_imgs/test_imgs/0243.png\n",
            "Error reading image /content/test_imgs/test_imgs/0245.png: Cannot read image: /content/test_imgs/test_imgs/0245.png\n",
            "Error reading image /content/test_imgs/test_imgs/0264.png: Cannot read image: /content/test_imgs/test_imgs/0264.png\n",
            "Error reading image /content/test_imgs/test_imgs/0291.png: Cannot read image: /content/test_imgs/test_imgs/0291.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  11%|█         | 2/19 [00:02<00:16,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0297.png: Cannot read image: /content/test_imgs/test_imgs/0297.png\n",
            "Error reading image /content/test_imgs/test_imgs/0312.png: Cannot read image: /content/test_imgs/test_imgs/0312.png\n",
            "Error reading image /content/test_imgs/test_imgs/0313.png: Cannot read image: /content/test_imgs/test_imgs/0313.png\n",
            "Error reading image /content/test_imgs/test_imgs/0317.png: Cannot read image: /content/test_imgs/test_imgs/0317.png\n",
            "Error reading image /content/test_imgs/test_imgs/0319.png: Cannot read image: /content/test_imgs/test_imgs/0319.png\n",
            "Error reading image /content/test_imgs/test_imgs/0334.png: Cannot read image: /content/test_imgs/test_imgs/0334.png\n",
            "Error reading image /content/test_imgs/test_imgs/0341.png: Cannot read image: /content/test_imgs/test_imgs/0341.png\n",
            "Error reading image /content/test_imgs/test_imgs/0348.png: Cannot read image: /content/test_imgs/test_imgs/0348.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  16%|█▌        | 3/19 [00:02<00:11,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0351.png: Cannot read image: /content/test_imgs/test_imgs/0351.png\n",
            "Error reading image /content/test_imgs/test_imgs/0355.png: Cannot read image: /content/test_imgs/test_imgs/0355.png\n",
            "Error reading image /content/test_imgs/test_imgs/0357.png: Cannot read image: /content/test_imgs/test_imgs/0357.png\n",
            "Error reading image /content/test_imgs/test_imgs/0358.png: Cannot read image: /content/test_imgs/test_imgs/0358.png\n",
            "Error reading image /content/test_imgs/test_imgs/0359.png: Cannot read image: /content/test_imgs/test_imgs/0359.png\n",
            "Error reading image /content/test_imgs/test_imgs/0373.png: Cannot read image: /content/test_imgs/test_imgs/0373.png\n",
            "Error reading image /content/test_imgs/test_imgs/0376.png: Cannot read image: /content/test_imgs/test_imgs/0376.png\n",
            "Error reading image /content/test_imgs/test_imgs/0377.png: Cannot read image: /content/test_imgs/test_imgs/0377.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  21%|██        | 4/19 [00:02<00:09,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0379.png: Cannot read image: /content/test_imgs/test_imgs/0379.png\n",
            "Error reading image /content/test_imgs/test_imgs/0380.png: Cannot read image: /content/test_imgs/test_imgs/0380.png\n",
            "Error reading image /content/test_imgs/test_imgs/0385.png: Cannot read image: /content/test_imgs/test_imgs/0385.png\n",
            "Error reading image /content/test_imgs/test_imgs/0396.png: Cannot read image: /content/test_imgs/test_imgs/0396.png\n",
            "Error reading image /content/test_imgs/test_imgs/0401.png: Cannot read image: /content/test_imgs/test_imgs/0401.png\n",
            "Error reading image /content/test_imgs/test_imgs/0407.png: Cannot read image: /content/test_imgs/test_imgs/0407.png\n",
            "Error reading image /content/test_imgs/test_imgs/0411.png: Cannot read image: /content/test_imgs/test_imgs/0411.png\n",
            "Error reading image /content/test_imgs/test_imgs/0416.png: Cannot read image: /content/test_imgs/test_imgs/0416.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  26%|██▋       | 5/19 [00:03<00:07,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0443.png: Cannot read image: /content/test_imgs/test_imgs/0443.png\n",
            "Error reading image /content/test_imgs/test_imgs/0456.png: Cannot read image: /content/test_imgs/test_imgs/0456.png\n",
            "Error reading image /content/test_imgs/test_imgs/0464.png: Cannot read image: /content/test_imgs/test_imgs/0464.png\n",
            "Error reading image /content/test_imgs/test_imgs/0470.png: Cannot read image: /content/test_imgs/test_imgs/0470.png\n",
            "Error reading image /content/test_imgs/test_imgs/0479.png: Cannot read image: /content/test_imgs/test_imgs/0479.png\n",
            "Error reading image /content/test_imgs/test_imgs/0486.png: Cannot read image: /content/test_imgs/test_imgs/0486.png\n",
            "Error reading image /content/test_imgs/test_imgs/0496.png: Cannot read image: /content/test_imgs/test_imgs/0496.png\n",
            "Error reading image /content/test_imgs/test_imgs/0500.png: Cannot read image: /content/test_imgs/test_imgs/0500.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  32%|███▏      | 6/19 [00:03<00:06,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0513.png: Cannot read image: /content/test_imgs/test_imgs/0513.png\n",
            "Error reading image /content/test_imgs/test_imgs/0528.png: Cannot read image: /content/test_imgs/test_imgs/0528.png\n",
            "Error reading image /content/test_imgs/test_imgs/0534.png: Cannot read image: /content/test_imgs/test_imgs/0534.png\n",
            "Error reading image /content/test_imgs/test_imgs/0558.png: Cannot read image: /content/test_imgs/test_imgs/0558.png\n",
            "Error reading image /content/test_imgs/test_imgs/0570.png: Cannot read image: /content/test_imgs/test_imgs/0570.png\n",
            "Error reading image /content/test_imgs/test_imgs/0574.png: Cannot read image: /content/test_imgs/test_imgs/0574.png\n",
            "Error reading image /content/test_imgs/test_imgs/0589.png: Cannot read image: /content/test_imgs/test_imgs/0589.png\n",
            "Error reading image /content/test_imgs/test_imgs/0592.png: Cannot read image: /content/test_imgs/test_imgs/0592.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  37%|███▋      | 7/19 [00:04<00:05,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0596.png: Cannot read image: /content/test_imgs/test_imgs/0596.png\n",
            "Error reading image /content/test_imgs/test_imgs/0600.png: Cannot read image: /content/test_imgs/test_imgs/0600.png\n",
            "Error reading image /content/test_imgs/test_imgs/0601.png: Cannot read image: /content/test_imgs/test_imgs/0601.png\n",
            "Error reading image /content/test_imgs/test_imgs/0604.png: Cannot read image: /content/test_imgs/test_imgs/0604.png\n",
            "Error reading image /content/test_imgs/test_imgs/0609.png: Cannot read image: /content/test_imgs/test_imgs/0609.png\n",
            "Error reading image /content/test_imgs/test_imgs/0614.png: Cannot read image: /content/test_imgs/test_imgs/0614.png\n",
            "Error reading image /content/test_imgs/test_imgs/0616.png: Cannot read image: /content/test_imgs/test_imgs/0616.png\n",
            "Error reading image /content/test_imgs/test_imgs/0627.png: Cannot read image: /content/test_imgs/test_imgs/0627.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  42%|████▏     | 8/19 [00:04<00:05,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0634.png: Cannot read image: /content/test_imgs/test_imgs/0634.png\n",
            "Error reading image /content/test_imgs/test_imgs/0644.png: Cannot read image: /content/test_imgs/test_imgs/0644.png\n",
            "Error reading image /content/test_imgs/test_imgs/0647.png: Cannot read image: /content/test_imgs/test_imgs/0647.png\n",
            "Error reading image /content/test_imgs/test_imgs/0649.png: Cannot read image: /content/test_imgs/test_imgs/0649.png\n",
            "Error reading image /content/test_imgs/test_imgs/0661.png: Cannot read image: /content/test_imgs/test_imgs/0661.png\n",
            "Error reading image /content/test_imgs/test_imgs/0665.png: Cannot read image: /content/test_imgs/test_imgs/0665.png\n",
            "Error reading image /content/test_imgs/test_imgs/0669.png: Cannot read image: /content/test_imgs/test_imgs/0669.png\n",
            "Error reading image /content/test_imgs/test_imgs/0677.png: Cannot read image: /content/test_imgs/test_imgs/0677.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  47%|████▋     | 9/19 [00:05<00:04,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0679.png: Cannot read image: /content/test_imgs/test_imgs/0679.png\n",
            "Error reading image /content/test_imgs/test_imgs/0682.png: Cannot read image: /content/test_imgs/test_imgs/0682.png\n",
            "Error reading image /content/test_imgs/test_imgs/0687.png: Cannot read image: /content/test_imgs/test_imgs/0687.png\n",
            "Error reading image /content/test_imgs/test_imgs/0693.png: Cannot read image: /content/test_imgs/test_imgs/0693.png\n",
            "Error reading image /content/test_imgs/test_imgs/0700.png: Cannot read image: /content/test_imgs/test_imgs/0700.png\n",
            "Error reading image /content/test_imgs/test_imgs/0709.png: Cannot read image: /content/test_imgs/test_imgs/0709.png\n",
            "Error reading image /content/test_imgs/test_imgs/0726.png: Cannot read image: /content/test_imgs/test_imgs/0726.png\n",
            "Error reading image /content/test_imgs/test_imgs/0727.png: Cannot read image: /content/test_imgs/test_imgs/0727.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  53%|█████▎    | 10/19 [00:05<00:04,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0752.png: Cannot read image: /content/test_imgs/test_imgs/0752.png\n",
            "Error reading image /content/test_imgs/test_imgs/0757.png: Cannot read image: /content/test_imgs/test_imgs/0757.png\n",
            "Error reading image /content/test_imgs/test_imgs/0760.png: Cannot read image: /content/test_imgs/test_imgs/0760.png\n",
            "Error reading image /content/test_imgs/test_imgs/0763.png: Cannot read image: /content/test_imgs/test_imgs/0763.png\n",
            "Error reading image /content/test_imgs/test_imgs/0768.png: Cannot read image: /content/test_imgs/test_imgs/0768.png\n",
            "Error reading image /content/test_imgs/test_imgs/0770.png: Cannot read image: /content/test_imgs/test_imgs/0770.png\n",
            "Error reading image /content/test_imgs/test_imgs/0774.png: Cannot read image: /content/test_imgs/test_imgs/0774.png\n",
            "Error reading image /content/test_imgs/test_imgs/0778.png: Cannot read image: /content/test_imgs/test_imgs/0778.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  58%|█████▊    | 11/19 [00:06<00:03,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0791.png: Cannot read image: /content/test_imgs/test_imgs/0791.png\n",
            "Error reading image /content/test_imgs/test_imgs/0792.png: Cannot read image: /content/test_imgs/test_imgs/0792.png\n",
            "Error reading image /content/test_imgs/test_imgs/0799.png: Cannot read image: /content/test_imgs/test_imgs/0799.png\n",
            "Error reading image /content/test_imgs/test_imgs/0800.png: Cannot read image: /content/test_imgs/test_imgs/0800.png\n",
            "Error reading image /content/test_imgs/test_imgs/0804.png: Cannot read image: /content/test_imgs/test_imgs/0804.png\n",
            "Error reading image /content/test_imgs/test_imgs/0806.png: Cannot read image: /content/test_imgs/test_imgs/0806.png\n",
            "Error reading image /content/test_imgs/test_imgs/0816.png: Cannot read image: /content/test_imgs/test_imgs/0816.png\n",
            "Error reading image /content/test_imgs/test_imgs/0818.png: Cannot read image: /content/test_imgs/test_imgs/0818.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  63%|██████▎   | 12/19 [00:06<00:03,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0821.png: Cannot read image: /content/test_imgs/test_imgs/0821.png\n",
            "Error reading image /content/test_imgs/test_imgs/0823.png: Cannot read image: /content/test_imgs/test_imgs/0823.png\n",
            "Error reading image /content/test_imgs/test_imgs/0842.png: Cannot read image: /content/test_imgs/test_imgs/0842.png\n",
            "Error reading image /content/test_imgs/test_imgs/0852.png: Cannot read image: /content/test_imgs/test_imgs/0852.png\n",
            "Error reading image /content/test_imgs/test_imgs/0855.png: Cannot read image: /content/test_imgs/test_imgs/0855.png\n",
            "Error reading image /content/test_imgs/test_imgs/0868.png: Cannot read image: /content/test_imgs/test_imgs/0868.png\n",
            "Error reading image /content/test_imgs/test_imgs/0877.png: Cannot read image: /content/test_imgs/test_imgs/0877.png\n",
            "Error reading image /content/test_imgs/test_imgs/0883.png: Cannot read image: /content/test_imgs/test_imgs/0883.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  68%|██████▊   | 13/19 [00:07<00:03,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0890.png: Cannot read image: /content/test_imgs/test_imgs/0890.png\n",
            "Error reading image /content/test_imgs/test_imgs/0895.png: Cannot read image: /content/test_imgs/test_imgs/0895.png\n",
            "Error reading image /content/test_imgs/test_imgs/0896.png: Cannot read image: /content/test_imgs/test_imgs/0896.png\n",
            "Error reading image /content/test_imgs/test_imgs/0909.png: Cannot read image: /content/test_imgs/test_imgs/0909.png\n",
            "Error reading image /content/test_imgs/test_imgs/0912.png: Cannot read image: /content/test_imgs/test_imgs/0912.png\n",
            "Error reading image /content/test_imgs/test_imgs/0917.png: Cannot read image: /content/test_imgs/test_imgs/0917.png\n",
            "Error reading image /content/test_imgs/test_imgs/0924.png: Cannot read image: /content/test_imgs/test_imgs/0924.png\n",
            "Error reading image /content/test_imgs/test_imgs/0925.png: Cannot read image: /content/test_imgs/test_imgs/0925.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMaking predictions:  74%|███████▎  | 14/19 [00:07<00:02,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading image /content/test_imgs/test_imgs/0926.png: Cannot read image: /content/test_imgs/test_imgs/0926.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Making predictions: 100%|██████████| 19/19 [00:10<00:00,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Creating submission file...\n",
            "Creating submission file: /content/final_submission.csv\n",
            "Submission file created with 145 predictions\n",
            "First 5 predictions:\n",
            "  image_path          wp_r          wp_g          wp_b\n",
            "0   0001.png  32851.074219  33837.226562  32615.828125\n",
            "1   0015.png  32851.074219  33837.226562  32615.828125\n",
            "2   0018.png  32851.074219  33837.226562  32615.828125\n",
            "3   0019.png  32851.074219  33837.226562  32615.828125\n",
            "4   0020.png  32851.074219  33837.226562  32615.828125\n",
            "\n",
            "📊 Prediction statistics:\n",
            "Total predictions: 145\n",
            "Value ranges:\n",
            "  R: 32851.1 - 32851.1\n",
            "  G: 33837.2 - 33837.2\n",
            "  B: 32615.8 - 32615.8\n",
            "✅ All predictions are within valid range\n",
            "\n",
            "🎉 Done! Submission file saved to: /content/final_submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def check_submission_file(file_path):\n",
        "    \"\"\"Проверяет submission файл на проблемы\"\"\"\n",
        "    print(f\"🔍 Checking: {file_path}\")\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"📊 Shape: {df.shape}\")\n",
        "    print(f\"📋 Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Проверяем уникальность значений\n",
        "    print(\"\\n✅ Unique values check:\")\n",
        "    for col in ['wp_r', 'wp_g', 'wp_b']:\n",
        "        if col in df.columns:\n",
        "            unique_vals = df[col].nunique()\n",
        "            print(f\"   {col}: {unique_vals} unique values\")\n",
        "\n",
        "            if unique_vals == 1:\n",
        "                print(f\"   ⚠️  WARNING: All values are the same!\")\n",
        "                print(f\"   Value: {df[col].iloc[0]}\")\n",
        "\n",
        "    # Проверяем диапазон значений\n",
        "    print(\"\\n📈 Value ranges:\")\n",
        "    for col in ['wp_r', 'wp_g', 'wp_b']:\n",
        "        if col in df.columns:\n",
        "            min_val = df[col].min()\n",
        "            max_val = df[col].max()\n",
        "            mean_val = df[col].mean()\n",
        "            print(f\"   {col}: min={min_val:.2f}, max={max_val:.2f}, mean={mean_val:.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def fix_submission_file(input_path, output_path):\n",
        "    \"\"\"Создает исправленный submission файл\"\"\"\n",
        "    print(\"🛠️ Creating fixed submission file...\")\n",
        "\n",
        "    # Читаем оригинальный файл\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    # Проверяем, есть ли правильные значения\n",
        "    if df['wp_r'].nunique() == 1:\n",
        "        print(\"❌ All white balance values are identical!\")\n",
        "        print(\"   This suggests a problem with your model predictions.\")\n",
        "        print(\"   Please retrain your model with proper validation.\")\n",
        "\n",
        "        # Создаем случайные значения для демонстрации (ЗАМЕНИТЕ НА РЕАЛЬНЫЕ ПРЕДСКАЗАНИЯ)\n",
        "        np.random.seed(42)\n",
        "        n_samples = len(df)\n",
        "\n",
        "        # Генерируем реалистичные значения (пример)\n",
        "        df['wp_r'] = np.random.uniform(10000, 40000, n_samples)\n",
        "        df['wp_g'] = np.random.uniform(30000, 35000, n_samples)\n",
        "        df['wp_b'] = np.random.uniform(20000, 40000, n_samples)\n",
        "\n",
        "        print(\"   Generated demo values (REPLACE WITH REAL PREDICTIONS!)\")\n",
        "\n",
        "    # Сохраняем исправленный файл\n",
        "    df.to_csv(output_path, index=False, float_format='%.6f')\n",
        "    print(f\"✅ Fixed file saved: {output_path}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_proper_submission(template_path, predictions_path, output_path):\n",
        "    \"\"\"Создает правильный submission файл из предсказаний\"\"\"\n",
        "    print(\"📝 Creating proper submission file...\")\n",
        "\n",
        "    # Читаем файл-шаблон с именами изображений\n",
        "    template_df = pd.read_csv(template_path)\n",
        "    print(f\"Template: {len(template_df)} images\")\n",
        "\n",
        "    # Читаем файл с предсказаниями\n",
        "    try:\n",
        "        pred_df = pd.read_csv(predictions_path)\n",
        "        print(f\"Predictions: {len(pred_df)} rows\")\n",
        "\n",
        "        # Проверяем соответствие размеров\n",
        "        if len(template_df) != len(pred_df):\n",
        "            print(\"⚠️  Warning: Different number of rows!\")\n",
        "            print(f\"   Template: {len(template_df)}, Predictions: {len(pred_df)}\")\n",
        "\n",
        "        # Создаем финальный файл\n",
        "        final_df = template_df.copy()\n",
        "\n",
        "        # Добавляем предсказания (предполагаем, что порядок совпадает)\n",
        "        if 'wp_r' in pred_df.columns and 'wp_g' in pred_df.columns and 'wp_b' in pred_df.columns:\n",
        "            final_df['wp_r'] = pred_df['wp_r'].values\n",
        "            final_df['wp_g'] = pred_df['wp_g'].values\n",
        "            final_df['wp_b'] = pred_df['wp_b'].values\n",
        "        else:\n",
        "            print(\"❌ Prediction file doesn't have expected columns\")\n",
        "            return None\n",
        "\n",
        "        # Сохраняем\n",
        "        final_df.to_csv(output_path, index=False, float_format='%.6f')\n",
        "        print(f\"✅ Proper submission created: {output_path}\")\n",
        "        return final_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error reading predictions: {e}\")\n",
        "        return None\n",
        "\n",
        "# Основная функция\n",
        "def main():\n",
        "    \"\"\"Основная функция для исправления submission файла\"\"\"\n",
        "\n",
        "    # Ваш проблемный файл\n",
        "    problem_file = \"/content/final_submission (14) (1).csv\"\n",
        "    output_file = \"/content/fixed_submission.csv\"\n",
        "\n",
        "    # Проверяем файл\n",
        "    print(\"=\" * 50)\n",
        "    df = check_submission_file(problem_file)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ANALYSIS RESULTS:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Анализируем проблему\n",
        "    unique_r = df['wp_r'].nunique()\n",
        "    unique_g = df['wp_g'].nunique()\n",
        "    unique_b = df['wp_b'].nunique()\n",
        "\n",
        "    if unique_r == 1 and unique_g == 1 and unique_b == 1:\n",
        "        print(\"❌ CRITICAL ISSUE:\")\n",
        "        print(\"   All white balance values are identical for all images!\")\n",
        "        print(\"   This means your model is not learning properly.\")\n",
        "        print(\"\\n🔧 Possible solutions:\")\n",
        "        print(\"   1. Check your training data\")\n",
        "        print(\"   2. Verify your model architecture\")\n",
        "        print(\"   3. Ensure proper loss function\")\n",
        "        print(\"   4. Check for data leakage\")\n",
        "        print(\"   5. Validate your data preprocessing\")\n",
        "\n",
        "        # Создаем временный исправленный файл\n",
        "        fixed_df = fix_submission_file(problem_file, output_file)\n",
        "        print(f\"\\n📁 Temporary fix applied to: {output_file}\")\n",
        "        print(\"   NOTE: This contains demo values - RETRAIN YOUR MODEL!\")\n",
        "\n",
        "    else:\n",
        "        print(\"✅ File looks good!\")\n",
        "        # Просто копируем файл\n",
        "        df.to_csv(output_file, index=False, float_format='%.6f')\n",
        "        print(f\"📁 File copied to: {output_file}\")\n",
        "\n",
        "# Альтернатива: создание submission из шаблона\n",
        "def create_from_template():\n",
        "    \"\"\"Создает submission из шаблона и предсказаний\"\"\"\n",
        "    template_file = \"/content/submission_template.csv\"  # Файл с image_path\n",
        "    predictions_file = \"/content/model_predictions.csv\"  # Файл с wp_r, wp_g, wp_b\n",
        "    output_file = \"/content/final_submission.csv\"\n",
        "\n",
        "    result = create_proper_submission(template_file, predictions_file, output_file)\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n📊 Final submission preview:\")\n",
        "        print(result.head())\n",
        "        print(f\"\\n✅ Successfully created {output_file}\")\n",
        "\n",
        "# Запуск\n",
        "if __name__ == \"__main__\":\n",
        "    # Проверяем и исправляем текущий файл\n",
        "    main()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"NEXT STEPS:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"1. Check your model training process\")\n",
        "    print(\"2. Verify your data pipeline\")\n",
        "    print(\"3. Ensure you're using different images for train/val/test\")\n",
        "    print(\"4. Retrain your model with proper validation\")\n",
        "    print(\"5. Use the fixed file only as a temporary solution\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urcte5KntgNN",
        "outputId": "5fb6c2e0-d4ef-482c-c3e7-a5b6f2caffee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "🔍 Checking: /content/final_submission (14) (1).csv\n",
            "📊 Shape: (145, 4)\n",
            "📋 Columns: ['names', 'wp_r', 'wp_g', 'wp_b']\n",
            "\n",
            "✅ Unique values check:\n",
            "   wp_r: 1 unique values\n",
            "   ⚠️  WARNING: All values are the same!\n",
            "   Value: 31693.410156\n",
            "   wp_g: 1 unique values\n",
            "   ⚠️  WARNING: All values are the same!\n",
            "   Value: 33261.800781\n",
            "   wp_b: 1 unique values\n",
            "   ⚠️  WARNING: All values are the same!\n",
            "   Value: 32383.259766\n",
            "\n",
            "📈 Value ranges:\n",
            "   wp_r: min=31693.41, max=31693.41, mean=31693.41\n",
            "   wp_g: min=33261.80, max=33261.80, mean=33261.80\n",
            "   wp_b: min=32383.26, max=32383.26, mean=32383.26\n",
            "\n",
            "==================================================\n",
            "ANALYSIS RESULTS:\n",
            "==================================================\n",
            "❌ CRITICAL ISSUE:\n",
            "   All white balance values are identical for all images!\n",
            "   This means your model is not learning properly.\n",
            "\n",
            "🔧 Possible solutions:\n",
            "   1. Check your training data\n",
            "   2. Verify your model architecture\n",
            "   3. Ensure proper loss function\n",
            "   4. Check for data leakage\n",
            "   5. Validate your data preprocessing\n",
            "🛠️ Creating fixed submission file...\n",
            "❌ All white balance values are identical!\n",
            "   This suggests a problem with your model predictions.\n",
            "   Please retrain your model with proper validation.\n",
            "   Generated demo values (REPLACE WITH REAL PREDICTIONS!)\n",
            "✅ Fixed file saved: /content/fixed_submission.csv\n",
            "\n",
            "📁 Temporary fix applied to: /content/fixed_submission.csv\n",
            "   NOTE: This contains demo values - RETRAIN YOUR MODEL!\n",
            "\n",
            "==================================================\n",
            "NEXT STEPS:\n",
            "==================================================\n",
            "1. Check your model training process\n",
            "2. Verify your data pipeline\n",
            "3. Ensure you're using different images for train/val/test\n",
            "4. Retrain your model with proper validation\n",
            "5. Use the fixed file only as a temporary solution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def merge_submission_files(submission_path, source_path, output_path):\n",
        "    \"\"\"\n",
        "    Объединяет данные: берет первую колонку из submission файла\n",
        "    и остальные три колонки из source файла\n",
        "\n",
        "    Args:\n",
        "        submission_path: путь к файлу submission.csv (откуда берем image_path)\n",
        "        source_path: путь к исходному файлу (откуда берем wp_r, wp_g, wp_b)\n",
        "        output_path: путь для сохранения результата\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"🔍 Reading files...\")\n",
        "\n",
        "    try:\n",
        "        # Читаем submission файл\n",
        "        submission_df = pd.read_csv(submission_path)\n",
        "        print(f\"✅ Submission file loaded: {len(submission_df)} rows\")\n",
        "        print(f\"   Columns: {list(submission_df.columns)}\")\n",
        "\n",
        "        # Читаем source файл\n",
        "        source_df = pd.read_csv(source_path)\n",
        "        print(f\"✅ Source file loaded: {len(source_df)} rows\")\n",
        "        print(f\"   Columns: {list(source_df.columns)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error reading files: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Проверяем, что файлы имеют одинаковое количество строк\n",
        "    if len(submission_df) != len(source_df):\n",
        "        print(f\"⚠️  Warning: Different number of rows!\")\n",
        "        print(f\"   Submission: {len(submission_df)} rows\")\n",
        "        print(f\"   Source: {len(source_df)} rows\")\n",
        "\n",
        "        # Берем минимальное количество строк\n",
        "        min_rows = min(len(submission_df), len(source_df))\n",
        "        submission_df = submission_df.head(min_rows)\n",
        "        source_df = source_df.head(min_rows)\n",
        "        print(f\"   Using first {min_rows} rows from each file\")\n",
        "\n",
        "    # Проверяем наличие нужных колонок\n",
        "    submission_cols = submission_df.columns\n",
        "    source_cols = source_df.columns\n",
        "\n",
        "    # Определяем колонку с путями из submission файла\n",
        "    if 'image_path' in submission_cols:\n",
        "        image_path_col = 'image_path'\n",
        "    else:\n",
        "        # Берем первую колонку\n",
        "        image_path_col = submission_cols[0]\n",
        "        print(f\"ℹ️  Using first column as image_path: {image_path_col}\")\n",
        "\n",
        "    # Определяем колонки для white balance из source файла\n",
        "    wp_cols = []\n",
        "    for col in ['wp_r', 'wp_g', 'wp_b']:\n",
        "        if col in source_cols:\n",
        "            wp_cols.append(col)\n",
        "        else:\n",
        "            # Ищем альтернативные названия\n",
        "            for source_col in source_cols:\n",
        "                if col in source_col.lower() or 'white' in source_col.lower():\n",
        "                    wp_cols.append(source_col)\n",
        "                    print(f\"ℹ️  Using {source_col} as {col}\")\n",
        "                    break\n",
        "            else:\n",
        "                # Если не нашли, берем первые три колонки после image_path\n",
        "                if len(source_cols) >= 4:\n",
        "                    wp_cols.extend(source_cols[1:4])\n",
        "                else:\n",
        "                    wp_cols.extend(source_cols[:3])\n",
        "                print(f\"ℹ️  Using columns {wp_cols} for white balance values\")\n",
        "                break\n",
        "\n",
        "    # Ограничиваем до 3 колонок\n",
        "    wp_cols = wp_cols[:3]\n",
        "\n",
        "    # Создаем новый DataFrame\n",
        "    print(\"\\n🔄 Merging data...\")\n",
        "\n",
        "    try:\n",
        "        # Берем image_path из submission файла\n",
        "        result_df = pd.DataFrame()\n",
        "        result_df['image_path'] = submission_df[image_path_col]\n",
        "\n",
        "        # Берем white balance значения из source файла\n",
        "        for i, col in enumerate(wp_cols[:3]):  # Берем максимум 3 колонки\n",
        "            if i == 0:\n",
        "                result_df['wp_r'] = source_df[col]\n",
        "            elif i == 1:\n",
        "                result_df['wp_g'] = source_df[col]\n",
        "            elif i == 2:\n",
        "                result_df['wp_b'] = source_df[col]\n",
        "\n",
        "        # Если не хватило колонок, заполняем значениями по умолчанию\n",
        "        if len(wp_cols) < 3:\n",
        "            print(f\"⚠️  Only {len(wp_cols)} white balance columns found\")\n",
        "            if 'wp_r' not in result_df.columns:\n",
        "                result_df['wp_r'] = 32768.0\n",
        "            if 'wp_g' not in result_df.columns:\n",
        "                result_df['wp_g'] = 32768.0\n",
        "            if 'wp_b' not in result_df.columns:\n",
        "                result_df['wp_b'] = 32768.0\n",
        "\n",
        "        # Сохраняем результат\n",
        "        result_df.to_csv(output_path, index=False, float_format='%.6f')\n",
        "\n",
        "        print(f\"✅ Merged file saved: {output_path}\")\n",
        "        print(f\"📊 Result shape: {result_df.shape}\")\n",
        "        print(f\"📋 Columns: {list(result_df.columns)}\")\n",
        "        print(\"\\n📄 First 5 rows:\")\n",
        "        print(result_df.head())\n",
        "\n",
        "        return result_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error merging files: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_manual_merge(submission_path, source_path, output_path):\n",
        "    \"\"\"\n",
        "    Ручное объединение с выбором колонок\n",
        "    \"\"\"\n",
        "    print(\"📝 Manual merge mode\")\n",
        "\n",
        "    # Читаем файлы\n",
        "    submission_df = pd.read_csv(submission_path)\n",
        "    source_df = pd.read_csv(source_path)\n",
        "\n",
        "    print(f\"Submission file columns: {list(submission_df.columns)}\")\n",
        "    print(f\"Source file columns: {list(source_df.columns)}\")\n",
        "\n",
        "    # Создаем новый DataFrame\n",
        "    result_df = pd.DataFrame()\n",
        "\n",
        "    # Выбираем колонку для image_path\n",
        "    image_col = input(\"Enter column name for image_path from submission file: \").strip()\n",
        "    if image_col not in submission_df.columns:\n",
        "        print(f\"Column '{image_col}' not found, using first column\")\n",
        "        image_col = submission_df.columns[0]\n",
        "\n",
        "    result_df['image_path'] = submission_df[image_col]\n",
        "\n",
        "    # Выбираем колонки для white balance\n",
        "    wp_mapping = {}\n",
        "    for wp_col in ['wp_r', 'wp_g', 'wp_b']:\n",
        "        col_name = input(f\"Enter column name for {wp_col} from source file: \").strip()\n",
        "        if col_name in source_df.columns:\n",
        "            result_df[wp_col] = source_df[col_name]\n",
        "            wp_mapping[wp_col] = col_name\n",
        "        else:\n",
        "            print(f\"Column '{col_name}' not found, using default value 32768.0\")\n",
        "            result_df[wp_col] = 32768.0\n",
        "\n",
        "    # Сохраняем\n",
        "    result_df.to_csv(output_path, index=False, float_format='%.6f')\n",
        "    print(f\"✅ Manual merge saved to: {output_path}\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def quick_merge():\n",
        "    \"\"\"\n",
        "    Быстрое объединение с стандартными путями\n",
        "    \"\"\"\n",
        "    submission_file = \"/content/submission.csv\"\n",
        "    source_file = \"/content/final_submission (13) (1).csv\"  # ← ВАШ ФАЙЛ С WHITE BALANCE!\n",
        "    output_file = \"/content/merged_submission.csv\"\n",
        "\n",
        "    print(\"🚀 Quick merge with default paths:\")\n",
        "    print(f\"Submission: {submission_file}\")\n",
        "    print(f\"Source: {source_file}\")\n",
        "    print(f\"Output: {output_file}\")\n",
        "\n",
        "    return merge_submission_files(submission_file, source_file, output_file)\n",
        "\n",
        "# Дополнительные утилиты\n",
        "def check_file_info(file_path):\n",
        "    \"\"\"Проверка информации о файле\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"📁 File: {file_path}\")\n",
        "        print(f\"📊 Shape: {df.shape}\")\n",
        "        print(f\"📋 Columns: {list(df.columns)}\")\n",
        "        print(f\"🔢 Dtypes:\\n{df.dtypes}\")\n",
        "        print(\"\\n📄 First 3 rows:\")\n",
        "        print(df.head(3))\n",
        "        print(\"\\n📄 Last 3 rows:\")\n",
        "        print(df.tail(3))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error reading {file_path}: {e}\")\n",
        "\n",
        "def compare_files(file1_path, file2_path):\n",
        "    \"\"\"Сравнение двух файлов\"\"\"\n",
        "    df1 = pd.read_csv(file1_path)\n",
        "    df2 = pd.read_csv(file2_path)\n",
        "\n",
        "    print(\"📊 File Comparison:\")\n",
        "    print(f\"File 1: {file1_path} - {df1.shape}\")\n",
        "    print(f\"File 2: {file2_path} - {df2.shape}\")\n",
        "\n",
        "    print(\"\\n📋 Columns comparison:\")\n",
        "    print(f\"File 1 columns: {list(df1.columns)}\")\n",
        "    print(f\"File 2 columns: {list(df2.columns)}\")\n",
        "\n",
        "    common_cols = set(df1.columns) & set(df2.columns)\n",
        "    print(f\"Common columns: {common_cols}\")\n",
        "\n",
        "# Основная функция\n",
        "def main():\n",
        "    \"\"\"Основная функция для запуска в Colab\"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"📊 SUBISSION FILE MERGER\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Стандартные пути для Colab\n",
        "    files_to_check = [\n",
        "        \"/content/submission.csv\",\n",
        "        \"/content/final_submission.csv\",\n",
        "        \"/content/train.csv\"\n",
        "    ]\n",
        "\n",
        "    print(\"🔍 Checking available files:\")\n",
        "    for file_path in files_to_check:\n",
        "        exists = Path(file_path).exists()\n",
        "        status = \"✅\" if exists else \"❌\"\n",
        "        print(f\"{status} {file_path}\")\n",
        "\n",
        "    # Автоматический поиск source файла\n",
        "    source_candidates = [\n",
        "        \"/content/you1.csv\",\n",
        "        \"/content/train.csv\",\n",
        "        \"/content/test.csv\",\n",
        "        \"/content/data.csv\"\n",
        "    ]\n",
        "\n",
        "    source_file = None\n",
        "    for candidate in source_candidates:\n",
        "        if Path(candidate).exists():\n",
        "            source_file = candidate\n",
        "            break\n",
        "\n",
        "    if source_file:\n",
        "        print(f\"\\n🎯 Found source file: {source_file}\")\n",
        "\n",
        "        # Запускаем автоматическое объединение\n",
        "        result = merge_submission_files(\n",
        "            submission_path=\"/content/submission.csv\",\n",
        "            source_path=source_file,\n",
        "            output_path=\"/content/final_submission.csv\"\n",
        "        )\n",
        "\n",
        "        if result is not None:\n",
        "            print(\"\\n🎉 Merge completed successfully!\")\n",
        "            print(\"📁 Final files:\")\n",
        "            !ls -la /content/*.csv\n",
        "        else:\n",
        "            print(\"\\n❌ Merge failed!\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n❌ No source file found for white balance values!\")\n",
        "        print(\"Please specify paths manually:\")\n",
        "\n",
        "        submission_path = input(\"Enter submission file path: \").strip() or \"/content/submission.csv\"\n",
        "        source_path = input(\"Enter source file path: \").strip()\n",
        "        output_path = input(\"Enter output file path: \").strip() or \"/content/merged_submission.csv\"\n",
        "\n",
        "        if Path(submission_path).exists() and Path(source_path).exists():\n",
        "            merge_submission_files(submission_path, source_path, output_path)\n",
        "        else:\n",
        "            print(\"❌ Files not found!\")\n",
        "\n",
        "# Функции для быстрого использования в Colab\n",
        "def show_csv_preview():\n",
        "    \"\"\"Показать превью всех CSV файлов\"\"\"\n",
        "    csv_files = list(Path(\"/content\").glob(\"*.csv\"))\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"📄 {csv_file.name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "            print(\"\\nFirst 2 rows:\")\n",
        "            print(df.head(2))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading: {e}\")\n",
        "\n",
        "# Запуск\n",
        "if __name__ == \"__main__\":\n",
        "    # Показываем доступные файлы\n",
        "    show_csv_preview()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    # Запускаем основной процесс\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2mVY95Omadj",
        "outputId": "cce05f94-c2ff-4e8f-ffa8-b21badad8935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "📄 final_submission (13) (1).csv\n",
            "==================================================\n",
            "Shape: (145, 4)\n",
            "Columns: ['names', 'wp_r', 'wp_g', 'wp_b']\n",
            "\n",
            "First 2 rows:\n",
            "                names      wp_r      wp_g      wp_b\n",
            "0  test_imgs/0001.png  0.173683  0.508642  0.215429\n",
            "1  test_imgs/0015.png  0.266894  0.956725  0.577948\n",
            "\n",
            "==================================================\n",
            "📄 test.csv\n",
            "==================================================\n",
            "Shape: (145, 1)\n",
            "Columns: ['names']\n",
            "\n",
            "First 2 rows:\n",
            "                names\n",
            "0  test_imgs/0001.png\n",
            "1  test_imgs/0015.png\n",
            "\n",
            "==================================================\n",
            "📄 submission.csv\n",
            "==================================================\n",
            "Shape: (145, 4)\n",
            "Columns: ['names', 'wp_r', 'wp_g', 'wp_b']\n",
            "\n",
            "First 2 rows:\n",
            "                names      wp_r      wp_g      wp_b\n",
            "0  test_imgs/0001.png  0.393742  0.610143  0.786508\n",
            "1  test_imgs/0015.png  0.738509  0.593700  0.253388\n",
            "\n",
            "==================================================\n",
            "📄 final_submission.csv\n",
            "==================================================\n",
            "Shape: (145, 4)\n",
            "Columns: ['image_path', 'wp_r', 'wp_g', 'wp_b']\n",
            "\n",
            "First 2 rows:\n",
            "           image_path      wp_r      wp_g      wp_b\n",
            "0  test_imgs/0001.png  0.173683  0.508642  0.215429\n",
            "1  test_imgs/0015.png  0.266894  0.956725  0.577948\n",
            "\n",
            "==================================================\n",
            "📄 train.csv\n",
            "==================================================\n",
            "Shape: (570, 4)\n",
            "Columns: ['names', 'wp_r', 'wp_g', 'wp_b']\n",
            "\n",
            "First 2 rows:\n",
            "                 names      wp_r      wp_g      wp_b\n",
            "0  train_imgs/0000.png  0.173683  0.508642  0.215429\n",
            "1  train_imgs/0002.png  0.266894  0.956725  0.577948\n",
            "\n",
            "============================================================\n",
            "============================================================\n",
            "📊 SUBISSION FILE MERGER\n",
            "============================================================\n",
            "🔍 Checking available files:\n",
            "✅ /content/submission.csv\n",
            "✅ /content/final_submission.csv\n",
            "✅ /content/train.csv\n",
            "\n",
            "🎯 Found source file: /content/train.csv\n",
            "🔍 Reading files...\n",
            "✅ Submission file loaded: 145 rows\n",
            "   Columns: ['names', 'wp_r', 'wp_g', 'wp_b']\n",
            "✅ Source file loaded: 570 rows\n",
            "   Columns: ['names', 'wp_r', 'wp_g', 'wp_b']\n",
            "⚠️  Warning: Different number of rows!\n",
            "   Submission: 145 rows\n",
            "   Source: 570 rows\n",
            "   Using first 145 rows from each file\n",
            "ℹ️  Using first column as image_path: names\n",
            "\n",
            "🔄 Merging data...\n",
            "✅ Merged file saved: /content/final_submission.csv\n",
            "📊 Result shape: (145, 4)\n",
            "📋 Columns: ['image_path', 'wp_r', 'wp_g', 'wp_b']\n",
            "\n",
            "📄 First 5 rows:\n",
            "           image_path      wp_r      wp_g      wp_b\n",
            "0  test_imgs/0001.png  0.173683  0.508642  0.215429\n",
            "1  test_imgs/0015.png  0.266894  0.956725  0.577948\n",
            "2  test_imgs/0018.png  0.146930  0.495538  0.265573\n",
            "3  test_imgs/0019.png  0.218046  0.712538  0.402101\n",
            "4  test_imgs/0020.png  0.070384  0.183209  0.125570\n",
            "\n",
            "🎉 Merge completed successfully!\n",
            "📁 Final files:\n",
            "-rw-r--r-- 1 root root  6691 Sep  2 14:18 '/content/final_submission (13) (1).csv'\n",
            "-rw-r--r-- 1 root root  6696 Sep  2 14:19  /content/final_submission.csv\n",
            "-rw-r--r-- 1 root root 11167 Sep  2 13:53  /content/submission.csv\n",
            "-rw-r--r-- 1 root root  2761 Sep  2 13:53  /content/test.csv\n",
            "-rw-r--r-- 1 root root 43527 Sep  2 13:14  /content/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfKH9Rt-rF1L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}