{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PLOlU_Ccge4X",
        "outputId": "9d268b51-c8d6-4ec8-9510-434e93f34eb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.12.49-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl.metadata (439 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.2 (from llama_index)\n",
            "  Downloading llama_index_cli-0.4.4-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.13,>=0.12.49 (from llama_index)\n",
            "  Downloading llama_index_core-0.12.49-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.7.10-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.5.3-py3-none-any.whl.metadata (441 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.4.11-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.95.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (3.11.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading banks-2.1.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (0.28.1)\n",
            "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading llama_index_workflows-1.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (2.32.3)\n",
            "Collecting setuptools>=80.9.0 (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.49->llama_index) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (4.14.1)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.49->llama_index) (1.17.2)\n",
            "Collecting llama-cloud==0.1.32 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud-0.1.32-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.32->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (2025.7.14)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (4.13.4)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.2.2)\n",
            "Collecting pypdf<6,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.49-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.49->llama_index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.49->llama_index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.49->llama_index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.49->llama_index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.49->llama_index) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.49->llama_index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.49->llama_index) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.49->llama_index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.49->llama_index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.49->llama_index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.49->llama_index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.49->llama_index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.49->llama_index) (0.16.0)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl.metadata (252 bytes)\n",
            "Collecting llama-cloud-services>=0.6.49 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.49-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.49->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.49->llama_index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.49->llama_index) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.49->llama_index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.49->llama_index) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.49->llama_index) (3.2.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.48-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.48 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.48-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.47-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.47 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.47-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.46-py3-none-any.whl.metadata (6.9 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.45 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.46-py3-none-any.whl.metadata (3.5 kB)\n",
            "  Downloading llama_cloud_services-0.6.45-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.45-py3-none-any.whl.metadata (6.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading llama_parse-0.6.44-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.44 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.44-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.43-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.43 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.43-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.43->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.49->llama_index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (1.17.0)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.49->llama_index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.49->llama_index) (3.0.2)\n",
            "Downloading llama_index-0.12.49-py3-none-any.whl (7.1 kB)\n",
            "Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_cli-0.4.4-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.49-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.7.10-py3-none-any.whl (16 kB)\n",
            "Downloading llama_cloud-0.1.32-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.4.7-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.5.3-py3-none-any.whl (3.4 kB)\n",
            "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_readers_file-0.4.11-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading banks-2.1.3-py3-none-any.whl (28 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_workflows-1.1.0-py3-none-any.whl (37 kB)\n",
            "Downloading llama_parse-0.6.43-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.43-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, setuptools, python-dotenv, pypdf, mypy-extensions, marshmallow, deprecated, colorama, aiosqlite, typing-inspect, griffe, llama-index-instrumentation, llama-cloud, dataclasses-json, banks, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiosqlite-0.21.0 banks-2.1.3 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-cloud-0.1.32 llama-cloud-services-0.6.43 llama-index-agent-openai-0.4.12 llama-index-cli-0.4.4 llama-index-core-0.12.49 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.10 llama-index-instrumentation-0.2.0 llama-index-llms-openai-0.4.7 llama-index-multi-modal-llms-openai-0.5.3 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.11 llama-index-readers-llama-parse-0.4.0 llama-index-workflows-1.1.0 llama-parse-0.6.43 llama_index-0.12.49 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-5.8.0 python-dotenv-1.1.1 setuptools-80.9.0 striprtf-0.0.26 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources"
                ]
              },
              "id": "df22c70ada4e42feb2326130afb78b1b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index.llms.mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0qgkFlFiNA4",
        "outputId": "2d0f3eae-ab71-4459-fc17-d9be112db953"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index.llms.mistralai\n",
            "  Downloading llama_index_llms_mistralai-0.6.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama_index.llms.mistralai) (0.12.49)\n",
            "Collecting mistralai>=1.8.2 (from llama_index.llms.mistralai)\n",
            "  Downloading mistralai-1.9.2-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.17.2)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.8.2->llama_index.llms.mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.8.2->llama_index.llms.mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.8.2->llama_index.llms.mistralai) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (4.3.8)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.8.2->llama_index.llms.mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama_index.llms.mistralai) (3.0.2)\n",
            "Downloading llama_index_llms_mistralai-0.6.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading mistralai-1.9.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: eval-type-backport, mistralai, llama_index.llms.mistralai\n",
            "Successfully installed eval-type-backport-0.2.2 llama_index.llms.mistralai-0.6.1 mistralai-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index.embeddings.mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW6mm-h7iQg3",
        "outputId": "16aa1e49-0b9e-4b0f-dc31-6f31044dc108"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index.embeddings.mistralai\n",
            "  Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl.metadata (696 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama_index.embeddings.mistralai) (0.12.49)\n",
            "Requirement already satisfied: mistralai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from llama_index.embeddings.mistralai) (1.9.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.17.2)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama_index.embeddings.mistralai) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama_index.embeddings.mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama_index.embeddings.mistralai) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (4.3.8)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.0.0->llama_index.embeddings.mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama_index.embeddings.mistralai) (3.0.2)\n",
            "Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: llama_index.embeddings.mistralai\n",
            "Successfully installed llama_index.embeddings.mistralai-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3cE9DLLu91i_"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "import os\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"your-api-key\"  # для OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/dd/\").load_data()  # из папки\n",
        "# Поддерживает PDF, TXT, Word, HTML и др."
      ],
      "metadata": {
        "id": "cblLpnr6gbe5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "xrPl8cbsgl-e",
        "outputId": "e9258c54-e6f2-4bdb-d644-5bbf786cd2cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/embeddings/utils.py\u001b[0m in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mvalidate_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/embeddings/openai/utils.py\u001b[0m in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_API_KEY_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-44880425.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             )\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             return cls(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mstorage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_nodes_override\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_nodes_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         self._embed_model = resolve_embed_model(\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0membed_model\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/settings.py\u001b[0m in \u001b[0;36membed_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"\"\"Get the embedding model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_embed_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/embeddings/utils.py\u001b[0m in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m             )\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;34m\"\\n******\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;34m\"Could not load OpenAI embedding model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "\n",
        "# Инициализация Mistral\n",
        "llm = MistralAI(api_key=\"MIttQcFf85fUBuQ4CxRiUVO0MNgpWlxW\", model=\"mistral-small\")\n",
        "embed_model = MistralAIEmbedding(api_key=\"MIttQcFf85fUBuQ4CxRiUVO0MNgpWlxW\", model_name=\"mistral-embed\")\n",
        "\n",
        "# Чтение документов\n",
        "documents = SimpleDirectoryReader(\"/content/dd\").load_data()  # Поддерживает PDF, TXT, Word, HTML\n",
        "\n",
        "# Создание индекса с Mistral\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "# Сохранение индекса\n",
        "index.storage_context.persist(persist_dir=\"storage_mistral/\")\n",
        "\n",
        "# Загрузка индекса\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"storage_mistral/\")\n",
        "loaded_index = VectorStoreIndex.from_documents(\n",
        "    [],\n",
        "    storage_context=storage_context,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9r5maZshZMZ",
        "outputId": "e65c868a-f5a8-4cac-87b1-e628b6ca68d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading llama_index.core.storage.kvstore.simple_kvstore from storage_mistral/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from storage_mistral/index_store.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Базовый запрос\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Объясни LlamaIndex простыми словами\")\n",
        "print(response)\n",
        "\n",
        "# Кастомные промпты\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "mistral_prompt = PromptTemplate(\"\"\"<s>[INST]\n",
        "Контекст: {context_str}\n",
        "Вопрос: {query_str}\n",
        "[/INST]\"\"\")\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": mistral_prompt}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "vJ8bgzNugwqG",
        "outputId": "2414fda0-94bd-4a4a-bf03-22b01491abaa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/utils.py\u001b[0m in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mvalidate_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/openai/utils.py\u001b[0m in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_API_KEY_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-1721670865.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Базовый запрос\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_query_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Объясни LlamaIndex простыми словами\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36mas_query_engine\u001b[0;34m(self, llm, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mresolve_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         )\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/settings.py\u001b[0m in \u001b[0;36mllm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Get the LLM.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/utils.py\u001b[0m in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     47\u001b[0m             )\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;34m\"\\n******\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;34m\"Could not load OpenAI model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка параметров Mistral\n",
        "llm.temperature = 0.7  # Креативность\n",
        "llm.max_tokens = 2048  # Максимальная длина ответа\n",
        "\n",
        "# Пакетная обработка\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Объясни RAG\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"Retrieval-Augmented Generation это...\"),\n",
        "    ChatMessage(role=\"user\", content=\"Как это реализовано в LlamaIndex?\")\n",
        "]\n",
        "response = llm.chat(messages)"
      ],
      "metadata": {
        "id": "Kt_nhWXaj4a4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHvGtNGBkDA-",
        "outputId": "1d82fe69-5bef-4aa0-b166-625041445340"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Retrieval-Augmented Generation (RAG) модель в LlamaIndex реализована путем комбинирования модели генерации языка с моделью поиска.\n",
            "\n",
            "В частности, LlamaIndex использует модель поиска, которая индексирует текстовые документы и способна быстро и точно отвечать на запросы к этим документам. Когда пользователь задает вопрос, модель поиска извлекает соответствующие документы из индекса.\n",
            "\n",
            "Затем эти документы передаются в модель генерации языка, которая использует их в качестве контекста для генерирования ответа на запрос пользователя. Это позволяет LlamaIndex предоставлять более информативные и точные ответы, поскольку модель генерации языка может использовать знания, содержащиеся в документах, для создания своих ответов.\n",
            "\n",
            "Эта комбинация модели поиска и модели генерации языка позволяет LlamaIndex достигать более высокого уровня точности и полезности в своих ответах, чем модель генерации языка, работающая в изоляции.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка чанкинга\n",
        "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
        "\n",
        "node_parser = SentenceWindowNodeParser(\n",
        "    window_size=3,\n",
        "    window_metadata_key=\"window\",\n",
        "    original_text_metadata_key=\"original_text\"\n",
        ")\n",
        "\n",
        "# Альтернативные ретриверы\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=3,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "2ew-E13MkKpP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=3,\n",
        "    llm=llm\n",
        ")\n",
        "response = query_engine.query(\"Какие есть альтернативы LlamaIndex?\")"
      ],
      "metadata": {
        "id": "Jy05RFQikNp-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EMTQ1jhkT-3",
        "outputId": "8f097fa8-7fd8-40dc-c00a-db5b9f71fe44"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context provided, alternative models to LlamaIndex include Longformer, BigBird, Linformer, and LongT5. These models are designed to handle long-context tasks and may serve as alternatives to LlamaIndex in various applications. However, the specific context does not provide information about the direct comparison or evaluation of these models with LlamaIndex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "\n",
        "memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
        "chat_engine = index.as_chat_engine(\n",
        "    chat_mode=\"context\",\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "5kDKiJh7kYV2",
        "outputId": "3873a8ea-4e18-4d38-86fb-e48a036ab560"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/utils.py\u001b[0m in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mvalidate_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/openai/utils.py\u001b[0m in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_API_KEY_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-773143758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatMemoryBuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m chat_engine = index.as_chat_engine(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mchat_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36mas_chat_engine\u001b[0;34m(self, chat_mode, llm, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mresolve_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         )\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/settings.py\u001b[0m in \u001b[0;36mllm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Get the LLM.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/utils.py\u001b[0m in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     47\u001b[0m             )\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;34m\"\\n******\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;34m\"Could not load OpenAI model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# Инициализация Mistral\n",
        "llm = MistralAI(\n",
        "    api_key=\"MIttQcFf85fUBuQ4CxRiUVO0MNgpWlxW\",\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Кастомный промпт для Mistral в формате INST\n",
        "mistral_rag_prompt = PromptTemplate(\"\"\"<s>[INST]\n",
        "Ты - эксперт по анализу документов. Ответь на вопрос, используя только предоставленный контекст.\n",
        "Контекст: {context_str}\n",
        "Вопрос: {query_str}\n",
        "Ответь подробно и точно. [/INST]\"\"\")\n",
        "nodes = index.as_retriever().retrieve(\"Что такое RAG?\")\n",
        "# Инициализация синтезатора с настройками для Mistral\n",
        "synth = get_response_synthesizer(\n",
        "    llm=llm,\n",
        "    response_mode=\"tree_summarize\",\n",
        "    text_qa_template=mistral_rag_prompt,\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "# Генерация ответа\n",
        "response = synth.synthesize(\n",
        "    \"Что такое RAG?\",\n",
        "    nodes = nodes\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzLnhoFrlZzW",
        "outputId": "e5958c6f-e056-4abc-ba14-bf61cc1f6b4a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The query is in Russian, asking \"What is RAG?\" However, there is no information provided about \"RAG\" in the given context. Therefore, I cannot provide an answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "openai"
      ],
      "metadata": {
        "id": "MzHgd_SPkcwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Что такое LlamaIndex?\")\n",
        "print(response)  # ответ LLM"
      ],
      "metadata": {
        "id": "kuhl6-q1knoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt = PromptTemplate(\"\"\"Контекст: {context_str}\n",
        "Вопрос: {query_str}\n",
        "Ответ:\"\"\")\n",
        "\n",
        "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt})"
      ],
      "metadata": {
        "id": "Hdacx0z8koMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "создание индеса"
      ],
      "metadata": {
        "id": "7hqOhtjvmGIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"data/\").load_data()  # из папки\n",
        "# Поддерживает PDF, TXT, Word, HTML и др.\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "index.storage_context.persist(persist_dir=\"storage/\")  # сохранить\n",
        "# Загрузка:\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"storage/\")\n",
        "loaded_index = load_index_from_storage(storage_context)"
      ],
      "metadata": {
        "id": "4O7KzQwdkpa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "web"
      ],
      "metadata": {
        "id": "LaN2_AvVlOpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "urls = [\"https://example.com\"]\n",
        "web_docs = SimpleWebPageReader().load_data(urls)"
      ],
      "metadata": {
        "id": "rHOI2mb0kwWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sql"
      ],
      "metadata": {
        "id": "aj2ntngVlPyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.database import DatabaseReader\n",
        "db_reader = DatabaseReader(\"sqlite:///database.db\")\n",
        "query = \"SELECT * FROM table\"\n",
        "db_docs = db_reader.load_data(query=query)"
      ],
      "metadata": {
        "id": "zKScpdClkxFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "4uvRbGxIk1h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "splitter = SentenceSplitter(chunk_size=512)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "sGgJSP02k15N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever, KeywordTableRetriever\n",
        "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=2)\n",
        "keyword_retriever = KeywordTableRetriever(index=index)"
      ],
      "metadata": {
        "id": "YeTgSJkok3NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rag"
      ],
      "metadata": {
        "id": "zdRPkIfplLhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "synth = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
        "response = synth.synthesize(\"Что такое RAG?\", nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "c0gafqPkk44W",
        "outputId": "8bf60bef-3864-481a-8ba7-fc5bfda516ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/utils.py\u001b[0m in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mvalidate_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/llms/openai/utils.py\u001b[0m in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_API_KEY_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-2045287750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_response_synthesizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msynth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_response_synthesizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tree_summarize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Что такое RAG?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/response_synthesizers/factory.py\u001b[0m in \u001b[0;36mget_response_synthesizer\u001b[0;34m(llm, prompt_helper, text_qa_template, refine_template, summary_template, simple_template, response_mode, callback_manager, use_async, streaming, structured_answer_filtering, output_cls, program_factory, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback_manager\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     prompt_helper = (\n\u001b[1;32m     60\u001b[0m         \u001b[0mprompt_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/settings.py\u001b[0m in \u001b[0;36mllm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Get the LLM.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/llms/utils.py\u001b[0m in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     47\u001b[0m             )\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;34m\"\\n******\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;34m\"Could not load OpenAI model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "agents"
      ],
      "metadata": {
        "id": "lF-qX8NClM0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import ReActAgent\n",
        "agent = ReActAgent.from_tools(tools=[...], llm=llm)\n",
        "response = agent.chat(\"Сравни LlamaIndex и LangChain\")"
      ],
      "metadata": {
        "id": "BqNMaOMqk8YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "логгирование"
      ],
      "metadata": {
        "id": "9k3C-NN0k--9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "HZuTkUN0k-Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "графы зависимостей"
      ],
      "metadata": {
        "id": "3mEaOiwTlCvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import KnowledgeGraphIndex\n",
        "kg_index = KnowledgeGraphIndex.from_documents(documents)\n",
        "kg_index.get_networkx_graph()  # для визуализации"
      ],
      "metadata": {
        "id": "GipLx9UglCTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}